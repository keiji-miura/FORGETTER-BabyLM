{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d1f54a-eff5-43bd-bad2-ae64c3513f64",
   "metadata": {},
   "source": [
    "##### Note: The summaries for the other layers will come soon in the same format after the clean up is done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4dc65c-6467-45fb-b5c9-7c24dac02a6a",
   "metadata": {},
   "source": [
    "# Summary of best models for given numbers of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395f15-8b92-41a5-8c3d-af5ec3f42f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is the output for a model in the following format. Different lines for different model structures. See the cell below for the excecuted codes.\n",
    "# print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')\n",
    "# Example: [16layers, 12(6)heads, hidden-dim=696, inter-layer-dim = 696x4, head-dim=224, #param=218million, val_loss_prev, #epochs, val_loss_current, val_loss_prev_prev, train_loss, Num-steps]\n",
    "\n",
    "# L36 in preparation\n",
    "# L30 in preparation\n",
    "[+24, 6, 3, 696, 2784, 288, 261.3,+3.0112, 1, 3.0146,  10000, 2.6960, 18000],          # minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-21-01-19.pth\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0053, 2,+3.0078, 3.0100, 2.6584, 9000],           # minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-06-21-17-20.pth\n",
    "[+20, 8, 4, 704, 2816, 256, 241.0,+3.0108,  2, 3.1759, 3.0122, 2.7514, 11000],         # minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-07-17-20-37.pth\n",
    "# L18 in preparation\n",
    "[+16, 8, 4, 704, 2816, 272, 204.2,+3.0087,  1, 3.0105,  10000, 2.7004,  9000],         # minGemma-hidden_layers16-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-07-21-20-23.pth\n",
    "[+12,12, 4, 768, 3072, 192, 180.3,+3.0071,  1, 3.0082,  10000, 2.6972,  9000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2,+3.0219, 11, 3.0224, 3.0228, 2.6944, 12000],\n",
    "[+ 9,16, 4, 704, 2816, 224, 145.7,+3.0152, 19, 3.0166, 3.0219, 2.7185, 10000, 0.0000], # minGemma-hidden_layers9-att_heads16-kv_heads4-hidden704-intermediate2816-head_dim224-T512--2025-07-05-14-02.pth\n",
    "[  8, 8, 4, 800, 3200, 480, 175.4,+3.0203, 14, 3.0210, 3.0216, 2.6994, 10950],         # minGemma-hidden_layers8-att_heads8-kv_heads4-hidden800-intermediate3200-head_dim480-T512--2025-05-04-01-18.pth\n",
    "[  6, 8, 4, 768, 3072, 256, 109.4,+3.0407, 12, 3.0411, 3.0409, 2.7164, 10000],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a28e70-030c-4682-ad3f-52def5cf28c2",
   "metadata": {},
   "source": [
    "# L36 (Models with 36 Layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e110e1d-5a70-44f1-93a1-8b547dc23a94",
   "metadata": {},
   "source": [
    "# L30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b00ad-da99-4f43-8ca7-fd8eada5e326",
   "metadata": {},
   "source": [
    "# L24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448541f4-9e60-4cbe-86ab-69bb857d34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L24 Forgetter with last drop (sleep2 in the end) (default: \n",
    "\n",
    "[ 24, 6, 3, 720, 2880, 240, 260.3, 3.0794, 7, 3.1096, 3.0191, 3.0659, 20000, 0.0000], # 1.0h F1:3.0191 F2@8th:+\n",
    "[ 24, 6, 3, 720, 2880, 208, 250.4, 3.0180, 7, 3.0192, 3.0729, 2.7457, 20000, 0.0000], # 1.0h F1:3.0253 F2@7th:-0.0073\n",
    "[ 24, 6, 3, 696, 2784, 304, 266.1, 3.0148,10, 3.0155, 3.1557, 2.7362, 20000, 0.0000], # 1.1h F1:3.0206 F2@10th:-0.0058 (7) # B2=12 15.1G\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3,+3.0147,12, 3.0154, 3.0183, 2.7094, 20000, 0.0000], # 1.1h F1:3.0182 F2@12th:-0.0035  criteria:3.0182  GC  F1:minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-17-06-08.pth F2:minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-17-08-46.pth\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 272, 256.5, 3.0742,11, 3.0838, 3.1565, 2.9403, 20000, 0.0000], # 1.1h F1:3.0209 F2@10th:+\n",
    "[ 24, 6, 3, 696, 2784, 256, 251.6, 3.0176,11, 3.1434, 3.0198, 2.7265, 20000, 0.0000], # 1.0h F1:3.0190 F2@11th:-0.0014 11.1G\n",
    "[ 24, 6, 3, 696, 2784, 240, 246.8, 3.0310, 5, 3.1220, 3.0580, 2.7812, 20000, 0.0000], # 1.0h F1:3.0462 F2@5th:-0.0152\n",
    "[ 24, 6, 3, 696, 2784, 224, 242.0, 3.0189, 7, 3.0869, 3.0206, 2.7193, 20000, 0.0000], # 1.0h F1:3.0285 F2@6th:-0.0079 10.8G\n",
    "[ 24, 6, 3, 696, 2784, 208, 237.2, 3.0187,12, 3.0542,+3.0181, 2.7219, 20000, 0.0000], # 1.0h F1:3.0181 F2@13th:+\n",
    "[ 24, 6, 3, 696, 2784, 192, 232.4, 3.0190,10, 3.0229, 3.0256, 2.7308, 20000, 0.0000], # 0.9h F1:3.0252 F2@10th:-0.0062\n",
    "[ 24, 6, 3, 672, 2688, 288, 247.6, 3.0183,10, 3.0187, 3.0248, 2.8251, 20000, 0.0000], # 1.1h F1:3.0232 F2@10th:-0.0049 11.0G\n",
    "[ 24, 6, 3, 672, 2688, 256, 238.3,+3.0143, 9, 3.1357, 3.1546, 2.7206, 20000, 0.0000], # 1.0h F1:3.0211 F2@9th:-0.0068 10.3G\n",
    "[ 24, 6, 3, 672, 2688, 208, 224.4,+3.0142,12, 3.1270, 3.1238, 2.7160, 20000, 0.0000], # 1.0h F1:3.0193 F2@12th:-0.0051 10.1G\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.0188,10, 3.0190, 3.0201, 2.7312, 10000, 0.0000], # 0.9h F1:3.0259 F2@8th:-0.0055 14.6G ## TOO big???\n",
    "[ 24, 6, 3, 648, 2592, 224, 216.4, 3.0167, 9, 3.0183, 3.0216, 2.7274, 10000, 0.0000], # 0.9h F1:3.0216 F2@9th:-0.0049 14.4G\n",
    "[ 24, 6, 3, 648, 2592, 208, 211.9, 3.0161, 9, 3.0250, 3.0253, 2.7309, 10000, 0.0000], # 0.8h F1:3.0244 F2@9th:-0.0083\n",
    "[ 24, 6, 3, 648, 2592, 192, 207.4, 3.0212, 8, 3.0233, 3.1457, 2.7211, 10000, 0.0000], # 0.8h F1:3.0267 F2@8th:-0.0055\n",
    "[ 24, 6, 3, 648, 2592, 176, 202.9, 3.0174,12, 3.0233, 3.0957, 2.7176, 10000, 0.0000], # 0.8h F1:3.0233 F2@12th:-0.0059\n",
    "[ 24, 6, 3, 612, 2448, 256, 206.5, 3.0247,14, 3.0253, 3.0257, 2.7318, 10000, 0.0000], # 0.9h F1:3.0387 F2@7th:-0.0075\n",
    "[ 24, 6, 3, 612, 2448, 240, 202.2, 3.0252, 9, 3.1306, 3.0248, 2.7219, 10000, 0.0000], # 0.9h F1:3.0248 F2@10th:+(spike?) 14.2G\n",
    "[ 24, 6, 3, 612, 2448, 224, 198.0, 3.0171, 9, 3.1060, 3.0243, 2.7344, 10000, 0.0000], # 0.8h F1:3.0234 F2@9th:-0.0063\n",
    "[ 24, 6, 3, 612, 2448, 208, 193.8, 3.0290, 9, 3.0306, 3.1664, 2.7386, 10000, 0.0000], # 0.8h F1:3.0324 F2@9th:-0.0034\n",
    "[ 24, 6, 3, 612, 2448, 192, 189.5, 3.0243, 7, 3.0923, 3.0531, 2.7375, 10000, 0.0000], # 0.8h F1:3.0327 F2@7th:-0.0084\n",
    "[ 24, 6, 3, 612, 2448, 176, 185.3, 3.0260, 9, 3.0759, 3.0259, 2.7571, 10000, 0.0000], # 0.7h F1:3.0259 F2@10th:+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a327cf5-b8dc-4f66-8971-92a6836c46df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L24 Forgetter, continued from the saved 3.0182 model and only the last drop (sleep 2) was executed (default: \n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3,+3.0147,12, 3.0154, 3.0183, 2.7094, 20000, 0.0000], # 1.1h F1:3.0182 F2@12th:-0.0035 F1:minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-17-06-08.pth F2:minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-17-08-46.pth\n",
    "\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0164, 1, 3.0862,  10000, 2.7803, 22000], # continued lr12 WD1  ### cf. Original: 10000-10-1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0146, 2, 3.0162, 3.0154, 2.7672, 22000], # continued lr11 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0158, 1, 3.1360,  10000, 2.7635, 22000], # continued lr10 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0256, 4, 3.0737, 3.0318, 2.7128, 22000], # continued lr9 WD1\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0279, 4, 3.0896, 3.0322, 2.8014, 20000], # continued lr12 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0139, 1, 3.0156,  10000, 2.7579, 20000], # continued lr11 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0124, 1, 3.0132,  10000, 2.6820, 20000], # continued lr9 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0179, 1, 3.0211,  10000, 2.6239, 20000], # continued lr8 WD1\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0148, 1, 3.1132,  10000, 2.7640, 18000], # continued lr11 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0133, 1, 3.1104,  10000, 2.7478, 18000], # continued lr10 WD1 # 3.0126 again reproducible\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0134, 1, 3.0159,  10000, 2.7109, 18000], # continued lr9 WD1.1\n",
    "[+24, 6, 3, 696, 2784, 288, 261.3,+3.0112, 1, 3.0146,  10000, 2.6960, 18000], # continued lr9 WD1   putative: minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-21-01-19.pth\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0127, 1, 3.0163,  10000, 2.6375, 18000], # continued lr9 WD0.9\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0130, 1, 3.0167,  10000, 2.6554, 18000], # continued lr8 WD1\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0636, 3, 3.0998, 3.0950, 2.8651, 16000], # continued lr11 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0136, 1, 3.0140,  10000, 2.7350, 16000], # continued lr10 WD1\n",
    "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0665, 2, 3.0929, 3.1696, 2.8791, 16000], # continued lr9 WD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da84e84-a25f-4b2f-ad4b-fb28586075c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.44\n",
      "L24 att6 kv_heads3 hidden696 intermediate2784 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.939800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.93982890625 tensor(3.1693)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.826100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.826131640625 tensor(3.0610)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.760600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.760646484375 tensor(3.0401)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.744000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7440244140625 tensor(3.0312)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.721700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7217283203125 tensor(3.0253)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.712200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7122130859375 tensor(3.0228)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.708100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7080822265625 tensor(3.0200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.708300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.7082763671875 tensor(3.0192)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.702100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.702126171875 tensor(3.0189)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.708500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7085181640625 tensor(3.0182)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.7069435546875 tensor(3.0183)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.709400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.709409765625 tensor(3.0147)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 1:10:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.731400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.7313634765625 tensor(3.0154)\n",
      "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0147, 12, 3.0154, 3.0183, 2.7094, 20000, 0.0000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  24 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12//2; N_step=10000*2; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# F2(F1) criteria style\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100; flag=0; criteria=0.0005 * 0 # * 4 * 4 * 4 # count = all, 5, 3~4, 2\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if flag==1: # Forgetter_2\n",
    "        print(\"F2\"); del model; torch.cuda.empty_cache(); model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(gradient_accumulation_steps=2, learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    model.eval(); loss = []; B2=12 # Evaluation follows\n",
    "    for k in range(6000): # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); del common; torch.cuda.empty_cache(); model.train();\n",
    "    if loss_current < 3.0182:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= loss_prev) and (flag==1):\n",
    "        break\n",
    "    if (loss_current >= (loss_prev-criteria)) and (flag==0):\n",
    "        flag=1\n",
    "        if loss_current >= loss_prev:\n",
    "            model.load_state_dict(torch.load('test.pth')) # back to previous\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss; torch.save(model.state_dict(), 'test.pth'); # always save\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24342679-7892-4c44-a2a3-29152de7c900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.296\n",
      "L24 att6 kv_heads3 hidden696 intermediate2784 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:03:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.696000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.6959811197916665 tensor(3.0112)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:03:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.692900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.692870442708333 tensor(3.0146)\n",
      "[ 24, 6, 3, 696, 2784, 288, 261.3, 3.0112, 1, 3.0146, 10000.0000, 2.6960, 18000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  24 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12//2; N_step=9000*2; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "#\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers24-att_heads6-kv_heads3-hidden696-intermediate2784-head_dim288-T512--2025-07-17-06-08.pth'))\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter2 with save & load\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if ep > 0: # Forgetter_2-0\n",
    "        model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(gradient_accumulation_steps=2, learning_rate=9e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    loss = []; model.eval(); B2=12//6 # Evaluation follows\n",
    "    for k in range(6000*2): # 6000 # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "    if loss_current < 3.0147:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= (loss_prev-0.0005*0)): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "        break\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "    torch.save(model.state_dict(), 'test.pth'); del model; torch.cuda.empty_cache(); # Forgetter_2-0\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1e183-618b-476d-8ef3-b6b2ea3a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L24 Forgetter without last drop (no sleep2 in the end) (default: x4 lr10.5 WD1)\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.0200, 7, 3.0733, 3.0221, 2.7347, 11000], # 1.0h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb82a777-297e-4544-8ff0-20d022ede4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L24 att6 kv_heads3 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.836700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.8366999289772727 tensor(3.1369)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.824500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.8244621803977275 tensor(3.0496)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.769700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7697292258522728 tensor(3.0344)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.749300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7493412642045456 tensor(3.0284)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.739400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7393666548295457 tensor(3.0245)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.734900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7349078480113636 tensor(3.0221)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.734700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7346851917613635 tensor(3.0200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:01:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.029900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 3.0299279119318183 tensor(3.0733)\n",
      "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.0200, 7, 3.0733, 3.0221, 2.7347, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  24 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*108 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 103.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "        for k in range(6000): ## std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0162:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88310694-039f-45b8-be30-9d5f47517b00",
   "metadata": {},
   "source": [
    "# L22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8a553-7f42-4467-ba6d-c0773fe7381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L22 Forgetter, continued from the saved 3.0164 model and only the last drop (sleep 2) was executed (default: lr10.5 WD1)\n",
    "\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0164, 8, 3.0948, 3.0178, 2.7259, 11000], # minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-05-14-13-11.pth\n",
    "\n",
    "# The last drop was found by chance\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0103, 1, 3.1061, 100000, 2.7302, 11000], # Continued # minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-05-17-06-15.pth\n",
    "\n",
    "# Reproducibility of the last big drop checked with optimal B searched\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0105, 1, 3.0168,  10000, 2.6118,13500], # Continued lr10.5e-4 WD0.9 B16/2\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0131, 1, 3.0142,  10000, 2.6481,13500], # Continued lr 9.5e-4 WD0.9 B16/2\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0124, 1, 3.1651,  10000, 2.6823,15428], # Continued lr10.5e-4 WD0.9 B14/2 # 0.8h 10.7G\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0104, 2, 3.0157, 3.0138, 2.6937,15428], # Continued lr 9.5e-4 WD0.9 B14/2 # 0.8h 10.7G\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3,+3.0086, 1,+3.0092,  10000, 2.7559,18000], # Continued lr10.5e-4 WD0.9 B12/2 # 0.8h  9.9G # grad-acc reproducible!!!\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0123, 1, 3.1547,  10000, 2.6837,18000], # Continued lr 9.5e-4 WD0.9 B12/2 # 0.8h  9.9G\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3,+3.0080, 1,stopped,  10000, 2.6786, 9000], # Continued lr10.5e-4 WD0.9 B13 >4h 15.3G\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0253, 1, 3.0278,  10000, 2.8398, 9000], # Continued lr10.5e-4 WD1.0 B10 ### explore N fair manner???\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0484, 1, 3.0559,  10000, 2.9087, 9000], # Continued lr10.5e-4 WD1.0 B08\n",
    "\n",
    "# Reproducibility of the last big drop checked with optimal lr, WD searched\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0142, 1, 3.0169,  10000, 2.7764, 9000], # Continued lr10.5e-4 WD1.1\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0133, 1, 3.1211,  10000, 2.8166, 9000], # Continued lr 8.5e-4 WD1.1\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0169, 1, 3.0176,  10000, 2.8162, 9000], # Continued lr12.5e-4 WD1.0\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0138, 1, 3.0143,  10000, 2.7807, 9000], # Continued lr11.5e-4 WD1.0\n",
    "                                  +3.0097                                    # Continued lr10.5e-4 WD1.0 (copied)\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0105, 3, 3.1373, 3.0106, 2.7309, 9000], # Continued lr 9.5e-4 WD1.0\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3,+3.0095, 1, 3.0109,  10000, 2.7523, 9000], # Continued lr 8.5e-4 WD1.0\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0123, 1, 3.0174,  10000, 2.6304, 9000], # Continued lr 7.5e-4 WD1.0\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0177, 1, 3.0274,  10000, 2.5680, 9000], # Continued lr 6.5e-4 WD1.0\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3,+3.0068, 1,+3.0078,  10000, 2.7150, 9000], # Continued lr10.5e-4 WD0.9\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0053, 2,+3.0078, 3.0100, 2.6584, 9000], # Continued lr 9.5e-4 WD0.9 # 0.8h 14.0G  minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-06-21-17-20.pth\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0098, 1, 3.0118,  10000, 2.7680, 9000], # Continued lr 8.5e-4 WD0.9\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0122, 1, 3.1506,  10000, 2.6767, 9000], # Continued lr10.5e-4 WD0.8\n",
    "\n",
    "# N (only for the last drop) was searched: N9000 for the last drop is optimal\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0307, 3, 3.0518, 3.0338, 2.7061, 20000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0183, 2, 3.0185, 3.0194, 2.7158, 15000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0176, 2, 3.1007, 3.0179, 2.7312, 14000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0158, 1, 3.0990,  10000, 2.7337, 13000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0110, 2, 3.1010, 3.0113, 2.7225, 12000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0129, 1, 3.0140,  10000, 2.7956, 11000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0102, 1, 3.0109,  10000, 2.7341, 10000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0338, 4, 3.0967, 3.0401, 2.8676,  9500], # Continued x\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0117, 1, 3.0979,  10000, 2.7568,  9500], # Continued again\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0097, 1, 3.0109,  10000, 2.7413,  9000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0114, 1, 3.2050,  10000, 2.7495,  8500], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0103, 1, 3.0116,  10000, 2.7495,  8000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0143, 1, 3.0174,  10000, 2.7669,  7000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0187, 1, 3.0213,  10000, 2.7710,  6000], # Continued\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0261, 1, 3.2551,  10000, 2.8025,  5000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0364, 1, 3.0404,  10000, 2.8086,  4000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0495, 1, 3.0515,  10000, 2.8106,  3000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0789, 1, 3.1002,  10000, 2.8290,  2000], # Continued  F2 harms\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.1462, 1, 3.1798,  10000, 2.8933,  1000], # Continued  F2 harms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e83ee474-8b49-4f54-b3d3-da2a8200bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L22 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.730200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.730234375 tensor(3.0103)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.134300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3.1343394886363636 tensor(3.1061)\n",
      "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0103, 1, 3.1061, 10000.0000, 2.7302, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  22 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "#\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-05-14-13-11.pth'))\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-eval\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 103.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "        for k in range(6000): ## std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0162:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f4bff9-82fd-49d6-83ac-4615d1bb0d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\anaconda3\\envs\\minGemma\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.296\n",
      "L22 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\AppData\\Local\\Temp\\ipykernel_14028\\2997928410.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 49:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.681300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.6813359375 tensor(3.0100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 49:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.658400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.65836328125 tensor(3.0053)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 49:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.651400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.6514149305555557 tensor(3.0078)\n",
      "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0053, 2, 3.0078, 3.0100, 2.6584, 9000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  22 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=9000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "# Check reproducibility with Forgetter2-0!!!\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-05-14-13-11.pth'))\n",
    "#model.load_state_dict(torch.load('minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-06-08-14-30.pth')) # after N11000\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter2 with save & load\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if ep > 0: # Forgetter_2-0\n",
    "        model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=9.5e-4, weight_decay=0.9, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "    for k in range(6000): # 6000 # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "    if loss_current < 3.0053:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= (loss_prev-0.0005*0)): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "        break\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "    torch.save(model.state_dict(), 'test.pth'); del model; torch.cuda.empty_cache(); # Forgetter_2-0\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb07dfa-47c5-4b2f-91cf-8cc2747551d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L22 Forgetter without last drop (no sleep2 in the end) (default: x4 B12 lr10.5 WD1)\n",
    "\n",
    "[ 22, 8, 4, 736, 2944, 144, 236.1, 3.0374, 3, 3.0513, 3.0545, 2.7621, 11000], # 0.9h 14.6G\n",
    "[ 22, 8, 4, 704, 2816, 224, 249.7, 3.0226, 7, 3.1073, 3.0238, 2.7253, 11000], # 15G/16\n",
    "[ 22, 8, 4, 704, 2816, 192, 237.8, 3.0296, 4, 3.1437, 3.0372, 2.7414, 11000],\n",
    "[ 22, 8, 4, 704, 2816, 144, 219.9, 3.0251, 7, 3.0831, 3.0263, 2.7335, 11000], # 0.9h 14.3G\n",
    "[ 22, 8, 4, 672, 2688, 224, 232.6, 3.0207, 7, 3.0228, 3.0226, 2.7176, 11000], # 1.0h 14.7G\n",
    "[+22, 8, 4, 672, 2688, 192, 221.3,+3.0164, 8, 3.0948, 3.0178, 2.7259, 11000], #             minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-05-14-13-11.pth\n",
    "[ 22, 8, 4, 672, 2688, 160, 209.9, 3.0229,10, 3.0585, 3.0237, 2.7305, 11000], # 0.9h 14.1G\n",
    "[ 22, 8, 4, 672, 2688, 144, 204.2, 3.0384, 3, 3.0673, 3.0553, 2.7607, 11000],\n",
    "[ 22, 8, 4, 640, 2560, 224, 216.1, 3.0307, 4, 3.0573, 3.0362, 2.7418, 11000], # 1.0h 14.4G\n",
    "[ 22, 8, 4, 640, 2560, 192, 205.3, 3.0232, 7, 3.0233, 3.0255, 2.7268, 11000],\n",
    "[ 22, 8, 4, 640, 2560, 144, 189.1, 3.0209,13, 3.1162, 3.0216, 2.7297, 11000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26ccc7e-4332-4e44-a347-30c5acee2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L22 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.825600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.8255532670454544 tensor(3.1314)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.809800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.8098304332386363 tensor(3.0526)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.761400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7614140625 tensor(3.0336)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.746900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7469382102272726 tensor(3.0257)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.738200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7382153764204546 tensor(3.0204)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.733200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.733230646306818 tensor(3.0193)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.727100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.727079190340909 tensor(3.0178)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.725900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.7258991477272727 tensor(3.0164)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.087800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 3.0877741477272727 tensor(3.0948)\n",
      "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.0164, 8, 3.0948, 3.0178, 2.7259, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  22 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 103.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "        for k in range(6000): ## std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0162:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b673070-1cca-4c77-af10-3400caf1919b",
   "metadata": {},
   "source": [
    "# L20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e5994-a3b7-48f6-b2b3-2dc67be732f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L20 Forgetter, continued from the saved 3.0162 model and only the last drop (sleep 2) was executed (default: lr10e-4 for Forgetter but 10.5 might be best)\n",
    "\n",
    "[+20, 8, 4, 704, 2816, 256, 241.0,+3.0162, 9, 3.0170, 3.0182, 2.7230, 11000], # lr10.5  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-08-00-40.pth\n",
    "\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0128, 1, 3.0912,  10000, 2.7203, 11000], # continued lr10.5e-4 WD1.0 (same parameter)\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0149, 1, 3.0324,  10000, 2.6532, 11000], # continued lr9.5e-4 WD0.9 not saved\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0305, 4, 3.0391, 3.0332, 2.8400, 11500], # continued lr10.5e-4 WD1\n",
    "[+20, 8, 4, 704, 2816, 256, 241.0,+3.0108, 2, 3.1759, 3.0122, 2.7514, 11000], # continued lr11e-4 WD1  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-07-17-20-37.pth\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0337, 4, 3.0361, 3.0415, 2.8714, 11000], # continued lr10.5e-4 WD1.1\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0134, 1, 3.1385,  10000, 2.6887, 11000], # continued lr10.5e-4 WD0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1a7a93-9a94-45ea-9d93-7692ad5ed843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L20 att8 kv_heads4 hidden704 intermediate2816 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:05:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.770400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.770405362215909 tensor(3.0122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:05:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.751400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.751435546875 tensor(3.0108)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:05:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.329400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3.3293817471590907 tensor(3.1759)\n",
      "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0108, 2, 3.1759, 3.0122, 2.7514, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  20 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "#\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-08-00-40.pth'))\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter2 with save & load\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if ep > 0: # Forgetter_2-0\n",
    "        model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=11e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "    for k in range(6000): # 6000 # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "    if loss_current < 3.0162:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= (loss_prev-0.0005*0)): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "        break\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "    torch.save(model.state_dict(), 'test.pth'); del model; torch.cuda.empty_cache(); # Forgetter_2-0\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf4ec8-4f9c-42db-9433-75502b106c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L20 Forgetter without last drop (no sleep2 in the end) (default: lr10e-4 for Forgetter but 10.5 might be best)\n",
    "[ 20, 8, 4, 768, 3072, 256, 274.7, 3.0218, 6, 3.0218, 3.0240, 2.6993, 11000],\n",
    "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.0204, 8, 3.0207, 3.0223, 2.7037, 11000],\n",
    "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.0170,11, 3.1243, 3.0197, 2.7032, 10000], #  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-09-12-03\n",
    "\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0234, 6, 3.0786, 3.0243, 2.7060, 13000],\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0230, 5, 3.0297, 3.0260, 2.7097, 12000],\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0200,10, 3.1214, 3.0203, 2.7458, 10000],\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0188, 9, 3.0188, 3.0195, 2.7014,  9000], #  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-05-05-51\n",
    "\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0367, 4, 3.1471, 3.0419, 2.8340, 11000], # lr13.5e-4\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0220, 6, 3.0610, 3.0242, 2.7823, 11000], # lr11.5\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0276, 4, 3.1127, 3.0326, 2.7535, 11000], # lr11\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0252, 6, 3.0254, 3.0262, 2.7278, 12000], # lr10.5\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0208, 7, 3.1409, 3.0225, 2.7247, 11500], # lr10.5\n",
    "[+20, 8, 4, 704, 2816, 256, 241.0,+3.0162, 9, 3.0170, 3.0182, 2.7230, 11000], # lr10.5  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-08-00-40.pth\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0168, 9, 3.0169, 3.0196, 2.7236, 10500], # lr10.5  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-10-06-19\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0192, 8, 3.0747, 3.0200, 2.7191, 10000], # lr10.5\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0180,11, 3.1427, 3.0190, 2.6924, 11000], # lr10    minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim256-T512--2025-05-04-18-00\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0218, 8, 3.1522, 3.0224, 2.6978, 11000], # lr9.5\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0313, 8, 3.0653, 3.0333, 2.7169, 11000], # lr9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37156c66-55a1-46d1-9c99-222fa1782c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L20 att8 kv_heads4 hidden704 intermediate2816 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.872000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.87202734375 tensor(3.1247)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.805300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.805278231534091 tensor(3.0536)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.755900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.755859552556818 tensor(3.0379)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.736200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7361787997159093 tensor(3.0310)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.726200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7261645951704545 tensor(3.0261)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.722400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.722400568181818 tensor(3.0233)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.715800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7158442826704547 tensor(3.0211)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.721000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.721029296875 tensor(3.0182)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.723000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.722955078125 tensor(3.0162)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 1:06:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.773600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7736136363636366 tensor(3.0170)\n",
      "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.0162, 9, 3.0170, 3.0182, 2.7230, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  20 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-eval\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4 * (Max_ep-ep*0)/Max_ep, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 103.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "        for k in range(6000): ## std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0162:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa70c16-f0f6-493b-bfc3-cc392e6c52b2",
   "metadata": {},
   "source": [
    "# L18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b1352-f58a-453e-9233-18a50c7f0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L18 Forgetter with last drop (sleep2 in the end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f96fc-8448-47e0-a9a8-34bfb75ab128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502bb6d6-d16c-435c-8a7d-dab7626e0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L18 Forgetter without last drop (no sleep2 in the end) (default: x4 lr10.5 N11000)\n",
    "[ 18, 8, 4, 768, 3072, 256, 251.1, 3.0219, 6, 3.0219, 3.0244, 2.7176, 11000], # 1.0h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcefa4b0-d038-46a9-905f-092492453ec4",
   "metadata": {},
   "source": [
    "# L16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e04de-7bc9-4f7b-aef4-cf29397ceda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L16 Forgetter with last drop (sleep2 in the end)\n",
    "\n",
    "[ 16, 9, 3, 720, 2880, 224, 197.8, 3.0263, 17, 3.0289, 3.1207, 2.7079, 10000, 0.0000], # 0.8h F1:3.0307 F2@17th:-0.0044\n",
    "[ 16, 9, 3, 720, 2880, 192, 188.9, 3.0226,  8, 3.0238, 3.0290, 2.6978, 10000, 0.0000], # 0.7h F1:3.0284 F2@8th:-0.0058\n",
    "[ 16, 9, 3, 684, 2736, 256, 191.6, 3.0210, 10, 3.0214, 3.1262, 2.7034, 10000, 0.0000], # 0.8h F1:3.0224 F2@10th:-0.0014\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2,+3.0161, 12, 3.0170, 3.0236, 2.7082, 10000, 0.0000], # 0.8h F1:3.0235 F2@12th:-0.0074\n",
    "[ 16, 9, 3, 684, 2736, 192, 174.8, 3.0188, 14, 3.0188, 3.0231, 2.6957, 10000, 0.0000], # 0.7h F1:3.0223 F2@14th:-0.0035 # 12.4G\n",
    "[ 16, 9, 3, 648, 2592, 256, 177.0, 3.0213, 11, 3.0231, 3.0237, 2.7020, 10000, 0.0000], # 0.8h F1:3.0235 F2@11th:-0.0022\n",
    "[ 16, 9, 3, 648, 2592, 224, 169.0, 3.0182,  8, 3.0185, 3.1006, 2.7216, 10000, 0.0000], # 0.7h F1:3.0265 F2@8th:-0.0083\n",
    "[ 16, 9, 3, 648, 2592, 192, 161.1, 3.0197, 15, 3.0209, 3.0239, 2.7096, 10000, 0.0000], # 0.7h F1:3.0234 F2@15th:-0.0037\n",
    "\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0275,  6, 3.0885, 3.0295, 2.7310, 11000, 0.0000], # 0.9h F1:3.0380 F2@5th:-0.0085 ## best-F1:3.0160@N11000,10.5,WD1 # keep>=272 otherwise already explored\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2,+3.0190,  8, 3.0197, 3.1172, 2.7061, 10000, 0.0000], # 0.8h F1:3.0261 F2@8th:-0.0071 # Best-F1-model again\n",
    "[ 16, 8, 4, 640, 2560, 272, 177.8, 3.0214, 11, 3.0227, 3.0247, 2.7107, 10000, 0.0000], # 0.8h F1:3.0241 F2@11th:-0.0027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b9675b6-96e0-4835-8092-b24156e50ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.44\n",
      "L16 att8 kv_heads4 hidden704 intermediate2816 head_dim272 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.997200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.9972359375 tensor(3.1589)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.812800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.8128458984375 tensor(3.0554)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.751700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7517478515625 tensor(3.0392)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.729700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.729677734375 tensor(3.0316)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.715100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7150673828125 tensor(3.0282)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.707000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7070177734375 tensor(3.0261)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.149300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 3.149325 tensor(3.1172)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.706052734375 tensor(3.0190)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 53:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.697172265625 tensor(3.0197)\n",
      "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0190, 8, 3.0197, 3.1172, 2.7061, 10000, 0.0000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 272 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=10000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# F2(F1) criteria style\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100; flag=0; criteria=0.0005 * 0 # * 4 * 4 * 4 # count = all, 5, 3~4, 2\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if flag==1: # Forgetter_2\n",
    "        print(\"F2\"); del model; torch.cuda.empty_cache(); model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    model.eval(); loss = []; B2=12 # Evaluation follows\n",
    "    for k in range(6000): # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); del common; torch.cuda.empty_cache(); model.train();\n",
    "    if loss_current < 3.0182:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= loss_prev) and (flag==1):\n",
    "        break\n",
    "    if (loss_current >= (loss_prev-criteria)) and (flag==0):\n",
    "        flag=1\n",
    "        if loss_current >= loss_prev:\n",
    "            model.load_state_dict(torch.load('test.pth')) # back to previous\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss; torch.save(model.state_dict(), 'test.pth'); # always save\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0855088-c618-41a9-ab26-c12654382fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L16 Forgetter, continued from the saved 3.0160 model and only the last drop (sleep 2) was executed (default: x4 lr10.5 N11000)\n",
    "\n",
    "[+16, 8, 4, 704, 2816, 272, 204.2,+3.0160, 9, 3.0174, 3.0180, 2.7260, 11000], # 0.9h 13.3G minGemma-hidden_layers16-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-17-17-18\n",
    "\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0143, 1, 3.0156,  10000, 2.7196, 12000], # continued lr10.5e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0150, 1, 3.0167,  10000, 2.7437, 11000], # continued lr11.0e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0141, 1, 3.0155,  10000, 2.7269, 11000], # continued lr10.5e-4 WD1.0 original F1 param, lr10.5 seems best\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0153,12, 3.0161, 3.0164, 2.7407, 11000], # continued lr10.0e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0110, 2, 3.0123, 3.0117, 2.7298, 10000], # continued lr10.5e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0112, 1, 3.0120,  10000, 2.7380,  9000], # continued lr10.5e-4 WD1.0\n",
    "[+16, 8, 4, 704, 2816, 272, 204.2,+3.0087, 1, 3.0105,  10000, 2.7004,  9000], # continued lr10.5e-4 WD0.9  minGemma-hidden_layers16-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-07-21-20-23.pth\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0205, 6, 3.0241, 3.0236, 2.7291,  9000], # continued lr 9.5e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2,+3.0095, 1, 3.0107,  10000, 2.6978,  9000], # continued lr 9.5e-4 WD1.0\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0097, 1, 3.0109,  10000, 2.6903,  9000], # continued lr 9.5e-4,WD0.9\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0126, 1, 3.0173,  10000, 2.6240,  9000], # continued lr 9.5e-4,WD0.8\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0098, 1, 3.0142,  10000, 2.6171,  9000], # continued lr 8.5e-4,WD0.9\n",
    "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0109, 1, 3.0130,  10000, 2.6603,  8500], # continued lr 9.5e-4,WD0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc3719c-f50d-41c3-a4bc-cc570e3fb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.296\n",
      "L16 att8 kv_heads4 hidden704 intermediate2816 head_dim272 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 48:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.700400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.7004383680555555 tensor(3.0087)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 48:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.704600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.7046482204861113 tensor(3.0105)\n",
      "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0087, 1, 3.0105, 10000.0000, 2.7004, 9000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 272 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=9000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "#\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers16-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-17-17-18.pth'))\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter2 with save & load\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if ep > 0: # Forgetter_2-0\n",
    "        model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4, weight_decay=0.9, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "    for k in range(6000): # 6000 # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "    if loss_current < 3.0160:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= (loss_prev-0.0005*0)): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "        break\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "    torch.save(model.state_dict(), 'test.pth'); del model; torch.cuda.empty_cache(); # Forgetter_2-0\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23688b38-b830-4ee1-9887-b058be19ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L16 Forgetter # x4 lr10.5 N11000\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.0202, 9, 3.0203, 3.0211, 2.7204, 11000], # 0.9h\n",
    "[ 16, 8, 4, 768, 3072, 224, 218.0, 3.0267, 6, 3.0269, 3.0293, 2.7130, 11000], # 0.8h 13.3G\n",
    "[ 16, 8, 4, 768, 3072, 192, 208.6, 3.0234, 6, 3.0267, 3.0245, 2.7251, 11000], # 0.8h 13.0G\n",
    "[ 16, 8, 4, 768, 3072, 160, 199.2, 3.0242, 6, 3.0246, 3.0279, 2.7273, 11000], # 0.7h\n",
    "\n",
    "[ 16, 8, 4, 736, 2944, 256, 213.5, 3.0211, 8, 3.0220, 3.0224, 2.7149, 11000], # 0.9h 13.4G\n",
    "[ 16, 8, 4, 736, 2944, 224, 204.4, 3.0240, 7, 3.0310, 3.0244, 2.7225, 11000], # 0.8h\n",
    "[ 16, 8, 4, 736, 2944, 192, 195.4, 3.0304, 5, 3.0316, 3.0322, 2.7259, 11000], # 0.8h\n",
    "[ 16, 8, 4, 736, 2944, 160, 186.3, 3.0189,15, 3.0198, 3.0198, 2.7498, 11000], # 0.7h 12.8G\n",
    "\n",
    "[+16, 8, 4, 704, 2816, 272, 204.2,+3.0160, 9, 3.0174, 3.0180, 2.7260, 11000], # 0.9h 13.3G minGemma-hidden_layers16-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-17-17-18.pth\n",
    "[ 16, 8, 4, 704, 2816, 256, 199.9, 3.0194, 9, 3.0199, 3.0207, 2.7278, 11000], # 0.8h 13.1G\n",
    "[ 16, 8, 4, 704, 2816, 224, 191.2, 3.0219, 8, 3.0225, 3.0238, 2.7134, 11000], # 0.8h 12.9G\n",
    "[ 16, 8, 4, 704, 2816, 192, 182.6, 3.0253, 8, 3.0257, 3.0288, 2.7369, 11000], # 0.7h\n",
    "[ 16, 8, 4, 704, 2816, 160, 173.9, 3.0350, 4, 3.0586, 3.0409, 2.7603, 11000], # 0.7h 12.7G\n",
    "\n",
    "[ 16, 8, 4, 672, 2688, 256, 186.7, 3.0224,11, 3.1106, 3.0229, 2.7256, 11000], # 0.8h\n",
    "[ 16, 8, 4, 672, 2688, 224, 178.4, 3.0289, 7, 3.0956, 3.0296, 2.7260, 11000], # 0.8h\n",
    "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.0253, 8, 3.0263, 3.0261, 2.7216, 11000], # 0.7h\n",
    "[ 16, 8, 4, 672, 2688, 160, 161.9, 3.0410, 3, 3.1052, 3.0547, 2.7614, 11000], # 0.7h 12.2G\n",
    "\n",
    "[ 16, 8, 4, 640, 2560, 256, 173.8, 3.0337, 5, 3.0883, 3.0364, 2.7367, 11000], # 0.8h\n",
    "[ 16, 8, 4, 640, 2560, 224, 166.0, 3.0241, 9, 3.0243, 3.0253, 2.7437, 11000], # 0.7h\n",
    "[ 16, 8, 4, 640, 2560, 192, 158.1, 3.0277,12, 3.1098, 3.0281, 2.7092, 11000], # 0.7h\n",
    "[ 16, 8, 4, 640, 2560, 160, 150.2, 3.0297, 6, 3.0298, 3.0327, 2.7566, 11000], # 0.6h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "581540db-91b8-4fa9-8b90-0d35981f66fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L16 att8 kv_heads4 hidden704 intermediate2816 head_dim272 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.958700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.9586761363636365 tensor(3.1450)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.827800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.827818181818182 tensor(3.0508)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.770600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7705724431818184 tensor(3.0317)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.747900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.747916015625 tensor(3.0241)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.737300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.737291370738636 tensor(3.0213)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.729500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7294877485795452 tensor(3.0199)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.727200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7271986860795456 tensor(3.0184)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.725600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.7256147017045453 tensor(3.0180)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.726000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.7260015980113637 tensor(3.0160)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 58:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.726100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.726116122159091 tensor(3.0174)\n",
      "[ 16, 8, 4, 704, 2816, 272, 204.2, 3.0160, 9, 3.0174, 3.0180, 2.7260, 11000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 272 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 103.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "        for k in range(6000): ## std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0162:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3e9b9-fdda-4eb9-b4ef-6bf1cf411959",
   "metadata": {},
   "source": [
    "# L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce358d49-4ae1-4ffd-996b-3513292c2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L12 Forgetter, continued from the saved 3.0165 model and only the last drop (sleep 2) was executed (default: lr10e-4 WD1)\n",
    "\n",
    "[+12,12, 4, 768, 3072, 192, 180.3,+3.0094,19, 3.0102, 3.0181, 2.6899, 10000, 0.0000], # 0.7h F1:3.0165 F2@19th:-0.0071 # F1:3.0165 minGemma-hidden_layers12-att_heads12-kv_heads4-hidden768-intermediate3072-head_dim192-T512--2025-06-25-03-30.pth # F2@19th minGemma-hidden_layers12-att_heads12-kv_heads4-hidden768-intermediate3072-head_dim192-T512--2025-06-25-05-11.pth\n",
    "\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0103, 1, 3.0130,  10000, 2.6453,  9000], # continued lr9.5e-4 WD0.9\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0108, 1, 3.0119,  10000, 2.6865, 10500], # continued lr10e-4 WD1.0\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0110, 1, 3.0132,  10000, 2.6901, 10000], # continued lr10e-4 WD1.0 (=again)\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0100, 1, 3.0127,  10000, 2.7224, 10000], # continued lr10e-4 WD1.1\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0135, 1, 3.0152,  10000, 2.6548, 10000], # continued lr10e-4 WD0.9\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0100, 1, 3.0110,  10000, 2.7111,  9500], # continued lr10e-4 WD1.0\n",
    "[+12,12, 4, 768, 3072, 192, 180.3,+3.0071, 1, 3.0082,  10000, 2.6972,  9000], # continued lr10e-4 WD1.0\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0119, 1, 3.0121,  10000, 2.7096, 10000], # continued lr10.5e-4 WD1.0\n",
    "[ 12,12, 4, 768, 3072, 192, 180.3, 3.0104, 1, 3.0122,  10000, 2.6690, 10000], # continued lr9.5e-4 WD1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e9f46c-6526-4dab-8f95-f4c653afd229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.44\n",
      "L12 att12 kv_heads4 hidden768 intermediate3072 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.129700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.129701171875 tensor(3.1561)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.805000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.80498515625 tensor(3.0575)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.743200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.74321171875 tensor(3.0406)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.718600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7185765625 tensor(3.0340)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.705100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7051029296875 tensor(3.0302)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.697700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.6976892578125 tensor(3.0292)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.691700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.6916533203125 tensor(3.0254)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.688362109375 tensor(3.0236)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.685000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.684952734375 tensor(3.0229)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.683300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.6832689453125 tensor(3.0210)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.683200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.6831814453125 tensor(3.0198)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.681200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.6812123046875 tensor(3.0192)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.679800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.6797990234375 tensor(3.0183)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.680700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.6806845703125 tensor(3.0177)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.680400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2.68040078125 tensor(3.0177)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.679400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2.679430859375 tensor(3.0174)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.680500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2.680506640625 tensor(3.0165)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.680300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 2.6802845703125 tensor(3.0181)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.689900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2.68989140625 tensor(3.0094)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 43:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.690600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2.6906326171875 tensor(3.0102)\n",
      "[ 12, 12, 4, 768, 3072, 192, 180.3, 3.0094, 19, 3.0102, 3.0181, 2.6899, 10000, 0.0000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  12 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =12 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*64 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=10000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# F2(F1) criteria style\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100; flag=0; criteria=0.0005 * 0 # * 4 * 4 * 4 # count = all, 5, 3~4, 2\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if flag==1: # Forgetter_2\n",
    "        print(\"F2\"); del model; torch.cuda.empty_cache(); model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    model.eval(); loss = []; B2=12 # Evaluation follows\n",
    "    for k in range(6000): # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); del common; torch.cuda.empty_cache(); model.train();\n",
    "    if loss_current < 3.0228: # for L12 # 3.0103:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= loss_prev) and (flag==1):\n",
    "        break\n",
    "    if (loss_current >= (loss_prev-criteria)) and (flag==0):\n",
    "        flag=1\n",
    "        if loss_current >= loss_prev:\n",
    "            model.load_state_dict(torch.load('test.pth')) # back to previous\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss; torch.save(model.state_dict(), 'test.pth'); # always save\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e5db03-86f5-49cb-868c-069d05be8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.296\n",
      "L12 att12 kv_heads4 hidden768 intermediate3072 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 39:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.697152126736111 tensor(3.0071)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 39:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.693400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.6933561197916664 tensor(3.0082)\n",
      "[ 12, 12, 4, 768, 3072, 192, 180.3, 3.0071, 1, 3.0082, 10000.0000, 2.6972, 9000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  12 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =12 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*64 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=9000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device);\n",
    "#\n",
    "model.load_state_dict(torch.load('minGemma-hidden_layers12-att_heads12-kv_heads4-hidden768-intermediate3072-head_dim192-T512--2025-06-25-03-30.pth'))\n",
    "#\n",
    "print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter2 with save & load\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if ep > 0: # Forgetter_2-0\n",
    "        model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    loss = []; model.eval(); B2=12 # Evaluation follows\n",
    "    for k in range(6000): # 6000 # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "    if loss_current < 3.0165:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= (loss_prev-0.0005*0)): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "        break\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "    torch.save(model.state_dict(), 'test.pth'); del model; torch.cuda.empty_cache(); # Forgetter_2-0\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74f50f-080d-440a-94f8-7abd192e0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L12 Forgetter without last drop (no sleep2 in the end) (default: lr10e-4 WD1)\n",
    "\n",
    "# B12 fixed, learning-rate searched\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0374, 6, 3.0377, 3.0381, 2.7919, 11000], # lr12e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0365, 7, 3.0367, 3.0373, 2.7489, 11000], # lr11e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0338, 6, 3.0350, 3.0340, 2.6562, 11000], # lr9e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0500, 4, 3.0506, 3.0504, 2.6309, 11000], # lr8e-4\n",
    "\n",
    "# B16 fixed\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0747, 3, 3.0783, 3.0754, 2.5920, 12000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0779, 3, 3.0840, 3.0824, 2.5901, 11000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0619, 3, 3.0630, 3.0628, 2.6056, 10000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0554, 3, 3.0564, 3.0713, 2.6296,  9000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0492, 5, 3.0527, 3.0493, 2.5844,  8000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0449, 6, 3.0455, 3.0463, 2.5864,  7000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0466, 5, 3.0467, 3.0505, 2.6243,  6000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0409, 9, 3.0410, 3.0418, 2.5880,  5000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0571, 9, 3.0577, 3.0597, 2.6159,  4000],\n",
    "\n",
    "# B14 fixed\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0438,  6, 3.0444, 3.0483, 2.6964, 12000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0446,  4, 3.0463, 3.0478, 2.6509, 11000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0495,  3, 3.0509, 3.0598, 2.6758, 10000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0417,  4, 3.0427, 3.0450, 2.6602,  9500],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0,+3.0247, 10, 3.0262, 3.0256, 2.6267,  9000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0416,  5, 3.0417, 3.0451, 2.6567,  8500],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0,+3.0258, 10, 3.0529, 3.0274, 2.6312,  8000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0340,  9, 3.0341, 3.0346, 2.6420,  7500],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0338,  9, 3.0339, 3.0352, 2.6757,  7000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0286, 13, 3.0296, 3.0308, 2.6291,  6000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0451, 13, 3.0451, 3.0460, 2.6391,  5000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0490, 16, 3.0506, 3.0505, 2.6451,  4000],\n",
    "\n",
    "# B12 fixed (lr10e-4 default) # BEST\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0291, 12, 3.0300, 3.0307, 2.6962, 13000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0270, 12, 3.0271, 3.0279, 2.7115, 12000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0279,  7, 3.0287, 3.0295, 2.7258, 11500],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0365,  6, 3.0368, 3.0365, 2.7165, 11400],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0388,  6, 3.0397, 3.0408, 2.7134, 11300],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0386,  7, 3.0397, 3.0390, 2.7184, 11200],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0352, 12, 3.0382, 3.0363, 2.7047, 11100],\n",
    "[+12, 6, 3, 792, 3168, 256, 174.0,+3.0228, 10, 3.0234, 3.0235, 2.7091, 11000], # minGemma-hidden_layers12-att_heads6-kv_heads3-hidden792-intermediate3168-head_dim256-T512--2025-03-16-21-31.pth\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0269,  8, 3.0277, 3.0280, 2.6930, 11000], # again\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0355,  7, 3.0363, 3.0361, 2.7088, 10900],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0356,  7, 3.0363, 3.0357, 2.7055, 10800],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0380,  7, 3.0382, 3.0390, 2.7263, 10700],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0382,  6, 3.0384, 3.0388, 2.7116, 10600],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0,+3.0238, 12, 3.0249, 3.0241, 2.7351, 10500],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0341, 10, 3.0342, 3.0355, 2.7100, 10400],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0379,  6, 3.0382, 3.0381, 2.7152, 10300],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0342, 11, 3.0347, 3.0350, 2.7033, 10200],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0395,  6, 3.0397, 3.0420, 2.7391, 10100],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0333,  8, 3.0341, 3.0353, 2.7104, 10000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0305,  7, 3.0317, 3.0318, 2.7098, 10000], # again\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0406,  6, 3.0406, 3.0418, 2.7252,  9700],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0266, 10, 3.1093, 3.0269, 2.7153,  9500], # spike at last\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0384, 10, 3.0387, 3.0402, 2.7137,  9200],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0261, 13, 3.0261, 3.0277, 2.6979,  9000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0355, 11, 3.0363, 3.0356, 2.7089,  8800],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0392,  8, 3.0392, 3.0404, 2.7185,  8500],\n",
    "\n",
    "# B10 fixed\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0306,  5, 3.0399, 3.0333, 2.7852, 15000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0378,  7, 3.0551, 3.0399, 2.8134, 14000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0398,  6, 3.0589, 3.0434, 2.7949, 13000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0348, 13, 3.0802, 3.0360, 2.8021, 12000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0345, 12, 3.0600, 3.0350, 2.7951, 11000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0360, 12, 3.0828, 3.0367, 2.8193, 10000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0341,  9, 3.0364, 3.0354, 2.7976,  9000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0383, 14, 3.0393, 3.0389, 2.7945,  8000],\n",
    "\n",
    "# B8 fixed\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0537,  6, 3.0537, 3.0559, 2.9141, 17000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0483,  8, 3.0493, 3.0496, 2.8781, 16000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0745,  3, 3.0879,  10000, 2.9418, 15000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0576,  6, 3.0764, 3.0616, 2.8964, 14000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0545,  7, 3.0927, 3.0549, 2.9026, 13000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0571, 10, 3.0579, 3.0582, 2.8903, 12000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0609,  7, 3.0611, 3.0616, 2.9186, 11000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0730, 10, 3.0845, 3.0742, 2.9035, 10000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0828,  6, 3.1327, 3.0893, 2.9285,  9000],\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0738, 11, 3.0745, 3.0756, 2.9060,  8000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0031ef-7017-4a47-aa26-641c316592f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.584\n",
      "L12 att6 kv_heads3 hidden792 intermediate3168 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.201100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.806900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.806916015625 tensor(3.0509)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.752200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7521821732954543 tensor(3.0377)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.768600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.768578835227273 tensor(3.0333)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 47:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.743100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7431017400568183 tensor(3.0306)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.734000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.734017400568182 tensor(3.0283)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 47:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.731700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7317365056818184 tensor(3.0268)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.711300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.711272372159091 tensor(3.0258)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.726000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.726023615056818 tensor(3.0235)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 47:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.709100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.709052734375 tensor(3.0228)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11000' max='11000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11000/11000 46:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.7710454545454546 tensor(3.0234)\n",
      "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.0228, 10, 3.0234, 3.0235, 2.7091],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  12 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*132 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=11000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep):\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 3.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=18 # Evaluation follows\n",
    "        for k in range(4000): # std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0228:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e0ef2-3e6a-476c-82d7-363279894e26",
   "metadata": {},
   "source": [
    "# L10 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52cb20-e0b0-465e-96c9-30b8dbd37cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L10 Forgetter Model without last drop (no sleep2 in the end) (default: lr10e-4 WD1)\n",
    "\n",
    "# search N for a small model\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0265, 19, 3.0278, 3.0281, 2.6936, 15000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0369,  5, 3.0373, 3.0403, 2.7071, 14000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0370,  6, 3.0370, 3.0392, 2.7033, 13000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0331,  7, 3.0332, 3.0339, 2.6898, 12800],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0378,  6, 3.0404, 3.0403, 2.7054, 12600],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0347,  6, 3.0351, 3.0384, 2.7098, 12400],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0370,  7, 3.0370, 3.0384, 2.6943, 12200],\n",
    "[+10, 8, 4, 800, 3200, 256, 166.2,+3.0219, 11, 3.0224, 3.0228, 2.6944, 12000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2,+3.0244, 13, 3.0247, 3.0248, 2.6913, 12000], # again\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.3438,  6, 3.4328, 3.3952, 2.5164, 12000], # lr10e-5\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0285, 13, 3.0285, 3.0288, 2.7519, 11800],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0407,  4, 3.0478, 3.0449, 2.7304, 11600],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0289, 12, 3.0289, 3.0298, 2.6917, 11400],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0269,  9, 3.0276, 3.0281, 2.6980, 11200],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0294, 15, 3.0593, 3.0300, 2.6885, 11000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0241, 14, 3.0245, 3.0248, 2.6942, 10000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0320, 10, 3.0321, 3.0330, 2.7033,  9000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0320,  9, 3.0322, 3.0333, 2.7147,  8000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0311, 11, 3.0316, 3.0329, 2.7122,  7000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0384, 19, 3.0397, 3.0395, 2.7170,  6000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0449, 15, 3.0452, 3.0450, 2.7184,  5000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ddab72f-b985-4151-b60a-93c28ec30afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.728\n",
      "L10 att8 kv_heads4 hidden800 intermediate3200 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.930100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.795300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.795304361979167 tensor(3.0557)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.744400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.7443707682291665 tensor(3.0427)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.724900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7248570963541665 tensor(3.0385)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.712700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.7127286783854165 tensor(3.0345)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.705600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.70558984375 tensor(3.0321)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.701400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.70136865234375 tensor(3.0314)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.698500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.6984703776041665 tensor(3.0297)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.694800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.69481884765625 tensor(3.0277)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.694000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.6940096028645835 tensor(3.0268)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.692200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.6921715494791667 tensor(3.0261)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.691700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.6917140299479168 tensor(3.0248)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.691300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.6913180338541665 tensor(3.0244)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 54:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.701100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.7010777994791666 tensor(3.0247)\n",
      "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.0244, 13, 3.0247, 3.0248, 2.6913],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers = 10 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=12000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "# just check N10000 again\n",
    "# Forgetter with reliable hand-eval\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep):\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 3.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=18 # Evaluation follows\n",
    "        for k in range(4000): # std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0231:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2987ef-e3ca-43b5-b24e-a63d28aa82af",
   "metadata": {},
   "source": [
    "# L9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd01c5c2-f247-4abc-88b9-777bc994ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L9 Forgetter with last drop (sleep2 in the end)   # F2(F1_criteria())\n",
    "\n",
    "[ 9, 16, 4, 832, 3328, 256, 193.3, 3.0239, 15, 3.0244, 3.0290, 2.7176, 10000, 0.0000], # F1:3.0279 F2@15th:-0.0040 TOO big\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.0240, 16, 3.0240, 3.0251, 2.7132, 10000, 0.0005], # F1:3.03?? F2@ 9th:-0.0050 TOO big\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.0180, 17, 3.0204, 3.0249, 2.7148, 10000, 0.0000], #+F1:3.0239 F2@17th:-0.0059 TOO big\n",
    "[ 9, 16, 4, 832, 3328, 240, 188.5, 3.0240, 10, 3.0246, 3.0286, 2.7265, 10000, 0.0000], # F1:3.0282 F2@10th:-0.0042\n",
    "[ 9, 16, 4, 832, 3328, 192, 174.2, 3.0254, 10, 3.0254, 3.0301, 2.7254, 10000, 0.0000], # F1:3.0295 F2@10th:-0.0041\n",
    "[ 9, 16, 4, 832, 3328, 176, 169.4, 3.0255, 11, 3.0266, 3.0310, 2.7142, 10000, 0.0000], # F1:3.0306 F2@11th:-0.0051\n",
    "\n",
    "[ 9, 16, 4, 800, 3200, 224, 173.9, 3.0309, 11, 3.0319, 3.0340, 2.7686, 10000, 0.0000], # F1:3.0339 F2@11th:-0.0030 12.3G\n",
    "\n",
    "[ 9, 16, 4, 768, 3072, 256, 173.2, 3.0197, 16, 3.0207, 3.0214, 2.7128, 10000, 0.0000], # F1:3.0324 F2@ 9th:-0.0061\n",
    "[ 9, 16, 4, 768, 3072, 240, 168.7, 3.0231, 14, 3.0232, 3.0277, 2.7161, 10000, 0.0000], # F1:3.0273 F2@14th:-0.0042\n",
    "[ 9, 16, 4, 768, 3072, 224, 164.3,+3.0153, 18, 3.0170, 3.0228, 2.7180, 10000, 0.0000], #+F1:3.0226 F2@18th:-0.0073  minGemma-hidden_layers9-att_heads16-kv_heads4-hidden768-intermediate3072-head_dim224-T512--2025-06-23-16-07.pth\n",
    "[ 9, 16, 4, 768, 3072, 208, 159.9, 3.0326, 10, 3.0331, 3.0330, 2.7261, 10000, 0.0000], # F1:3.0350 F2@ 9th:-0.0020\n",
    "[ 9, 16, 4, 768, 3072, 192, 155.5, 3.0272, 14, 3.0274, 3.0281, 2.7088, 10000, 0.0000], # F1:3.0336 F2@11th:-0.0047\n",
    "\n",
    "[ 9, 16, 4, 736, 2944, 224, 154.9, 3.0183, 18, 3.0190, 3.0247, 2.7166, 10000, 0.0000], # F1:3.0243 F2@18th:-0.0060 0.7h 10.7G\n",
    "\n",
    "[ 9, 16, 4, 704, 2816, 256, 153.9, 3.0243, 13, 3.0251, 3.0299, 2.7302, 10000, 0.0000], # F1:3.0281 F2@13th:-0.0038\n",
    "[ 9, 16, 4, 704, 2816, 240, 149.8, 3.0240, 14, 3.0248, 3.0285, 2.7235, 10000, 0.0000], # F1:3.0278 F2@14th:-0.0038\n",
    "[+9, 16, 4, 704, 2816, 224, 145.7,+3.0152, 19, 3.0166, 3.0219, 2.7185, 10000, 0.0000], #+F1:3.0209 F2@19th:-0.0057  minGemma-hidden_layers9-att_heads16-kv_heads4-hidden704-intermediate2816-head_dim224-T512--2025-07-05-14-02.pth\n",
    "[ 9, 16, 4, 704, 2816, 208, 141.7, 3.0237, 14, 3.0246, 3.0276, 2.7213, 10000, 0.0000], # F1:3.0274 F2@14th:-0.0037\n",
    "[ 9, 16, 4, 704, 2816, 192, 137.6, 3.0312,  9, 3.0322, 3.0328, 2.7156, 10000, 0.0000], # F1:3.0378 F2@ 8th:-0.0050\n",
    "[ 9, 16, 4, 704, 2816, 176, 133.6, 3.0236, 14, 3.0241, 3.0300, 2.7168, 10000, 0.0000], # F1:3.0298 F2@14th:-0.0062\n",
    "\n",
    "[ 9, 16, 4, 672, 2688, 224, 136.8, 3.0267, 16, 3.0274, 3.0305, 2.7222, 10000, 0.0000], # F1:3.0299 F2@16th:-0.0032\n",
    "\n",
    "[ 9, 16, 4, 640, 2560, 256, 135.4, 3.0252, 14, 3.0269, 3.0293, 2.7372, 10000, 0.0000], # F1:3.0293 F2@14th:-0.0041\n",
    "[ 9, 16, 4, 640, 2560, 240, 131.8, 3.0279, 13, 3.0283, 3.0312, 2.7316, 10000, 0.0000], # F1:3.0306 F2@13th:-0.0027\n",
    "[ 9, 16, 4, 640, 2560, 224, 128.1, 3.0268, 13, 3.0280, 3.0278, 2.7229, 10000, 0.0000], # F1:3.0330 F2@12th:-0.0052\n",
    "[ 9, 16, 4, 640, 2560, 208, 124.4, 3.0270, 14, 3.0288, 3.0309, 2.7251, 10000, 0.0000], # F1:3.0307 F2@14th:-0.0037\n",
    "[ 9, 16, 4, 640, 2560, 192, 120.7, 3.0305, 15, 3.0323, 3.0371, 2.7298, 10000, 0.0000], # F1:3.0367 F2@15th:-0.0062\n",
    "[ 9, 16, 4, 640, 2560, 176, 117.0, 3.0360, 10, 3.0364, 3.0435, 2.7385, 10000, 0.0000], # F1:3.0433 F2@10th:-0.0073 10.9G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0880c263-fd5c-49a8-8fee-1c96b79abde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.44\n",
      "L9 att16 kv_heads4 hidden704 intermediate2816 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.019800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.01977421875 tensor(3.1532)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.836000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.836011328125 tensor(3.0680)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.771200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.771162109375 tensor(3.0489)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.745900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7459267578125 tensor(3.0404)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.733200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.73321796875 tensor(3.0361)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.725400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7254123046875 tensor(3.0337)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.720400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.72037890625 tensor(3.0314)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.716700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.71665859375 tensor(3.0289)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.713900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.7138525390625 tensor(3.0275)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.712600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7126388671875 tensor(3.0269)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.711600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.711623828125 tensor(3.0254)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.708900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.708865625 tensor(3.0253)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.707600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.70755234375 tensor(3.0246)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.7066607421875 tensor(3.0228)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2.706885546875 tensor(3.0222)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2.7068376953125 tensor(3.0210)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.706300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2.7063240234375 tensor(3.0209)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.707200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 2.7072279296875 tensor(3.0219)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.718500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2.7184564453125 tensor(3.0152)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 41:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.713600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2.7135705078125 tensor(3.0166)\n",
      "[ 9, 16, 4, 704, 2816, 224, 145.7, 3.0152, 19, 3.0166, 3.0219, 2.7185, 10000, 0.0000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   9 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =16 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*44 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=10000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# F2(F1) criteria style\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100; flag=0; criteria=0.0005 * 0 # * 4 * 4 * 4 # count = all, 5, 3~4, 2\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if flag==1: # Forgetter_2\n",
    "        print(\"F2\"); del model; torch.cuda.empty_cache(); model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4 * (Max_ep-ep*0)/Max_ep, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    model.eval(); loss = []; B2=12 # Evaluation follows\n",
    "    for k in range(6000): # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); del common; torch.cuda.empty_cache(); model.train();\n",
    "    if loss_current < 3.0203: # for L9 # 3.0103:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= loss_prev) and (flag==1):  # BUG FIXED\n",
    "        break\n",
    "    if (loss_current >= (loss_prev-criteria)) and (flag==0):   ### modified here # if you simply want to repeat, use this code\n",
    "        flag=1\n",
    "        if loss_current >= loss_prev:\n",
    "            model.load_state_dict(torch.load('test.pth')) # back to previous\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss; torch.save(model.state_dict(), 'test.pth'); # always save\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b6502c-13da-48b9-9aef-60ebe38ce56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.44\n",
      "L9 att16 kv_heads4 hidden768 intermediate3072 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.150200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.1502 tensor(3.1515)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.822500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.822476171875 tensor(3.0611)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.774800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.77481796875 tensor(3.0431)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.752100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.75209296875 tensor(3.0370)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.732700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.73267734375 tensor(3.0343)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.727500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.72747109375 tensor(3.0321)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.722300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.722309765625 tensor(3.0298)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.71736171875 tensor(3.0285)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.714600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.7146203125 tensor(3.0273)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.712800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7128224609375 tensor(3.0267)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.710900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.7108927734375 tensor(3.0246)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.709000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.7090341796875 tensor(3.0242)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.708100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.708077734375 tensor(3.0237)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.707400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.7073892578125 tensor(3.0235)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.705500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2.7054658203125 tensor(3.0233)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 2.7054048828125 tensor(3.0226)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.709800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 2.709770703125 tensor(3.0228)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.718000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 2.7179869140625 tensor(3.0153)\n",
      "F2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 44:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.713500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2.7134517578125 tensor(3.0170)\n",
      "[ 9, 16, 4, 768, 3072, 224, 164.3, 3.0153, 18, 3.0170, 3.0228, 2.7180, 10000, 0.0000],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   9 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =16 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*48 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=10000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# F2(F1) criteria style\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100; flag=0; criteria=0.0005 * 0 # * 4 * 4 * 4 # count = all, 5, 3~4, 2\n",
    "for ep in range(Max_ep): #13.5e-4 or 10e-4\n",
    "    if flag==1: # Forgetter_2\n",
    "        print(\"F2\"); del model; torch.cuda.empty_cache(); model = minGemma().to(device); model.load_state_dict(torch.load('test.pth'))\n",
    "    training_args = TrainingArguments(learning_rate=10.5e-4 * (Max_ep-ep*0)/Max_ep, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    model.eval(); loss = []; B2=12 # Evaluation follows\n",
    "    for k in range(6000): # std=0.0056 for 1000 with 89sec #5min\n",
    "        val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "        loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "    loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); del common; torch.cuda.empty_cache(); model.train();\n",
    "    if loss_current < 3.0203: # for L9 # 3.0103:\n",
    "        torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "    if (loss_current >= loss_prev) and (flag==1):  # BUG FIXED\n",
    "        break\n",
    "    if (loss_current >= (loss_prev-criteria)) and (flag==0):   ### modified here # if you simply want to repeat, use this code\n",
    "        flag=1\n",
    "        if loss_current >= loss_prev:\n",
    "            model.load_state_dict(torch.load('test.pth')) # back to previous\n",
    "    loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss; torch.save(model.state_dict(), 'test.pth'); # always save\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}, {criteria:.4f}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c0f6b-73d1-476a-bb90-7644f4bd9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (LEGACY) # L9 Forgetter without last drop (no sleep2 in the end) (default: WD1 lr10e-4)\n",
    "\n",
    "# search N for a small model\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0360, 13, 3.0871, 3.0371, 2.6903, 16000],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0317, 11, 3.0318, 3.0329, 2.7007, 11000],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0311, 13, 3.0315, 3.0322, 2.7130, 10700],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0351, 10, 3.0359, 3.0361, 2.7015, 10600],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0300, 13, 3.0306, 3.0308, 2.6919, 10500],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0294, 11, 3.0297, 3.0301, 2.6980, 10400],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0282, 10, 3.0282, 3.0296, 2.7054, 10350],\n",
    "[+9, 8, 4, 832, 3328, 256, 162.7,+3.0264, 18, 3.0274, 3.0272, 2.6910, 10300],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0318,  7, 3.0330, 3.0352, 2.7121, 10250],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0267, 11, 3.0269, 3.0284, 2.6942, 10200],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0295, 11, 3.0301, 3.0301, 2.6976, 10100],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0295, 12, 3.0304, 3.0300, 2.6986, 10000],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0335, 10, 3.0335, 3.0341, 2.7258,  9900],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0330, 11, 3.0331, 3.0339, 2.7022,  9800],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0323, 11, 3.0326, 3.0335, 2.7096,  9500],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0321, 11, 3.0331, 3.0349, 2.7101,  9200],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0356,  9, 3.0357, 3.0380, 2.7064,  9100],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0293, 12, 3.0297, 3.0304, 2.7030,  9000],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0280, 11, 3.0281, 3.0284, 2.7073,  8900],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0337, 13, 3.0341, 3.0347, 2.7457,  8800],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0336,  8, 3.0347, 3.0361, 2.7181,  8700],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0292, 13, 3.0297, 3.0310, 2.7047,  8600],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0300, 13, 3.0305, 3.0303, 2.7052,  8500],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0304, 12, 3.0649, 3.0319, 2.7158,  8400],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0335, 12, 3.0350, 3.0345, 2.7132,  8300],\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.0358,  8, 3.0361, 3.0385, 2.7200,  8000],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfc548-c276-4bc9-a373-ad381023daf3",
   "metadata": {},
   "source": [
    "# L8 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f5683-8669-4537-bc9b-9c325c1bf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L8 Forgetter without last drop (no sleep2 in the end) (default: WD1 lr10e-4)\n",
    "[ 8, 8, 4, 800, 3200, 320, 150.9,+3.0295, 12, 3.0299, 3.0303, 2.7036, 11000],\n",
    "[ 8, 8, 4, 800, 3200, 256, 141.0, 3.0306, 11, 3.0307, 3.0308, 2.7189, 11000],\n",
    "[ 8, 8, 4, 800, 3200, 192, 131.2, 3.0325,  9, 3.0342, 3.0356, 2.7060, 11000],\n",
    "[ 8, 8, 4, 768, 3072, 320, 142.5, 3.0304, 11, 3.0305, 3.0324, 2.7021, 11000],\n",
    "[ 8, 8, 4, 768, 3072, 256, 133.0, 3.0300, 13, 3.0308, 3.0310, 2.7021, 11000],\n",
    "[ 8, 8, 4, 768, 3072, 192, 123.6, 3.0339, 10, 3.0343, 3.0341, 2.7104, 11000],\n",
    "[ 8, 8, 4, 736, 2944, 320, 134.3, 3.0309, 13, 3.0313, 3.0326, 2.6987, 11000],\n",
    "[ 8, 8, 4, 736, 2944, 256, 125.2, 3.0307,  8, 3.0308, 3.0338, 2.7147, 11000],\n",
    "[ 8, 8, 4, 736, 2944, 192, 116.2, 3.0317, 14, 3.0319, 3.0319, 2.7078, 11000],\n",
    "\n",
    "# search N(=inter-sleep interval) for a small model\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0499, 13, 3.0505, 3.0520, 2.7348,  5000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0457, 15, 3.0458, 3.0469, 2.7308,  6000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0283, 14, 3.0288, 3.0304, 2.7140,  7000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0272, 11, 3.0274, 3.0289, 2.7226,  8000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0325, 10, 3.0326, 3.0330, 2.7190,  9000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0303, 12, 3.0303, 3.0306, 2.7081, 10000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0264,  9, 3.0274, 3.0296, 2.7119, 10200],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0264, 15, 3.0267, 3.0274, 2.7121, 10400],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0343,  8, 3.0345, 3.0374, 2.7160, 10600],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0283, 13, 3.0291, 3.0293, 2.7074, 10800],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0308,  9, 3.0309, 3.0322, 2.7138, 10900],\n",
    "[+8, 8, 4, 800, 3200, 480, 175.4,+3.0203, 14, 3.0210, 3.0216, 2.6994, 10950], # minGemma-hidden_layers8-att_heads8-kv_heads4-hidden800-intermediate3200-head_dim480-T512--2025-05-04-01-18.pth\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0213, 18, 3.0223, 3.0214, 2.7089, 11000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0260, 21, 3.0264, 3.0263, 2.7155, 11000], # again\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0316,  7, 3.0317, 3.0325, 2.7202, 11000], # again\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0339,  7, 3.0340, 3.0369, 2.7213, 11050],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0255, 11, 3.0261, 3.0256, 2.7021, 11100],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0318,  6, 3.0327, 3.0346, 2.7231, 11200],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0268, 15, 3.0269, 3.0275, 2.7057, 11400],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0309,  8, 3.0310, 3.0330, 2.7103, 11600],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0295,  7, 3.0307, 3.0327, 2.7151, 11800],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0259, 10, 3.0266, 3.0275, 2.7114, 12000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0264, 13, 3.0272, 3.0278, 2.6990, 13000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0303, 12, 3.0310, 3.0309, 2.6945, 14000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0384,  8, 3.0386, 3.0397, 2.7096, 15000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.3007,  7, 3.3967, 3.3023, 2.3937, 11000], # lr10e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3a0466-cc27-49c5-b754-f92c0a9a67f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.2768\n",
      "L8 att8 kv_heads4 hidden800 intermediate3200 head_dim480 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>4.093200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.093194206621004 tensor(3.1373)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.813200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2.8132446846461185 tensor(3.0571)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.755000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.754998037956621 tensor(3.0425)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.733400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.7333643692922376 tensor(3.0357)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.720400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.72039044663242 tensor(3.0318)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.712800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7127875285388128 tensor(3.0294)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.708100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.708111444063927 tensor(3.0271)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.704000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.7039752782534245 tensor(3.0263)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.701700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.7017369434931506 tensor(3.0254)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.701200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7011595676369864 tensor(3.0238)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.699000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.699006135844749 tensor(3.0224)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.699800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.6997533176369863 tensor(3.0220)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.697000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 2.696957227454338 tensor(3.0216)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.699400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2.6994283319063928 tensor(3.0203)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10950' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10950/10950 51:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>2.696600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 2.6965949628995434 tensor(3.0210)\n",
      "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.0203, 14, 3.0210, 3.0216, 2.6994],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   8 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 480 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=10950; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation without last drop (no sleep2 in the end)\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep):\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 4.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=18 # Evaluation follows\n",
    "        for k in range(4000): # std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.0180:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80ae4c0-e41c-475b-aaf7-40a202a482b2",
   "metadata": {},
   "source": [
    "# L6 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07477a-8b1a-4dbe-8929-81bbce83fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L6 Forgetter without last drop (no sleep2 in the end) (default: WD1 lr10e-4)\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0624,  6, 3.0633, 3.0648, 2.7113, 20000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0434,  9, 3.0437, 3.0438, 2.7142, 15000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0413, 13, 3.0423, 3.0424, 2.7203, 12000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0435, 10, 3.0438, 3.0446, 2.7185, 11000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0410, 12, 3.0413, 3.0420, 2.7203, 10500],\n",
    "[+6, 8, 4, 768, 3072, 256, 109.4,+3.0407, 12, 3.0411, 3.0409, 2.7164, 10000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0440, 13, 3.0440, 3.0446, 2.7216,  9500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0484, 10, 3.0490, 3.0499, 2.7331,  9000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0418, 11, 3.0419, 3.0425, 2.7351,  8000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0489, 19, 3.0490, 3.0492, 2.7279,  7000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0563, 13, 3.0568, 3.0580, 2.7389,  6000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0681, 11, 3.0682, 3.0703, 2.7538,  5000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ccdd84-c1c7-4fc0-a5fe-4202232e9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.72\n",
      "L6 att8 kv_heads4 hidden768 intermediate3072 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.697000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.091500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3.0914890625 tensor(3.1967)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.908600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2.90856171875 tensor(3.1233)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.845200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2.8452033203125 tensor(3.1006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.814100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2.814109765625 tensor(3.0887)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.794000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2.7939841796875 tensor(3.0819)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.780400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 2.7804451171875 tensor(3.0777)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2.770973828125 tensor(3.0735)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.763700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2.76374375 tensor(3.0718)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.757700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.7576623046875 tensor(3.0703)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.753800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2.75382109375 tensor(3.0681)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 14:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.750300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 2.7502673828125 tensor(3.0682)\n",
      "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.0681, 11, 3.0682, 3.0703, 2.7538],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   6 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*96 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=5000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Forgetter with reliable hand-evaluation\n",
    "loss_prev=10000.0; loss_prev_prev=loss_prev; tloss_prev=10000.0; Max_ep=100\n",
    "for ep in range(Max_ep):\n",
    "    training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./test', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "    result = trainer.train(); tloss=result[2][\"train_loss\"]\n",
    "    if tloss < 3.15: # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"];\n",
    "        loss = []; model.eval(); B2=18 # Evaluation follows\n",
    "        for k in range(4000): # std=0.0056 for 1000 with 89sec #5min\n",
    "            val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "            loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "        loss_current = torch.Tensor(loss).mean(); print(ep+1, tloss, loss_current); model.train(); del common; torch.cuda.empty_cache();\n",
    "        if loss_current < 3.03:\n",
    "            torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "        if (loss_current >= loss_prev): #(loss_current >= loss_prev_prev) and (loss_prev >= loss_prev_prev):\n",
    "            break\n",
    "        loss_prev_prev = loss_prev; loss_prev = loss_current; tloss_prev=tloss\n",
    "\n",
    "print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {loss_prev:.4f}, {ep}, {loss_current:.4f}, {loss_prev_prev:.4f}, {tloss_prev:.4f}, {N_step}],')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
