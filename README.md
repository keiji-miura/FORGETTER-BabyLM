# FORGETTER-BabyLM

The results (validated losses of next token prediction task) for training minGemma models with BabyLM10M or BabyLM100M dataset by using normal or FORGETTER learning algorithm.

(Three Major premises: Input token length = 512, GPT2 Tokenizer, no drop-out)

