{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53742c28-b4e5-41ad-9a51-ca187578fa81",
   "metadata": {},
   "source": [
    "# load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d47bb8-8830-4b41-a436-c150db2042d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab\\.conda\\envs\\minGemma2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L18 att8 kv_heads4 hidden576 intermediate2304 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Please choose a model (L48 Normal Baby10M / L22 FORGETTER Baby10M / L18 Normal Baby100M / L16 FORGETTER Baby100M )\n",
    "# num_hidden_layers = 48; num_attention_heads = 3; num_key_value_heads = 1; hidden_size = num_attention_heads*208; intermediate_size = hidden_size*4; head_dim = 224; rms_norm_eps = 1e-6; rope_theta = 1000.0\n",
    "# num_hidden_layers = 22; num_attention_heads = 8; num_key_value_heads = 4; hidden_size = num_attention_heads*84; intermediate_size = hidden_size*4; head_dim = 192; rms_norm_eps = 1e-6; rope_theta = 1000.0\n",
    "num_hidden_layers = 18; num_attention_heads = 8; num_key_value_heads = 4; hidden_size = num_attention_heads*72; intermediate_size = hidden_size*4; head_dim = 256; rms_norm_eps = 1e-4; rope_theta = 4000.0\n",
    "# num_hidden_layers = 16; num_attention_heads = 9; num_key_value_heads = 3; hidden_size = num_attention_heads*72; intermediate_size = hidden_size*4; head_dim = 192; rms_norm_eps = 1e-4; rope_theta = 4000.0\n",
    "\n",
    "\n",
    "vocab_size = 50257; T=512; from transformers import AutoTokenizer; import matplotlib.pyplot as plt; import numpy as np; import torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "\n",
    "### Please choose a model to load\n",
    "# model.load_state_dict(torch.load('Normal Models_Baby10M/minGemma-hidden_layers48-att_heads3-kv_heads1-hidden624-intermediate2496-head_dim224-T512--2025-07-20-01-06.pth'))\n",
    "# model.load_state_dict(torch.load('FORGETTER Models_Baby10M/minGemma-hidden_layers22-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-06-21-17-20.pth'))\n",
    "model.load_state_dict(torch.load('Normal Models_Baby100M/minGemma-hidden_layers18-att_heads8-kv_heads4-hidden576-intermediate2304-head_dim256-T512--2025-07-21-10-59.pth'))\n",
    "# model.load_state_dict(torch.load('FORGETTER Models_Baby100M/minGemma-hidden_layers16-att_heads9-kv_heads3-hidden648-intermediate2592-head_dim192-T512--2025-06-25-10-20.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b17bb-d626-472e-8985-bd9535dc2c42",
   "metadata": {},
   "source": [
    "### BLiMP for best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f660fee-0efe-4cb9-8209-fcd17e015781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjunct_island.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab\\AppData\\Local\\Temp\\ipykernel_7688\\812307827.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bads = torch.tensor(bads, device='cuda'); # torch.cuda.empty_cache(); print(bads.shape)\n",
      "C:\\Users\\miura_lab\\AppData\\Local\\Temp\\ipykernel_7688\\3399825607.py:39: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs\n",
      "C:\\Users\\miura_lab\\AppData\\Local\\Temp\\ipykernel_7688\\812307827.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goods = torch.tensor(goods, device='cuda'); # torch.cuda.empty_cache()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801\n",
      "anaphor_gender_agreement.jsonl\n",
      "0.946\n",
      "anaphor_number_agreement.jsonl\n",
      "0.976\n",
      "animate_subject_passive.jsonl\n",
      "0.793\n",
      "animate_subject_trans.jsonl\n",
      "0.87\n",
      "causative.jsonl\n",
      "0.724\n",
      "complex_NP_island.jsonl\n",
      "0.518\n",
      "coordinate_structure_constraint_complex_left_branch.jsonl\n",
      "0.552\n",
      "coordinate_structure_constraint_object_extraction.jsonl\n",
      "0.805\n",
      "determiner_noun_agreement_1.jsonl\n",
      "0.98\n",
      "determiner_noun_agreement_2.jsonl\n",
      "0.97\n",
      "determiner_noun_agreement_irregular_1.jsonl\n",
      "0.858\n",
      "determiner_noun_agreement_irregular_2.jsonl\n",
      "0.956\n",
      "determiner_noun_agreement_with_adjective_1.jsonl\n",
      "0.946\n",
      "determiner_noun_agreement_with_adj_2.jsonl\n",
      "0.908\n",
      "determiner_noun_agreement_with_adj_irregular_1.jsonl\n",
      "0.837\n",
      "determiner_noun_agreement_with_adj_irregular_2.jsonl\n",
      "0.885\n",
      "distractor_agreement_relational_noun.jsonl\n",
      "0.804\n",
      "distractor_agreement_relative_clause.jsonl\n",
      "0.7\n",
      "drop_argument.jsonl\n",
      "0.778\n",
      "ellipsis_n_bar_1.jsonl\n",
      "0.812\n",
      "ellipsis_n_bar_2.jsonl\n",
      "0.831\n",
      "existential_there_object_raising.jsonl\n",
      "0.702\n",
      "existential_there_quantifiers_1.jsonl\n",
      "0.984\n",
      "existential_there_quantifiers_2.jsonl\n",
      "0.617\n",
      "existential_there_subject_raising.jsonl\n",
      "0.848\n",
      "expletive_it_object_raising.jsonl\n",
      "0.723\n",
      "inchoative.jsonl\n",
      "0.631\n",
      "intransitive.jsonl\n",
      "0.811\n",
      "irregular_past_participle_adjectives.jsonl\n",
      "0.852\n",
      "irregular_past_participle_verbs.jsonl\n",
      "0.872\n",
      "irregular_plural_subject_verb_agreement_1.jsonl\n",
      "0.881\n",
      "irregular_plural_subject_verb_agreement_2.jsonl\n",
      "0.852\n",
      "left_branch_island_echo_question.jsonl\n",
      "0.498\n",
      "left_branch_island_simple_question.jsonl\n",
      "0.679\n",
      "matrix_question_npi_licensor_present.jsonl\n",
      "0.46\n",
      "npi_present_1.jsonl\n",
      "0.558\n",
      "npi_present_2.jsonl\n",
      "0.609\n",
      "only_npi_licensor_present.jsonl\n",
      "0.673\n",
      "only_npi_scope.jsonl\n",
      "0.563\n",
      "passive_1.jsonl\n",
      "0.85\n",
      "passive_2.jsonl\n",
      "0.843\n",
      "principle_A_case_1.jsonl\n",
      "1.0\n",
      "principle_A_case_2.jsonl\n",
      "0.911\n",
      "principle_A_c_command.jsonl\n",
      "0.637\n",
      "principle_A_domain_1.jsonl\n",
      "0.99\n",
      "principle_A_domain_2.jsonl\n",
      "0.807\n",
      "principle_A_domain_3.jsonl\n",
      "0.554\n",
      "principle_A_reconstruction.jsonl\n",
      "0.141\n",
      "regular_plural_subject_verb_agreement_1.jsonl\n",
      "0.86\n",
      "regular_plural_subject_verb_agreement_2.jsonl\n",
      "0.834\n",
      "sentential_negation_npi_licensor_present.jsonl\n",
      "0.993\n",
      "sentential_negation_npi_scope.jsonl\n",
      "0.645\n",
      "sentential_subject_island.jsonl\n",
      "0.318\n",
      "superlative_quantifiers_1.jsonl\n",
      "0.685\n",
      "superlative_quantifiers_2.jsonl\n",
      "0.909\n",
      "tough_vs_raising_1.jsonl\n",
      "0.625\n",
      "tough_vs_raising_2.jsonl\n",
      "0.858\n",
      "transitive.jsonl\n",
      "0.821\n",
      "wh_island.jsonl\n",
      "0.82\n",
      "wh_questions_object_gap.jsonl\n",
      "0.761\n",
      "wh_questions_subject_gap.jsonl\n",
      "0.893\n",
      "wh_questions_subject_gap_long_distance.jsonl\n",
      "0.831\n",
      "wh_vs_that_no_gap.jsonl\n",
      "0.958\n",
      "wh_vs_that_no_gap_long_distance.jsonl\n",
      "0.962\n",
      "wh_vs_that_with_gap.jsonl\n",
      "0.553\n",
      "wh_vs_that_with_gap_long_distance.jsonl\n",
      "0.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7668955223880597"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fast version # BLiMP for MinGemma \"model\"\n",
    "import os, json, re; import numpy as np; tokenizer = AutoTokenizer.from_pretrained('gpt2'); tokenizer.pad_token = tokenizer.eos_token; model.eval()\n",
    "accuracy=[]; files = os.listdir(\"./blimp-master/data/\")\n",
    "for filename in files:\n",
    "    correct = 0; total = 0\n",
    "    f = open(\"./blimp-master/data/\"+filename); print(filename)\n",
    "    for _ in range(20):\n",
    "        lines = [f.readline() for _ in range(50)] # 22.7G for L48 #  lines = f.readlines() # len(list)==1000\n",
    "        batch_bad = [re.sub(r\"\\n+\", \"\\n\", json.loads(x)[\"sentence_bad\"]).replace(\"\\n\",\" \") for x in lines]\n",
    "        batch_good = [re.sub(r\"\\n+\", \"\\n\", json.loads(x)[\"sentence_good\"]).replace(\"\\n\",\" \") for x in lines]\n",
    "        \n",
    "        bads = tokenizer.batch_encode_plus(batch_bad, padding=\"longest\", max_length=512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "        bads = torch.tensor(bads, device='cuda'); # torch.cuda.empty_cache(); print(bads.shape)\n",
    "        pred = model(bads)[1].to(\"cpu\")\n",
    "\n",
    "        goods = tokenizer.batch_encode_plus(batch_good, padding=\"longest\", max_length=512, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "        goods = torch.tensor(goods, device='cuda'); # torch.cuda.empty_cache()\n",
    "        pred2 = model(goods)[1].to(\"cpu\")\n",
    "\n",
    "        for l in range(len(lines)):\n",
    "            ans = bads[l,1:]; ans = ans[ans!=50256]\n",
    "            likeli_list = [pred[l, i, ans[i]] for i in range(len(ans))]\n",
    "            likelihood_bad = sum(torch.tensor(likeli_list) - torch.log(torch.sum(torch.exp(pred[l,0:len(ans),:]),1)))\n",
    "\n",
    "            ans2 = goods[l,1:]; ans2 = ans2[ans2!=50256]\n",
    "            likeli_list2 = [pred2[l, i, ans2[i]] for i in range(len(ans2))]\n",
    "            likelihood_good = sum(torch.tensor(likeli_list2) - torch.log(torch.sum(torch.exp(pred2[l,0:len(ans2),:]),1)))\n",
    "        \n",
    "            total += 1\n",
    "            if likelihood_bad < likelihood_good:\n",
    "                correct += 1\n",
    "    accuracy.append(correct/total); print(correct/total)\n",
    "np.mean(np.array(accuracy)) # 0.7033731343283581 for L48N_Baby10M  # 0.7256567164179104 for L22F_Baby10M  # 0.7668955223880597 for L18N_Baby100M  # 0.7748059701492538 for L16F_Baby100M     # cf:  0.7175970149253731 for our best model for WikiText-103"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
