{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4dc65c-6467-45fb-b5c9-7c24dac02a6a",
   "metadata": {},
   "source": [
    "# Summary of best models for given numbers of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395f15-8b92-41a5-8c3d-af5ec3f42f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is the output for a model in the following format. Different lines for different model structures. See the cell below for the excecuted codes.\n",
    "# print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')\n",
    "# Example: [16layers, 12(6)heads, hidden-dim=696, inter-layer-dim = 696x4, head-dim=224, #param=218million, train_loss, val_loss, Num-steps]\n",
    "\n",
    "# L108 in preparation (coming soon in the same format)\n",
    "[+54, 2, 1, 672, 2688, 192, 368.6, 3.4271,+3.0483, 19600], # minGemma-hidden_layers54-att_heads2-kv_heads1-hidden672-intermediate2688-head_dim192-T512--2025-08-16-17-14.pth\n",
    "# L48 in preparation (coming soon in the same format)\n",
    "[+36, 4, 2, 696, 2784, 240, 316.7, 3.4001,+3.0480, 19600], # x4\n",
    "[+30, 4, 2, 648, 2592, 224, 236.2, 3.3476,+3.0493, 19600], # minGemma-hidden_layers30-att_heads4-kv_heads2-hidden648-intermediate2592-head_dim224-T512--2025-06-07-15-50.pth  # +: used for figures\n",
    "[ 28, 6, 3, 648, 2592, 240, 252.2, 3.3411,+3.0502, 19600],\n",
    "[ 26, 4, 4, 648, 2592, 240, 228.5, 3.3902,+3.0504, 19600],\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3493,+3.0484, 19600], # minGemma-hidden_layers24-att_heads6-kv_heads3-hidden648-intermediate2592-head_dim240-T512--2025-05-16-05-52.pth\n",
    "[+24, 5, 1, 760, 3040, 224, 253.8, 3.4110,+3.0484, 19600],\n",
    "[+22, 8, 4, 704, 2816, 192, 237.8, 3.3583,+3.0505, 19600], # minGemma-hidden_layers22-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim192-T512--2025-05-10-14-35.pth\n",
    "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.4022,+3.0508, 19600], # minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-03-23-24.pth\n",
    "[+18, 6, 3, 720, 2880, 224, 200.6, 3.4125,+3.0504, 19600],\n",
    "[+16, 8, 4, 672, 2688, 192, 170.1, 3.3666,+3.0526, 19600], # minGemma-hidden_layers16-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-07-15-16-14.pth\n",
    "[ 14, 8, 4, 704, 2816, 192, 164.2, 3.3670,+3.0553, 20800],\n",
    "[+12,10, 5, 720, 2880, 288, 185.6, 3.4425,+3.0543, 18800],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.4480,+3.0815, 20000],\n",
    "[+ 9,16, 4, 832, 3328, 224, 183.7, 3.5710,+3.0603, 18800],\n",
    "[  8, 8, 4, 800, 3200, 480, 175.4, 3.4380,+3.0768, 21000],\n",
    "[  6, 8, 4, 768, 3072, 256, 109.4, 3.3593,+3.0972, 24500],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f20c11-804b-497f-934b-7eaf79b88de7",
   "metadata": {},
   "source": [
    "# L54 (Models with 54 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e96e1-ee39-404c-8f02-3474ca69ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L54 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 54, 3, 1, 720, 2880, 272, 457.1, 3.4361, 3.0563, 19600], # 3.8h\n",
    "[ 54, 3, 1, 720, 2880, 256, 452.2, 3.5596, 3.0630, 19600], # 3.6h\n",
    "[ 54, 3, 1, 720, 2880, 240, 447.2, 3.5237, 3.0633, 19600], # 3.6h\n",
    "[ 54, 3, 1, 720, 2880, 224, 442.2, 3.5299, 3.0677, 19600], # 3.5h\n",
    "[ 54, 3, 1, 720, 2880, 208, 437.2, 3.6124, 3.0659, 19600], # 3.4h\n",
    "[ 54, 3, 1, 720, 2880, 192, 432.3, 3.4645, 3.0570, 19600], # 3.4h\n",
    "[ 54, 3, 1, 720, 2880, 176, 427.3, 3.4739, 3.0564, 19600], # 3.3h\n",
    "\n",
    "[ 54, 3, 1, 672, 2688, 272, 405.8, 3.3901, 3.0569, 19600], # 3.6h\n",
    "[ 54, 3, 1, 672, 2688, 256, 401.1, 3.4185, 3.0521, 19600], # 3.4h\n",
    "[ 54, 3, 1, 672, 2688, 240, 396.5, 3.3928,+3.0497, 19600], # 3.4h 23.8G\n",
    "[ 54, 3, 1, 672, 2688, 224, 391.8, 3.3919, 3.0542, 19600], # 3.3h\n",
    "[ 54, 3, 1, 672, 2688, 208, 387.2, 3.3837, 3.0548, 19600], # 3.2h\n",
    "[ 54, 3, 1, 672, 2688, 192, 382.5, 3.4246, 3.0519, 19600], # 3.2h\n",
    "[ 54, 3, 1, 672, 2688, 176, 377.9, 3.4170, 3.0601, 19600], # 3.1h 22.8G\n",
    "\n",
    "[ 54, 3, 1, 624, 2496, 272, 357.4, 3.4587, 3.0658, 19600], # 3.4h\n",
    "[ 54, 3, 1, 624, 2496, 256, 353.1, 3.4074, 3.0522, 19600], # 3.2h\n",
    "[ 54, 3, 1, 624, 2496, 240, 348.7, 3.5142, 3.0692, 19600], # 3.2h\n",
    "[ 54, 3, 1, 624, 2496, 224, 344.4, 3.3875,+3.0505, 19600], # 3.1h\n",
    "[ 54, 3, 1, 624, 2496, 208, 340.1, 3.4789, 3.0584, 19600], # 3.0h\n",
    "[ 54, 3, 1, 624, 2496, 192, 335.8, 3.5253, 3.0562, 19600], # 2.9h\n",
    "[ 54, 3, 1, 624, 2496, 176, 331.5, 3.3976, 3.0556, 19600], # 2.9h 21.6G\n",
    "\n",
    "[ 54, 3, 1, 576, 2304, 272, 312.0, 3.5039, 3.0814, 19600], # 3.2h 22.1G\n",
    "[ 54, 3, 1, 576, 2304, 256, 308.0, 3.2914, 3.0510, 19600], # 3.0h\n",
    "[ 54, 3, 1, 576, 2304, 240, 304.0, 3.4282, 3.0647, 19600], # 3.0h 21.7G\n",
    "[ 54, 3, 1, 576, 2304, 224, 300.0, 3.3055, 3.0562, 19600], # 2.9h\n",
    "[ 54, 3, 1, 576, 2304, 208, 296.0, 3.2978, 3.0579, 19600], # 2.9h 21.2G\n",
    "[ 54, 3, 1, 576, 2304, 192, 292.1, 3.4676, 3.0667, 19600], # 2.8h\n",
    "[ 54, 3, 1, 576, 2304, 176, 288.1, 3.4513, 3.0861, 19600], # 2.7h 20.5G\n",
    "\n",
    "\n",
    "[ 54, 2, 2, 680, 2720, 224, 400.0, 3.5551, 3.0619, 19600], # 3.4h 23.3G\n",
    "[ 54, 2, 2, 680, 2720, 192, 390.6, 3.5056, 3.0557, 19600], # 3.2h 23.2G\n",
    "[ 54, 2, 2, 624, 2496, 224, 344.4, 3.5218, 3.0575, 19600], # 3.0h 21.9G\n",
    "[ 54, 2, 2, 624, 2496, 208, 340.1, 3.4335, 3.0580, 19600], # 2.9h\n",
    "[ 54, 2, 2, 624, 2496, 192, 335.8, 3.4289,+3.0551, 19600], # 2.9h\n",
    "[ 54, 2, 2, 624, 2496, 176, 331.5, 3.4909, 3.0595, 19600], # 2.8h\n",
    "[ 54, 2, 2, 624, 2496, 160, 327.2, 3.5640, 3.0651, 19600], # 2.8h\n",
    "[ 54, 2, 2, 576, 2304, 224, 300.0, 3.5200, 3.0618, 19600], # 2.8h 20.4G\n",
    "[ 54, 2, 2, 576, 2304, 208, 296.0, 3.3500, 3.0617, 19600], # 2.7h\n",
    "[ 54, 2, 2, 576, 2304, 192, 292.1, 3.4102,+3.0547, 19600], # 2.7h\n",
    "[ 54, 2, 2, 576, 2304, 176, 288.1, 3.4221, 3.0632, 19600], # 2.6h\n",
    "[ 54, 2, 2, 576, 2304, 160, 284.1, 3.3434, 3.0615, 19600], # 2.6h\n",
    "[ 54, 2, 2, 528, 2112, 192, 251.3, 3.5129, 3.0884, 19600], # 2.5h\n",
    "[ 54, 2, 2, 448, 1792, 224, 196.2, 3.3046, 3.0871, 19600], # 2.3h 17.8G\n",
    "\n",
    "\n",
    "[ 54, 2, 1, 768, 3072, 256, 485.0, 3.5042, 3.0585, 19600], # 3.4h\n",
    "[ 54, 2, 1, 768, 3072, 240, 481.0, 3.4979, 3.0599, 19600], # 3.4h\n",
    "[ 54, 2, 1, 768, 3072, 224, 477.0, 3.5317, 3.0527, 19600], # 3.3h\n",
    "[ 54, 2, 1, 768, 3072, 208, 473.0, 3.5513, 3.0540, 19600], # 3.3h\n",
    "[ 54, 2, 1, 768, 3072, 192, 469.0, 3.7122, 3.0711, 19600], # 3.2h\n",
    "[ 54, 2, 1, 768, 3072, 176, 465.1, 3.6695, 3.0635, 19600], # 3.2h\n",
    "\n",
    "[ 54, 2, 1, 720, 2880, 256, 432.3, 3.5785, 3.0580, 19600], # 3.3h\n",
    "[ 54, 2, 1, 720, 2880, 240, 428.5, 3.5684, 3.0551, 19600], # 3.3h\n",
    "[ 54, 2, 1, 720, 2880, 224, 424.8, 3.4940,+3.0499, 19600], # 3.2h\n",
    "[ 54, 2, 1, 720, 2880, 208, 421.1, 3.4366,+3.0509, 19600], # 3.2h 24.1G\n",
    "[ 54, 2, 1, 720, 2880, 192, 417.3, 3.5664, 3.0561, 19600], # 3.1h\n",
    "[ 54, 2, 1, 720, 2880, 176, 413.6, 3.6339, 3.0583, 19600], # 3.0h\n",
    "\n",
    "[ 54, 2, 1, 672, 2688, 256, 382.5, 3.5768, 3.0713, 19600], # 3.1h 23.6G\n",
    "[ 54, 2, 1, 672, 2688, 240, 379.1, 3.4703, 3.0524, 19600], # 3.1h 23.4G\n",
    "[ 54, 2, 1, 672, 2688, 224, 375.6, 3.4959, 3.0533, 19600], # 3.0h\n",
    "[ 54, 2, 1, 672, 2688, 208, 372.1, 3.5396, 3.0526, 19600], # 3.0h\n",
    "[+54, 2, 1, 672, 2688, 192, 368.6, 3.4271,+3.0483, 19600], # 2.9h 22.4G  minGemma-hidden_layers54-att_heads2-kv_heads1-hidden672-intermediate2688-head_dim192-T512--2025-08-16-17-14.pth\n",
    "[ 54, 2, 1, 672, 2688, 176, 365.1, 3.4010, 3.0526, 19600], # 2.9h\n",
    "[ 54, 2, 1, 672, 2688, 160, 361.6, 3.4379, 3.0577, 19600], # 2.8h 21.9G\n",
    "\n",
    "[ 54, 2, 1, 624, 2496, 256, 335.8, 3.6524, 3.0739, 19600], # 2.9h\n",
    "[ 54, 2, 1, 624, 2496, 240, 332.6, 3.4693, 3.0544, 19600], # 2.9h\n",
    "[ 54, 2, 1, 624, 2496, 224, 329.3, 3.4309, 3.0593, 19600], # 2.8h 20.9G\n",
    "[ 54, 2, 1, 624, 2496, 208, 326.1, 3.5551, 3.0760, 19600], # 2.8h\n",
    "[ 54, 2, 1, 624, 2496, 192, 322.9, 3.4262, 3.0546, 19600], # 2.7h\n",
    "[ 54, 2, 1, 624, 2496, 176, 319.6, 3.3688, 3.0571, 19600], # 2.7h\n",
    "\n",
    "[ 54, 2, 1, 576, 2304, 224, 286.1, 3.3545, 3.0575, 19600], # 2.7h 19.9G\n",
    "[ 54, 2, 1, 576, 2304, 192, 280.1, 3.3284, 3.0561, 19600], # 2.6h\n",
    "[ 54, 2, 1, 576, 2304, 160, 274.1, 3.4594, 3.0616, 19600], # 2.5h\n",
    "\n",
    "[ 54, 2, 1, 528, 2112, 192, 240.3, 3.4893, 3.0816, 19600], # 2.4h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd88b5b3-a6c9-41a2-9f63-900ba6e943bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L54 att2 kv_heads1 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:56:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.427100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54, 2, 1, 672, 2688, 192, 368.6, 3.4271, 3.0483, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  54 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 2 # 4 # G16 G8\n",
    "num_key_value_heads = 1 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*336 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0483:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50037e80-f91a-439a-baec-00da88f1a7fc",
   "metadata": {},
   "source": [
    "# L36 (x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e16efb-33fe-4638-ae01-f4a84fff1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L36 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "\n",
    "[ 36, 4, 2, 840, 3360, 224, 428.7, 3.4850, 3.0593, 19600], # 3.3h\n",
    "\n",
    "[ 36, 4, 2, 792, 3168, 256, 398.7, 3.5811, 3.0678, 19600], # 3.3h 21.0G\n",
    "[ 36, 4, 2, 792, 3168, 224, 387.7, 3.5276, 3.0599, 19600], # 3.2h\n",
    "[ 36, 4, 2, 792, 3168, 192, 376.8, 3.5021, 3.0624, 19600], # 3.1h 20.3G\n",
    "[ 36, 4, 2, 792, 3168, 160, 365.8, 3.6341, 3.0614, 19600], # 2.9h\n",
    "[ 36, 4, 2, 792, 3168, 128, 354.9, 3.5592, 3.0564, 19600], # 2.6h\n",
    "\n",
    "[ 36, 4, 2, 744, 2976, 288, 369.4, 3.4255, 3.0537, 19600], # 3.2h\n",
    "[ 36, 4, 2, 744, 2976, 272, 364.2, 3.4975, 3.0556, 19600], # 3.2h\n",
    "[ 36, 4, 2, 744, 2976, 256, 359.1, 3.4079, 3.0556, 19600], # 3.0h\n",
    "[ 36, 4, 2, 744, 2976, 240, 354.0, 3.3899, 3.0540, 19600], # 3.0h\n",
    "[ 36, 4, 2, 744, 2976, 224, 348.8, 3.4648, 3.0546, 19600], # 2.9h\n",
    "[ 36, 4, 2, 744, 2976, 208, 343.7, 3.4231, 3.0548, 19600], # 2.8h\n",
    "[ 36, 4, 2, 744, 2976, 192, 338.5, 3.5095, 3.0576, 19600], # 2.7h 19.3G\n",
    "[ 36, 4, 2, 744, 2976, 176, 333.4, 3.5031, 3.0574, 19600], # 2.7h\n",
    "[ 36, 4, 2, 744, 2976, 160, 328.2, 3.4438, 3.0560, 19600], # 2.7h\n",
    "[ 36, 4, 2, 744, 2976, 144, 323.1, 3.3994, 3.0560, 19600], # 2.6h\n",
    "[ 36, 4, 2, 744, 2976, 128, 318.0, 3.4963, 3.0534, 19600], # 2.3h\n",
    "\n",
    "[ 36, 4, 2, 720, 2880, 288, 350.0, 3.4103, 3.0541, 19600], # 3.2h\n",
    "[ 36, 4, 2, 720, 2880, 272, 345.0, 3.4606, 3.0571, 19600], # 3.1h\n",
    "[ 36, 4, 2, 720, 2880, 256, 340.0, 3.5208, 3.0560, 19600], # 3.0h\n",
    "[ 36, 4, 2, 720, 2880, 240, 335.1, 3.5214, 3.0580, 19600], # 2.9h\n",
    "[ 36, 4, 2, 720, 2880, 224, 330.1, 3.5830, 3.0598, 19600], # 2.8h\n",
    "[ 36, 4, 2, 720, 2880, 208, 325.1, 3.4316, 3.0566, 19600], # 2.8h\n",
    "[ 36, 4, 2, 720, 2880, 192, 320.1, 3.4362,+3.0523, 19600], # 2.7h\n",
    "[ 36, 4, 2, 720, 2880, 176, 315.2, 3.3932,+3.0505, 19600], # 2.7h\n",
    "[ 36, 4, 2, 720, 2880, 160, 310.2, 3.4212,+3.0496, 19600], # 2.6h\n",
    "[ 36, 4, 2, 720, 2880, 144, 305.2, 3.5086, 3.0551, 19600], # 2.5h\n",
    "[ 36, 4, 2, 720, 2880, 128, 300.2, 3.4822, 3.0571, 19600], # 2.3h\n",
    "\n",
    "[ 36, 4, 2, 696, 2784, 288, 331.1, 3.3970, 3.0578, 19600], # 3.1h\n",
    "[ 36, 4, 2, 696, 2784, 272, 326.3, 3.3828, 3.0569, 19600], # 3.1h\n",
    "[ 36, 4, 2, 696, 2784, 256, 321.5, 3.4542, 3.0544, 19600], # 2.9h 19.6G\n",
    "[+36, 4, 2, 696, 2784, 240, 316.7, 3.4001,+3.0480, 19600], # 2.8h          BEST (not saved)\n",
    "[ 36, 4, 2, 696, 2784, 240, 316.7, 3.3726, 3.0538, 19600], # again\n",
    "[ 36, 4, 2, 696, 2784, 240, 316.7, 3.3702, 3.0509, 19600], # again\n",
    "[ 36, 4, 2, 696, 2784, 240, 316.7, 3.3986, 3.0545, 19600], # again\n",
    "[ 36, 4, 2, 696, 2784, 224, 311.9, 3.3695,+3.0500, 19600], # 2.6h x4\n",
    "[ 36, 4, 2, 696, 2784, 224, 311.9, 3.3796,+3.0496, 19600], # again\n",
    "[ 36, 4, 2, 696, 2088, 224, 259.5, 3.3724, 3.0527, 19600], # 2.5h x3\n",
    "[ 36, 4, 2, 696, 2784, 208, 307.1, 3.5951, 3.0783, 19600], # 2.7h\n",
    "[ 36, 4, 2, 696, 2784, 192, 302.3, 3.3794, 3.0532, 19600], # 2.6h\n",
    "[ 36, 4, 2, 696, 2784, 176, 297.4, 3.4687,+3.0510, 19600], # 2.6h\n",
    "[ 36, 4, 2, 696, 2784, 160, 292.6, 3.4674, 3.0567, 19600], # 2.5h\n",
    "[ 36, 4, 2, 696, 2784, 144, 287.8, 3.3928, 3.0541, 19600], # 2.5h\n",
    "[ 36, 4, 2, 696, 2784, 128, 283.0, 3.3971, 3.0531, 19600], # 2.2h\n",
    "\n",
    "[ 36, 4, 2, 672, 2688, 288, 312.7, 3.4080, 3.0538, 19600], # 3.0h\n",
    "[ 36, 4, 2, 672, 2688, 272, 308.1, 3.4216, 3.0567, 19600], # 3.0h\n",
    "[ 36, 4, 2, 672, 2688, 256, 303.4, 3.4013, 3.0526, 19600], # 2.8h\n",
    "[ 36, 4, 2, 672, 2688, 240, 298.8, 3.4572, 3.0589, 19600], # 2.8h\n",
    "[ 36, 4, 2, 672, 2688, 224, 294.2, 3.4881, 3.0547, 19600], # 2.7h\n",
    "[ 36, 4, 2, 672, 2688, 208, 289.5, 3.5365, 3.0680, 19600], # 2.6h\n",
    "[ 36, 4, 2, 672, 2688, 192, 284.9, 3.3781, 3.0524, 19600], # 2.6h\n",
    "[ 36, 4, 2, 672, 2688, 176, 280.2, 3.4654,+3.0490, 19600], # 2.5h\n",
    "[ 36, 4, 2, 672, 2688, 176, 280.2, 3.3600, 3.0509, 19600], # again\n",
    "[ 36, 4, 2, 672, 2688, 176, 280.2, 3.3623, 3.0504, 19600], # again\n",
    "[ 36, 4, 2, 672, 2688, 160, 275.6, 3.4952, 3.0582, 19600], # 2.5h\n",
    "[ 36, 4, 2, 672, 2688, 144, 270.9, 3.3701,+3.0508, 19600], # 2.4h\n",
    "[ 36, 4, 2, 672, 2688, 128, 266.3, 3.3734, 3.0558, 19600], # 2.1h 16.6G\n",
    "\n",
    "[ 36, 4, 2, 648, 2592, 336, 308.3, 3.3869, 3.0619, 19600], # 3.2h\n",
    "[ 36, 4, 2, 648, 2592, 320, 303.8, 3.3703, 3.0529, 19600], # 3.1h\n",
    "[ 36, 4, 2, 648, 2592, 304, 299.3, 3.3744,+3.0507, 19600], # 3.1h\n",
    "[ 36, 4, 2, 648, 2592, 288, 294.8, 3.4148, 3.0589, 19600], # 3.0h\n",
    "[ 36, 4, 2, 648, 2592, 272, 290.4, 3.3845, 3.0516, 19600], # 2.9h 18.3G\n",
    "[ 36, 4, 2, 648, 2592, 256, 285.9, 3.3446, 3.0511, 19600], # 2.8h 18.0G\n",
    "[ 36, 4, 2, 648, 2592, 240, 281.4, 3.3328, 3.0510, 19600], # 2.7h\n",
    "[ 36, 4, 2, 648, 2592, 224, 276.9, 3.3580, 3.0509, 19600], # 2.7h\n",
    "[ 36, 4, 2, 648, 2592, 208, 272.4, 3.3831,+3.0503, 19600], # 2.6h\n",
    "[ 36, 4, 2, 648, 2592, 192, 268.0, 3.3483, 3.0526, 19600], # 2.5h\n",
    "[ 36, 4, 2, 648, 2592, 176, 263.5, 3.3668, 3.0539, 19600], # 2.5h\n",
    "[ 36, 4, 2, 648, 2592, 160, 259.0, 3.3515, 3.0526, 19600], # 2.4h\n",
    "[ 36, 4, 2, 648, 2592, 144, 254.5, 3.4667, 3.0671, 19600], # 2.4h\n",
    "[ 36, 4, 2, 648, 2592, 128, 250.1, 3.3995, 3.0610, 19600], # 2.1h\n",
    "[ 36, 4, 2, 648, 2592,  96, 241.1, 3.4715, 3.0634, 19600], # 1.9h\n",
    "\n",
    "[ 36, 4, 2, 624, 2496, 288, 277.5, 3.5308, 3.0760, 19600], # 1.8h 18.3G\n",
    "[ 36, 4, 2, 624, 2496, 272, 273.1, 3.3184,+3.0496, 19600], # 2.7h\n",
    "[ 36, 4, 2, 624, 2496, 272, 273.1, 3.3301, 3.0498, 19600], # again\n",
    "[ 36, 4, 2, 624, 2496, 272, 273.1, 3.3259, 3.0500, 19600], # again\n",
    "[ 36, 4, 2, 624, 2496, 256, 268.8, 3.3253, 3.0547, 19600], # 2.5h\n",
    "[ 36, 4, 2, 624, 2496, 240, 264.5, 3.3560,+3.0502, 19600], # 2.5h\n",
    "[ 36, 4, 2, 624, 2496, 240, 264.5, 3.3557, 3.0548, 19600], # again\n",
    "[ 36, 4, 2, 624, 2496, 224, 260.2, 3.3689, 3.0558, 19600], # 2.5h\n",
    "[ 36, 4, 2, 624, 2496, 208, 255.9, 3.3524, 3.0569, 19600], # 2.4h\n",
    "[ 36, 4, 2, 624, 2496, 192, 251.6, 3.3197, 3.0573, 19600], # 2.3h\n",
    "[ 36, 4, 2, 624, 2496, 176, 247.3, 3.3438,+3.0489, 19600], # 2.3h\n",
    "[ 36, 4, 2, 624, 2496, 176, 247.3, 3.3418, 3.0519, 19600], # again\n",
    "[ 36, 4, 2, 624, 2496, 176, 247.3, 3.5566, 3.0782, 19600], # again\n",
    "[ 36, 4, 2, 624, 2496, 160, 242.9, 3.4653, 3.0578, 19600], # 2.2h\n",
    "[ 36, 4, 2, 624, 2496, 144, 238.6, 3.4458, 3.0565, 19600], # 2.2h\n",
    "[ 36, 4, 2, 624, 2496, 128, 234.3, 3.3363, 3.0543, 19600], # 1.9h\n",
    "\n",
    "[ 36, 4, 2, 600, 2400, 288, 260.6, 3.3073, 3.0569, 19600], # 2.7h\n",
    "[ 36, 4, 2, 600, 2400, 272, 256.4, 3.3167, 3.0578, 19600], # 2.7h\n",
    "[ 36, 4, 2, 600, 2400, 256, 252.3, 3.3698, 3.0520, 19600], # 2.5h\n",
    "[ 36, 4, 2, 600, 2400, 240, 248.1, 3.3334, 3.0539, 19600], # 2.5h\n",
    "[ 36, 4, 2, 600, 2400, 224, 244.0, 3.3321,+3.0505, 19600], # 2.4h\n",
    "[ 36, 4, 2, 600, 2400, 208, 239.8, 3.3866,+3.0504, 19600], # 2.4h\n",
    "[ 36, 4, 2, 600, 2400, 192, 235.7, 3.3344, 3.0518, 19600], # 2.3h\n",
    "[ 36, 4, 2, 600, 2400, 176, 231.5, 3.3537, 3.0563, 19600], # 2.2h\n",
    "[ 36, 4, 2, 600, 2400, 160, 227.4, 3.3687, 3.0621, 19600], # 2.2h\n",
    "[ 36, 4, 2, 600, 2400, 144, 223.2, 3.4565, 3.0619, 19600], # 2.1h\n",
    "[ 36, 4, 2, 600, 2400, 128, 219.1, 3.4145, 3.0650, 19600], # 1.9h\n",
    "\n",
    "[ 36, 4, 2, 576, 2304, 256, 236.2, 3.3541, 3.0574, 19600], # 2.4h 16.5G\n",
    "[ 36, 4, 2, 576, 2304, 240, 232.2, 3.3370, 3.0536, 19600], # 2.4h\n",
    "[ 36, 4, 2, 576, 2304, 224, 228.2, 3.3021, 3.0625, 19600], # 2.3h\n",
    "[ 36, 4, 2, 576, 2304, 208, 224.3, 3.3672, 3.0615, 19600], # 2.3h\n",
    "[ 36, 4, 2, 576, 2304, 192, 220.3, 3.3539, 3.0586, 19600], # 2.2h\n",
    "[ 36, 4, 2, 576, 2304, 176, 216.3, 3.4127, 3.0608, 19600], # 2.1h\n",
    "[ 36, 4, 2, 576, 2304, 160, 212.3, 3.3288, 3.0605, 19600], # 2.1h\n",
    "[ 36, 4, 2, 576, 2304, 144, 208.3, 3.3587, 3.0579, 19600], # 2.0h\n",
    "[ 36, 4, 2, 576, 2304, 128, 204.4, 3.3323, 3.0693, 19600], # 1.8h\n",
    "\n",
    "[ 36, 4, 2, 552, 2208, 256, 220.6, 3.2906, 3.0609, 19600], # 2.4h\n",
    "[ 36, 4, 2, 552, 2208, 224, 213.0, 3.3441, 3.0594, 19600], # 2.3h\n",
    "[ 36, 4, 2, 552, 2208, 192, 205.4, 3.3360, 3.0661, 19600], # 2.2h\n",
    "\n",
    "[ 36, 4, 2, 504, 2016, 256, 191.0, 3.2875, 3.0640, 19600], # 2.2h\n",
    "[ 36, 4, 2, 504, 2016, 224, 184.0, 3.3013, 3.0691, 19600], # 2.1h\n",
    "[ 36, 4, 2, 504, 2016, 192, 177.1, 3.2946, 3.0662, 19600], # 2.0h\n",
    "\n",
    "\n",
    "[ 36, 3, 3, 768, 3072, 256, 378.6, 3.4727, 3.0578, 19600], # 2.9h\n",
    "[ 36, 3, 3, 768, 3072, 224, 368.0, 3.5242, 3.0586, 19600], # 2.8h\n",
    "[ 36, 3, 3, 768, 3072, 192, 357.4, 3.5808, 3.0590, 19600], # 2.7h\n",
    "\n",
    "[ 36, 3, 3, 720, 2880, 256, 340.0, 3.4866, 3.0553, 19600], # 2.8h\n",
    "[ 36, 3, 3, 720, 2880, 224, 330.1, 3.5809, 3.0654, 19600], # 2.7h\n",
    "[ 36, 3, 3, 720, 2880, 192, 320.1, 3.4059, 3.0593, 19600], # 2.6h\n",
    "\n",
    "[ 36, 3, 3, 696, 2784, 256, 321.5, 3.5938, 3.0659, 19600], # 2.7h\n",
    "[ 36, 3, 3, 696, 2784, 240, 316.7, 3.4857, 3.0625, 19600], # 2.7h\n",
    "[ 36, 3, 3, 696, 2784, 224, 311.9, 3.3981, 3.0541, 19600], # 2.6h\n",
    "[ 36, 3, 3, 696, 2784, 208, 307.1, 3.5373, 3.0614, 19600], # 2.6h\n",
    "[ 36, 3, 3, 696, 2784, 192, 302.3, 3.4465, 3.0554, 19600], # 2.5h\n",
    "[ 36, 3, 3, 696, 2784, 176, 297.4, 3.5031, 3.0557, 19600], # 2.5h\n",
    "\n",
    "[ 36, 3, 3, 672, 2688, 256, 303.4, 3.3963, 3.0580, 19600], # 2.6h\n",
    "[ 36, 3, 3, 672, 2688, 240, 298.8, 3.4239, 3.0520, 19600], # 2.6h\n",
    "[ 36, 3, 3, 672, 2688, 224, 294.2, 3.4381,+3.0504, 19600], # 2.6h\n",
    "[ 36, 3, 3, 672, 2688, 224, 294.2, 3.4357, 3.0547, 19600], # again\n",
    "[ 36, 3, 3, 672, 2688, 208, 289.5, 3.4657, 3.0542, 19600], # 2.5h 18.6G\n",
    "[ 36, 3, 3, 672, 2688, 192, 284.9, 3.4151,+3.0505, 19600], # 2.4h\n",
    "[ 36, 3, 3, 672, 2688, 192, 284.9, 3.3958,+3.0490, 19600], # again\n",
    "[ 36, 3, 3, 672, 2688, 192, 284.9, 3.4007,+3.0507, 19600], # again\n",
    "[ 36, 3, 3, 672, 2688, 176, 280.2, 3.4945, 3.0571, 19600], # 2.4h\n",
    "\n",
    "[ 36, 3, 3, 648, 2592, 272, 290.4, 3.4527, 3.0590, 19600], # 2.8h 18.4G\n",
    "[ 36, 3, 3, 648, 2592, 256, 285.9, 3.4051, 3.0513, 19600], # 2.6h 19.0G\n",
    "[ 36, 3, 3, 648, 2592, 240, 281.4, 3.3465, 3.0536, 19600], # 2.6h\n",
    "[ 36, 3, 3, 648, 2592, 224, 276.9, 3.3766, 3.0572, 19600], # 2.5h\n",
    "[ 36, 3, 3, 648, 2592, 208, 272.4, 3.4343, 3.0547, 19600], # 2.5h\n",
    "[ 36, 3, 3, 648, 2592, 192, 268.0, 3.4586, 3.0563, 19600], # 2.4h\n",
    "[ 36, 3, 3, 648, 2592, 176, 263.5, 3.3968, 3.0581, 19600], # 2.4h\n",
    "\n",
    "[ 36, 3, 3, 624, 2496, 256, 268.8, 3.4141, 3.0583, 19600], # 2.4h 18.2G\n",
    "[ 36, 3, 3, 624, 2496, 224, 260.2, 3.3773, 3.0593, 19600], # 2.3h\n",
    "[ 36, 3, 3, 624, 2496, 192, 251.6, 3.3535, 3.0594, 19600], # 2.2h\n",
    "\n",
    "[ 36, 3, 3, 576, 2304, 256, 236.2, 3.3596, 3.0575, 19600], # 2.2h\n",
    "[ 36, 3, 3, 576, 2304, 224, 228.2, 3.3662, 3.0556, 19600], # 2.2h\n",
    "[ 36, 3, 3, 576, 2304, 192, 220.3, 3.3567, 3.0628, 19600], # 2.1h\n",
    "\n",
    "\n",
    "[ 36, 3, 1, 816, 3264, 288, 396.7, 3.4831, 3.0566, 19600], # 3.2h\n",
    "[ 36, 3, 1, 816, 3264, 272, 392.9, 3.4954, 3.0558, 19600], # 3.2h\n",
    "[ 36, 3, 1, 816, 3264, 256, 389.1, 3.4799, 3.0557, 19600], # 3.0h\n",
    "[ 36, 3, 1, 816, 3264, 240, 385.4, 3.5465, 3.0550, 19600], # 3.0h 21.4G\n",
    "[ 36, 3, 1, 816, 3264, 224, 381.6, 3.7241, 3.0928, 19600], # 3.0h 21.2G\n",
    "[ 36, 3, 1, 816, 3264, 208, 377.9, 3.4750, 3.0569, 19600], # 2.9h\n",
    "[ 36, 3, 1, 816, 3264, 192, 374.1, 3.5147, 3.0564, 19600], # 2.9h\n",
    "[ 36, 3, 1, 816, 3264, 176, 370.3, 3.5454, 3.0537, 19600], # 2.9h\n",
    "[ 36, 3, 1, 816, 3264, 160, 366.6, 3.5683, 3.0586, 19600], # 2.8h\n",
    "\n",
    "[ 36, 3, 1, 792, 3168, 288, 376.8, 3.5112, 3.0542, 19600], # 3.2h\n",
    "[ 36, 3, 1, 792, 3168, 272, 373.1, 3.5201, 3.0552, 19600], # 3.1h\n",
    "[ 36, 3, 1, 792, 3168, 256, 369.5, 3.4773,+3.0495, 19600], # 3.0h\n",
    "[ 36, 3, 1, 792, 3168, 256, 369.5, 3.5376, 3.0565, 19600], # again\n",
    "[ 36, 3, 1, 792, 3168, 240, 365.8, 3.4493, 3.0524, 19600], # 3.0h\n",
    "[ 36, 3, 1, 792, 3168, 240, 365.8, 3.5325, 3.0542, 19600], # again\n",
    "[ 36, 3, 1, 792, 3168, 224, 362.2, 3.6735, 3.0681, 19600], # 2.9h\n",
    "[ 36, 3, 1, 792, 3168, 208, 358.5, 3.5488, 3.0503, 19600], # 2.9h\n",
    "[ 36, 3, 1, 792, 3168, 208, 358.5, 3.6151, 3.0614, 19600], # again\n",
    "[ 36, 3, 1, 792, 3168, 192, 354.9, 3.7164, 3.0550, 19600], # 2.8h\n",
    "[ 36, 3, 1, 792, 3168, 176, 351.2, 3.6640, 3.0648, 19600], # 2.8h\n",
    "[ 36, 3, 1, 792, 3168, 160, 347.6, 3.5406, 3.0552, 19600], # 2.7h\n",
    "\n",
    "[ 36, 3, 1, 768, 3072, 288, 357.4, 3.5176, 3.0553, 19600], # 2.9h\n",
    "[ 36, 3, 1, 768, 3072, 272, 353.9, 3.5089, 3.0558, 19600], # 2.8h\n",
    "[ 36, 3, 1, 768, 3072, 256, 350.3, 3.4429, 3.0569, 19600], # 2.7h\n",
    "[ 36, 3, 1, 768, 3072, 240, 346.8, 3.5695, 3.0585, 19600], # 2.7h 19.6G\n",
    "[ 36, 3, 1, 768, 3072, 224, 343.3, 3.4571, 3.0551, 19600], # 2.6h\n",
    "[ 36, 3, 1, 768, 3072, 208, 339.7, 3.5932, 3.0591, 19600], # 2.6h\n",
    "[ 36, 3, 1, 768, 3072, 192, 336.2, 3.4498, 3.0573, 19600], # 2.6h\n",
    "[ 36, 3, 1, 768, 3072, 176, 332.6, 3.4800, 3.0540, 19600], # 2.5h\n",
    "[ 36, 3, 1, 768, 3072, 160, 329.1, 3.4416, 3.0540, 19600], # 2.5h\n",
    "\n",
    "[ 36, 3, 1, 744, 2976, 304, 342.0, 3.5729, 3.0671, 19600], # 2.9h\n",
    "[ 36, 3, 1, 744, 2976, 288, 338.5, 3.4905, 3.0537, 19600], # 2.9h\n",
    "[ 36, 3, 1, 744, 2976, 272, 335.1, 3.4834, 3.0611, 19600], # 2.8h\n",
    "[ 36, 3, 1, 744, 2976, 256, 331.7, 3.5078, 3.0552, 19600], # 2.7h\n",
    "[ 36, 3, 1, 744, 2976, 240, 328.2, 3.4458, 3.0595, 19600], # 2.7h 19.5G\n",
    "[ 36, 3, 1, 744, 2976, 224, 324.8, 3.4346,+3.0514, 19600], # 2.6h 19.8G\n",
    "[ 36, 3, 1, 744, 2976, 224, 324.8, 3.4408, 3.0548, 19600], # again\n",
    "[ 36, 3, 1, 744, 2976, 208, 321.4, 3.5070,+3.0514, 19600], # 2.6h 19.1G\n",
    "[ 36, 3, 1, 744, 2976, 208, 321.4, 3.5121, 3.0560, 19600], # again\n",
    "[ 36, 3, 1, 744, 2976, 192, 318.0, 3.4460, 3.0566, 19600], # 2.5h\n",
    "[ 36, 3, 1, 744, 2976, 176, 314.5, 3.4466,+3.0521, 19600], # 2.5h\n",
    "[ 36, 3, 1, 744, 2976, 160, 311.1, 3.5466, 3.0581, 19600], # 2.5h 18.6G\n",
    "\n",
    "[ 36, 3, 1, 720, 2880, 288, 320.1, 3.4049, 3.0555, 19600], # 2.8h\n",
    "[ 36, 3, 1, 720, 2880, 272, 316.8, 3.4513,+3.0520, 19600], # 2.8h\n",
    "[ 36, 3, 1, 720, 2880, 256, 313.5, 3.5063, 3.0539, 19600], # 2.6h 18.4G\n",
    "[ 36, 3, 1, 720, 2880, 240, 310.2, 3.5308, 3.0542, 19600], # 2.6h\n",
    "[ 36, 3, 1, 720, 2880, 224, 306.9, 3.6585, 3.0750, 19600], # 2.6h\n",
    "[ 36, 3, 1, 720, 2880, 208, 303.6, 3.5026, 3.0569, 19600],\n",
    "[ 36, 3, 1, 720, 2880, 192, 300.2, 3.4697, 3.0598, 19600], # 2.5h\n",
    "[ 36, 3, 1, 720, 2880, 176, 296.9, 3.5560,+3.0512, 19600], # 2.4h\n",
    "[ 36, 3, 1, 720, 2880, 160, 293.6, 3.6121, 3.0798, 19600], # 2.4h\n",
    "[ 36, 3, 1, 720, 2880, 144, 290.3, 3.4914, 3.0567, 19600], # 2.6h\n",
    "\n",
    "[ 36, 3, 1, 696, 2784, 288, 302.3, 3.6165, 3.0589, 19600], # 2.7h\n",
    "[ 36, 3, 1, 696, 2784, 272, 299.0, 3.4740, 3.0563, 19600], # 2.7h\n",
    "[ 36, 3, 1, 696, 2784, 256, 295.8, 3.4062, 3.0527, 19600], # 2.6h\n",
    "[ 36, 3, 1, 696, 2784, 240, 292.6, 3.4857, 3.0563, 19600], # 2.5h\n",
    "[ 36, 3, 1, 696, 2784, 224, 289.4, 3.4400,+3.0498, 19600], # 2.5h\n",
    "[ 36, 3, 1, 696, 2784, 224, 289.4, 3.6490, 3.0645, 19600], # again\n",
    "[ 36, 3, 1, 696, 2784, 208, 286.2, 3.5043, 3.0556, 19600], # 2.4h\n",
    "[ 36, 3, 1, 696, 2784, 192, 283.0, 3.4052,+3.0488, 19600], # 2.4h\n",
    "[ 36, 3, 1, 696, 2784, 192, 283.0, 3.5395, 3.0630, 19600], # again\n",
    "[ 36, 3, 1, 696, 2784, 176, 279.8, 3.3985,+3.0507, 19600], # 2.4h\n",
    "[ 36, 3, 1, 696, 2784, 176, 279.8, 3.4903, 3.0572, 19600], # again\n",
    "[ 36, 3, 1, 696, 2784, 160, 276.6, 3.5360, 3.0620, 19600], # 2.3h\n",
    "\n",
    "[ 36, 3, 1, 672, 2688, 336, 294.2, 3.3898, 3.0574, 19600], # 2.8h 17.8G\n",
    "[ 36, 3, 1, 672, 2688, 320, 291.1, 3.3541, 3.0525, 19600], # 2.7h 18.1G\n",
    "[ 36, 3, 1, 672, 2688, 304, 288.0, 3.3783,+3.0517, 19600], # 2.7h 18.7G\n",
    "[ 36, 3, 1, 672, 2688, 288, 284.9, 3.3797, 3.0530, 19600], # 2.7h\n",
    "[ 36, 3, 1, 672, 2688, 272, 281.8, 3.5555, 3.0673, 19600], # 2.6h\n",
    "[ 36, 3, 1, 672, 2688, 256, 278.7, 3.4379, 3.0536, 19600], # 2.5h\n",
    "[ 36, 3, 1, 672, 2688, 240, 275.6, 3.4226,+3.0489, 19600], # 2.5h\n",
    "[ 36, 3, 1, 672, 2688, 224, 272.5, 3.3763,+3.0485, 19600], # 2.4h\n",
    "[ 36, 3, 1, 672, 2688, 224, 272.5, 3.3802,+3.0482, 19600], # again  2nd BEST   minGemma-hidden_layers36-att_heads3-kv_heads1-hidden672-intermediate2688-head_dim224-T512--2025-07-19-00-54.pth\n",
    "[ 36, 3, 1, 672, 2688, 208, 269.4, 3.3576,+3.0503, 19600], # 2.4h\n",
    "[ 36, 3, 1, 672, 2688, 192, 266.3, 3.3585, 3.0552, 19600], # 2.3h\n",
    "[ 36, 3, 1, 672, 2688, 176, 263.2, 3.4220, 3.0534, 19600], # 2.3h\n",
    "[ 36, 3, 1, 672, 2688, 160, 260.1, 3.3704, 3.0558, 19600], # 2.3h\n",
    "\n",
    "[ 36, 3, 1, 648, 2592, 304, 271.0, 3.3699, 3.0555, 19600], # 2.7h\n",
    "[ 36, 3, 1, 648, 2592, 288, 268.0, 3.4793, 3.0529, 19600], # 2.6h\n",
    "[ 36, 3, 1, 648, 2592, 272, 265.0, 3.4980,+3.0502, 19600], # 2.6h\n",
    "[ 36, 3, 1, 648, 2592, 272, 265.0, 3.3608, 3.0523, 19600], # again\n",
    "[ 36, 3, 1, 648, 2592, 256, 262.0, 3.3514,+3.0506, 19600], # 2.5h\n",
    "[ 36, 3, 1, 648, 2592, 256, 262.0, 3.4220, 3.0561, 19600], # again\n",
    "[ 36, 3, 1, 648, 2592, 240, 259.0, 3.3552, 3.0544, 19600], # 2.4h\n",
    "[ 36, 3, 1, 648, 2592, 224, 256.0, 3.3764, 3.0534, 19600], # 2.4h\n",
    "[ 36, 3, 1, 648, 2592, 208, 253.0, 3.4931, 3.0605, 19600], # 2.4h\n",
    "[ 36, 3, 1, 648, 2592, 192, 250.1, 3.3386, 3.0545, 19600], # 2.3h\n",
    "[ 36, 3, 1, 648, 2592, 176, 247.1, 3.3615, 3.0526, 19600], # 2.3h\n",
    "[ 36, 3, 1, 648, 2592, 160, 244.1, 3.4183, 3.0536, 19600], # 2.2h\n",
    "[ 36, 3, 1, 648, 2592, 144, 241.1, 3.4809, 3.0548, 19600], # 2.2h\n",
    "\n",
    "[ 36, 3, 1, 624, 2496, 288, 251.6, 3.3221, 3.0543, 19600], # 2.4h 17.1G\n",
    "[ 36, 3, 1, 624, 2496, 272, 248.7, 3.3680, 3.0532, 19600], # 2.4h\n",
    "[ 36, 3, 1, 624, 2496, 256, 245.8, 3.5078,+3.0496, 19600], # 2.3h\n",
    "[ 36, 3, 1, 624, 2496, 256, 245.8, 3.3772, 3.0536, 19600], # again\n",
    "[ 36, 3, 1, 624, 2496, 240, 242.9, 3.3405, 3.0567, 19600], # 2.2h\n",
    "[ 36, 3, 1, 624, 2496, 224, 240.1, 3.3681, 3.0541, 19600], # 2.2h\n",
    "[ 36, 3, 1, 624, 2496, 208, 237.2, 3.3858,+3.0525, 19600], # 2.2h\n",
    "[ 36, 3, 1, 624, 2496, 192, 234.3, 3.4098, 3.0534, 19600], # 2.1h\n",
    "[ 36, 3, 1, 624, 2496, 176, 231.4, 3.6191, 3.1103, 19600], # 2.1h\n",
    "[ 36, 3, 1, 624, 2496, 160, 228.6, 3.3777, 3.0647, 19600], # 2.0h\n",
    "\n",
    "[ 36, 3, 1, 600, 2400, 304, 238.4, 3.3947, 3.0553, 19600], # 2.4h\n",
    "[ 36, 3, 1, 600, 2400, 288, 235.7, 3.3315, 3.0527, 19600], # 2.4h\n",
    "[ 36, 3, 1, 600, 2400, 272, 232.9, 3.3296, 3.0562, 19600], # 2.3h\n",
    "[ 36, 3, 1, 600, 2400, 256, 230.1, 3.3320, 3.0545, 19600], # 2.2h\n",
    "[ 36, 3, 1, 600, 2400, 240, 227.4, 3.4197, 3.0582, 19600], # 2.2h\n",
    "[ 36, 3, 1, 600, 2400, 224, 224.6, 3.4197, 3.0573, 19600], # 2.1h\n",
    "[ 36, 3, 1, 600, 2400, 208, 221.9, 3.3482, 3.0576, 19600], # 2.1h\n",
    "[ 36, 3, 1, 600, 2400, 192, 219.1, 3.5679, 3.0777, 19600], # 2.0h\n",
    "[ 36, 3, 1, 600, 2400, 176, 216.3, 3.4195, 3.0630, 19600], # 2.0h\n",
    "[ 36, 3, 1, 600, 2400, 160, 213.6, 3.3964, 3.0643, 19600], # 2.0h\n",
    "\n",
    "[ 36, 3, 1, 576, 2304, 288, 220.3, 3.3039, 3.0571, 19600], # 2.3h\n",
    "[ 36, 3, 1, 576, 2304, 272, 217.6, 3.3587,+3.0512, 19600], # 2.2h\n",
    "[ 36, 3, 1, 576, 2304, 256, 215.0, 3.3371, 3.0545, 19600], # 2.1h\n",
    "[ 36, 3, 1, 576, 2304, 240, 212.3, 3.3298, 3.0583, 19600], # 2.1h\n",
    "[ 36, 3, 1, 576, 2304, 224, 209.7, 3.3621, 3.0569, 19600], # 2.0h\n",
    "[ 36, 3, 1, 576, 2304, 208, 207.0, 3.4251, 3.0647, 19600], # 2.0h\n",
    "[ 36, 3, 1, 576, 2304, 192, 204.4, 3.4310, 3.0580, 19600], # 1.9h\n",
    "[ 36, 3, 1, 576, 2304, 176, 201.7, 3.4961, 3.0695, 19600], # 1.9h\n",
    "[ 36, 3, 1, 576, 2304, 160, 199.0, 3.3423, 3.0658, 19600], # 1.9h\n",
    "\n",
    "[ 36, 3, 1, 552, 2208, 288, 205.4, 3.3008, 3.0634, 19600], # 2.2h\n",
    "[ 36, 3, 1, 552, 2208, 272, 202.8, 3.2953, 3.0588, 19600], # 2.2h\n",
    "[ 36, 3, 1, 552, 2208, 256, 200.3, 3.3220, 3.0663, 19600], # 2.1h\n",
    "[ 36, 3, 1, 552, 2208, 240, 197.7, 3.3583, 3.0603, 19600], # 2.1h\n",
    "[ 36, 3, 1, 552, 2208, 224, 195.2, 3.3567, 3.0581, 19600], # 2.0h\n",
    "[ 36, 3, 1, 552, 2208, 208, 192.7, 3.3206, 3.0721, 19600], # 2.0h\n",
    "[ 36, 3, 1, 552, 2208, 192, 190.1, 3.3478, 3.0666, 19600], # 1.9h\n",
    "[ 36, 3, 1, 552, 2208, 176, 187.6, 3.4722, 3.0753, 19600], # 1.9h\n",
    "[ 36, 3, 1, 552, 2208, 160, 185.0, 3.3278, 3.0687, 19600], # 1.9h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "057cd47a-ce8d-44e2-8ed0-72949b0c7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L36 att4 kv_heads2 hidden696 intermediate2784 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:52:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.400100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36, 4, 2, 696, 2784, 240, 316.7, 3.4001, 3.0480, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  36 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*174 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0496:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bfeecb2e-d51c-4c0e-8f91-a1cd5e75e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L36 att3 kv_heads1 hidden672 intermediate2688 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:28:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.380200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36, 3, 1, 672, 2688, 224, 272.5, 3.3802, 3.0482, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  36 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 3 # 4 # G16 G8\n",
    "num_key_value_heads = 1 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*224 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0482:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5d89b-ca90-4dae-92cf-7a3722991302",
   "metadata": {},
   "source": [
    "# L36 (x5, not used in paper figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2518de-e151-4898-83f5-f742766325bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9091f70-a2f5-4a1d-91b6-f038c41ba94e",
   "metadata": {},
   "source": [
    "# L30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99cac9-b669-4ecd-9450-2cea05999741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L30 Normal Model (default: x4 B12 lr13.5e-4 WD1)  # Best model for each number of heads: 3(3):3.0542, 4(2):3.0493, 5(1):3.0498, 6(3):3.0500\n",
    "[ 30, 6, 3, 756, 3024, 256, 348.5, 3.5112, 3.0693, 19600], # 3.1h\n",
    "[ 30, 6, 3, 756, 3024, 240, 342.0, 3.6438, 3.0718, 19600], # 3.0h 18.8G(VRAM)\n",
    "[ 30, 6, 3, 756, 3024, 224, 335.4, 3.3749, 3.0595, 19600], # 2.9h\n",
    "[ 30, 6, 3, 756, 3024, 208, 328.9, 3.5371, 3.0578, 19600], # 2.9h\n",
    "[ 30, 6, 3, 756, 3024, 192, 322.4, 3.4953, 3.0545, 19600], # 2.8h\n",
    "[ 30, 6, 3, 756, 3024, 176, 315.8, 3.4483,+3.0533, 19600], # 2.7h\n",
    "[ 30, 6, 3, 756, 3024, 160, 309.3, 3.4155, 3.0563, 19600], # 2.7h (96,112,128,144,272)\n",
    "\n",
    "[ 30, 6, 3, 720, 2880, 272, 328.8, 3.3990, 3.0544, 19600], # 3.2h\n",
    "[ 30, 6, 3, 720, 2880, 256, 322.6, 3.3970, 3.0576, 19600], # 2.9h\n",
    "[ 30, 6, 3, 720, 2880, 240, 316.4, 3.3645, 3.0542, 19600], # 2.9h\n",
    "[ 30, 6, 3, 720, 2880, 224, 310.1, 3.4218, 3.0572, 19600], # 2.8h\n",
    "[+30, 6, 3, 720, 2880, 208, 303.9, 3.4047,+3.0500, 19600], # 2.8h\n",
    "[ 30, 6, 3, 720, 2880, 192, 297.7, 3.3858,+3.0522, 19600], # 2.6h\n",
    "[ 30, 6, 3, 720, 2880, 176, 291.5, 3.3717,+3.0507, 19600], # 2.6h\n",
    "[ 30, 6, 3, 720, 2880, 160, 285.3, 3.4169,+3.0525, 19600], # 2.5h\n",
    "[ 30, 6, 3, 720, 2880, 144, 279.0, 3.4071, 3.0595, 19600], # 2.5h\n",
    "[ 30, 6, 3, 720, 2880, 128, 272.8, 3.5295, 3.0676, 19600], # 2.1h 16.6G\n",
    "[ 30, 6, 3, 720, 2880, 112, 266.6, 3.4299, 3.0560, 19600], # 2.1h\n",
    "[ 30, 6, 3, 720, 2880,  96, 260.4, 3.4120, 3.0547, 19600],\n",
    "\n",
    "[ 30, 6, 3, 684, 2736, 320, 321.2, 3.4579, 3.0607, 19600], # 3.3h 18.5G\n",
    "[ 30, 6, 3, 684, 2736, 304, 315.3, 3.3796,+3.0527, 19600], # 3.3h\n",
    "[ 30, 6, 3, 684, 2736, 288, 309.4, 3.4154,+3.0536, 19600], # 3.2h\n",
    "[ 30, 6, 3, 684, 2736, 272, 303.5, 3.3830,+3.0514, 19600], # 3.1h\n",
    "[ 30, 6, 3, 684, 2736, 256, 297.6, 3.3459, 3.0570, 19600], # 2.9h\n",
    "[ 30, 6, 3, 684, 2736, 240, 291.7, 3.3628,+3.0520, 19600], # 2.9h\n",
    "[ 30, 6, 3, 684, 2736, 224, 285.8, 3.3681,+3.0519, 19600], # 2.8h\n",
    "[ 30, 6, 3, 684, 2736, 208, 279.9, 3.3712,+3.0513, 19600], # 2.7h\n",
    "[ 30, 6, 3, 684, 2736, 192, 273.9, 3.3857, 3.0541, 19600], # 2.6h\n",
    "[ 30, 6, 3, 684, 2736, 160, 262.1, 3.3688, 3.0559, 19600], # 2.5h\n",
    "[ 30, 6, 3, 684, 2736, 144, 256.2, 3.3902,+3.0525, 19600], # 2.5h\n",
    "[ 30, 6, 3, 684, 2736, 128, 250.3, 3.4015, 3.0580, 19600], # 2.1h\n",
    "[ 30, 6, 3, 684, 2736,  96, 238.5, 3.3598, 3.0613, 19600], # 1.9h\n",
    "\n",
    "[ 30, 6, 3, 648, 2592, 272, 279.1, 3.5865, 3.0815, 19600], # 3.0h\n",
    "[ 30, 6, 3, 648, 2592, 256, 273.5, 3.3280, 3.0543, 19600], # 2.8h\n",
    "[ 30, 6, 3, 648, 2592, 224, 262.3, 3.3216, 3.0544, 19600], # 2.7h 17.0G\n",
    "[ 30, 6, 3, 648, 2592, 192, 251.1, 3.3217, 3.0554, 19600], # 2.5h\n",
    "[ 30, 6, 3, 648, 2592, 176, 245.5, 3.3365, 3.0556, 19600], # 2.4h\n",
    "[ 30, 6, 3, 648, 2592, 160, 239.9, 3.3363, 3.0577, 19600], # 2.7h 15.6G\n",
    "[ 30, 6, 3, 648, 2592, 144, 234.3, 3.3932,+3.0537, 19600], # 17.9h\n",
    "[ 30, 6, 3, 648, 2592, 128, 228.7, 3.3485, 3.0606, 19600], # 1.4h\n",
    "[ 30, 6, 3, 648, 2592, 112, 223.1, 3.3513, 3.0566, 19600], # 1.4h\n",
    "[ 30, 6, 3, 648, 2592,  96, 217.5, 3.3735, 3.0636, 19600], # 1.3h\n",
    "\n",
    "[ 30, 6, 3, 612, 2448, 272, 255.7, 3.3341, 3.0566, 19600], # 2.8h\n",
    "[ 30, 6, 3, 612, 2448, 256, 250.4, 3.3796, 3.0550, 19600], # 2.6h\n",
    "[ 30, 6, 3, 612, 2448, 224, 239.8, 3.3351,+3.0541, 19600], # 2.5h\n",
    "[ 30, 6, 3, 612, 2448, 208, 234.5, 3.2887, 3.0556, 19600], # 2.2h 15.6G\n",
    "[ 30, 6, 3, 612, 2448, 192, 229.2, 3.3320, 3.0550, 19600], # 2.3h \n",
    "[ 30, 6, 3, 612, 2448, 176, 224.0, 3.4298, 3.0543, 19600], # 21.2h\n",
    "[ 30, 6, 3, 612, 2448, 144, 213.4, 3.3164, 3.0625, 19600], # 1.7h\n",
    "[ 30, 6, 3, 612, 2448, 128, 208.1, 3.3355, 3.0635, 19600], # 1.3h\n",
    "[ 30, 6, 3, 612, 2448, 112, 202.8, 3.3323, 3.0654, 19600], # 1.3h\n",
    "[ 30, 6, 3, 612, 2448,  96, 197.5, 3.3270, 3.0643, 19600], # 1.2h\n",
    "\n",
    "[ 30, 6, 3, 576, 2304, 256, 228.2, 3.2910, 3.0560, 19600], # 2.5h\n",
    "[ 30, 6, 3, 576, 2304, 224, 218.3, 3.2848, 3.0603, 19600], # 2.4h 16.0G\n",
    "[ 30, 6, 3, 576, 2304, 192, 208.3, 3.3373, 3.0574, 19600], # 2.2h \n",
    "[ 30, 6, 3, 576, 2304, 176, 203.3, 3.3430, 3.0574, 19600], # 1.8h\n",
    "[ 30, 6, 3, 576, 2304, 144, 193.4, 3.4027, 3.0640, 19600], # 1.6h\n",
    "[ 30, 6, 3, 576, 2304, 128, 188.4, 3.3448, 3.0656, 19600], # 1.2h\n",
    "[ 30, 6, 3, 576, 2304, 112, 183.4, 3.3133, 3.0675, 19600], # 1.2h\n",
    "[ 30, 6, 3, 576, 2304,  96, 178.4, 3.4221, 3.0722, 19600], # 1.1h\n",
    "\n",
    "[ 30, 6, 3, 540, 2160, 256, 206.9, 3.2858, 3.0691, 19600], # 2.5h 15.5G\n",
    "[ 30, 6, 3, 540, 2160, 224, 197.6, 3.3048, 3.0699, 19600], # 2.3h\n",
    "[ 30, 6, 3, 540, 2160, 192, 188.3, 3.2948, 3.0612, 19600], # 2.2h 14.8G\n",
    "[ 30, 6, 3, 540, 2160, 176, 183.6, 3.3108, 3.0590, 19600], # 2.1h\n",
    "[ 30, 6, 3, 540, 2160, 160, 178.9, 3.3201, 3.0617, 19600], # 1.6h 14.1G\n",
    "[ 30, 6, 3, 540, 2160, 144, 174.3, 3.3020, 3.0683, 19600], # 1.6h\n",
    "[ 30, 6, 3, 540, 2160, 128, 169.6, 3.4055, 3.0744, 19600], # 1.2h\n",
    "[ 30, 6, 3, 540, 2160, 112, 165.0, 3.3633, 3.0700, 19600], # 1.2h\n",
    "[ 30, 6, 3, 540, 2160,  96, 160.3, 3.2944, 3.0780, 19600], # 1.1h\n",
    "\n",
    "\n",
    "# 5(5) is TOO big? But there is no 5(2) or 5(3)...\n",
    "[ 30, 5, 5, 760, 3040, 256, 363.1, 3.4868, 3.0620, 19600], # 3.0h\n",
    "[ 30, 5, 5, 760, 3040, 224, 348.5, 3.3850, 3.0563, 19600], # 2.9h\n",
    "[ 30, 5, 5, 760, 3040, 192, 333.9, 3.4469, 3.0570, 19600], # 2.7h\n",
    "[ 30, 5, 5, 760, 3040, 160, 319.3, 3.5307, 3.0586, 19600], # 2.6h\n",
    "\n",
    "[ 30, 5, 5, 720, 2880, 256, 333.6, 3.4710, 3.0630, 19600], # 2.9h\n",
    "[ 30, 5, 5, 720, 2880, 224, 319.8, 3.4334, 3.0571, 19600], # 2.7h\n",
    "[ 30, 5, 5, 720, 2880, 192, 306.0, 3.4144, 3.0546, 19600], # 2.6h\n",
    "[ 30, 5, 5, 720, 2880, 160, 292.2, 3.4221, 3.0557, 19600], # 2.5h\n",
    "[ 30, 5, 5, 720, 2880, 144, 285.3, 3.4572, 3.0557, 19600], # 2.4h\n",
    "[ 30, 5, 5, 720, 2880, 128, 278.3, 3.4040, 3.0613, 19600], # 2.1h\n",
    "[ 30, 5, 5, 720, 2880,  96, 264.5, 3.4217, 3.0650, 19600], # 1.9h\n",
    "\n",
    "[ 30, 5, 5, 700, 2800, 144, 272.3, 3.3942, 3.0580, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 5, 680, 2720, 256, 305.3, 3.4081, 3.0567, 19600], # 2.8h\n",
    "[ 30, 5, 5, 680, 2720, 224, 292.3, 3.3949, 3.0566, 19600], # 2.6h\n",
    "[ 30, 5, 5, 680, 2720, 192, 279.2, 3.4985, 3.0565, 19600], # 2.5h\n",
    "[ 30, 5, 5, 680, 2720, 160, 266.1, 3.4381, 3.0591, 19600], # 2.4h\n",
    "[ 30, 5, 5, 680, 2720, 144, 259.6, 3.5180,+3.0509, 19600], # 2.3h\n",
    "[ 30, 5, 5, 680, 2720, 128, 253.1, 3.3855, 3.0616, 19600], # 2.0h\n",
    "[ 30, 5, 5, 680, 2720,  96, 240.0, 3.3802, 3.0641, 19600], # 1.8h\n",
    "\n",
    "[ 30, 5, 5, 660, 2640, 144, 247.2, 3.3556, 3.0578, 19600], # 2.3h\n",
    "\n",
    "[ 30, 5, 5, 640, 2560, 320, 302.7, 3.3873, 3.0604, 19600], # 2.9h 18.7G\n",
    "[ 30, 5, 5, 640, 2560, 304, 296.6, 3.3551, 3.0568, 19600], # 2.9h\n",
    "[ 30, 5, 5, 640, 2560, 288, 290.4, 3.3438, 3.0575, 19600], # 2.8h\n",
    "[ 30, 5, 5, 640, 2560, 272, 284.3, 3.3421, 3.0635, 19600], # 2.7h\n",
    "[ 30, 5, 5, 640, 2560, 256, 278.1, 3.5115, 3.0738, 19600], # 2.5h\n",
    "[ 30, 5, 5, 640, 2560, 240, 272.0, 3.3667, 3.0617, 19600], # 2.5h\n",
    "[ 30, 5, 5, 640, 2560, 224, 265.8, 3.3682, 3.0582, 19600], # 2.4h\n",
    "[ 30, 5, 5, 640, 2560, 208, 259.7, 3.3680, 3.0557, 19600], # 2.4h\n",
    "[ 30, 5, 5, 640, 2560, 192, 253.6, 3.3625, 3.0576, 19600], # 2.3h\n",
    "[ 30, 5, 5, 640, 2560, 176, 247.4, 3.3677, 3.0556, 19600], # 2.2h\n",
    "[ 30, 5, 5, 640, 2560, 160, 241.3, 3.3420, 3.0565, 19600], # 2.1h\n",
    "[ 30, 5, 5, 640, 2560, 144, 235.1, 3.3597,+3.0526, 19600], # 2.1h\n",
    "[ 30, 5, 5, 640, 2560, 128, 229.0, 3.3500, 3.0618, 19600], # 1.3h\n",
    "[ 30, 5, 5, 640, 2560, 112, 222.8, 3.4009, 3.0617, 19600], # 1.3h\n",
    "[ 30, 5, 5, 640, 2560,  96, 216.7, 3.4304, 3.0598, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 5, 620, 2480, 192, 241.2, 3.3452, 3.0561, 19600], # 2.3h\n",
    "[ 30, 5, 5, 620, 2480, 144, 223.3, 3.4249, 3.0594, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 5, 600, 2400, 256, 252.1, 3.3228, 3.0617, 19600], # 2.5h\n",
    "[ 30, 5, 5, 600, 2400, 224, 240.6, 3.3396, 3.0589, 19600], # 2.4h\n",
    "[ 30, 5, 5, 600, 2400, 208, 234.8, 3.3405, 3.0592, 19600], # 2.3h\n",
    "[ 30, 5, 5, 600, 2400, 192, 229.1, 3.3347,+3.0530, 19600], # 2.2h\n",
    "[ 30, 5, 5, 600, 2400, 176, 223.3, 3.3384, 3.0562, 19600], # 2.2h\n",
    "[ 30, 5, 5, 600, 2400, 160, 217.6, 3.3287, 3.0621, 19600], # 2.1h\n",
    "[ 30, 5, 5, 600, 2400, 144, 211.8, 3.4175,+3.0531, 19600], # 2.0h\n",
    "[ 30, 5, 5, 600, 2400, 128, 206.0, 3.3877, 3.0670, 19600], # 1.3h\n",
    "[ 30, 5, 5, 600, 2400, 112, 200.3, 3.3393, 3.0653, 19600], # 1.3h\n",
    "[ 30, 5, 5, 600, 2400,  96, 194.5, 3.3596, 3.0706, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 5, 580, 2320, 192, 217.3, 3.3805, 3.0620, 19600], # 2.3h\n",
    "[ 30, 5, 5, 580, 2320, 144, 200.6, 3.3160, 3.0621, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 5, 560, 2240, 256, 227.2, 3.3053, 3.0595, 19600], # 2.4h\n",
    "[ 30, 5, 5, 560, 2240, 224, 216.5, 3.3893, 3.0612, 19600], # 2.3h\n",
    "[ 30, 5, 5, 560, 2240, 192, 205.7, 3.3257, 3.0620, 19600], # 2.1h\n",
    "[ 30, 5, 5, 560, 2240, 176, 200.4, 3.3239, 3.0611, 19600], # 2.1h\n",
    "[ 30, 5, 5, 560, 2240, 144, 189.6, 3.3348, 3.0660, 19600], # 1.9h\n",
    "[ 30, 5, 5, 560, 2240, 128, 184.2, 3.4130, 3.0675, 19600], # 1.2h\n",
    "[ 30, 5, 5, 560, 2240, 112, 178.9, 3.3855, 3.0773, 19600], # 1.2h\n",
    "[ 30, 5, 5, 560, 2240,  96, 173.5, 3.3408, 3.0669, 19600], # 1.1h\n",
    "\n",
    "[ 30, 5, 5, 520, 2080, 256, 203.5, 3.3369, 3.0670, 19600], # 2.3h\n",
    "[ 30, 5, 5, 520, 2080, 224, 193.5, 3.2985, 3.0669, 19600], # 2.2h\n",
    "[ 30, 5, 5, 520, 2080, 192, 183.6, 3.2937, 3.0658, 19600], # 2.1h\n",
    "[ 30, 5, 5, 520, 2080, 176, 178.6, 3.3091, 3.0718, 19600], # 2.0h\n",
    "[ 30, 5, 5, 520, 2080, 144, 168.6, 3.3362, 3.0766, 19600], # 1.9h\n",
    "[ 30, 5, 5, 520, 2080, 128, 163.6, 3.2943, 3.0791, 19600], # 1.1h\n",
    "[ 30, 5, 5, 520, 2080, 112, 158.6, 3.3111, 3.0767, 19600], # 1.1h\n",
    "[ 30, 5, 5, 520, 2080,  96, 153.6, 3.3091, 3.0823, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 5, 1, 760, 3040, 288, 325.2, 3.4164, 3.0626, 19600], # 2.9h\n",
    "[ 30, 5, 1, 760, 3040, 256, 316.4, 3.5018, 3.0587, 19600], # 2.7h\n",
    "[ 30, 5, 1, 760, 3040, 224, 307.7, 3.4642, 3.0611, 19600], # 2.6h\n",
    "[ 30, 5, 1, 760, 3040, 192, 298.9, 3.6474, 3.0768, 19600], # 2.5h\n",
    "[ 30, 5, 1, 760, 3040, 160, 290.2, 3.3903, 3.0561, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 1, 740, 2960, 320, 319.8, 3.3693, 3.0632, 19600], # 3.0h\n",
    "[ 30, 5, 1, 740, 2960, 288, 311.3, 3.3513, 3.0565, 19600], # 2.9h\n",
    "[ 30, 5, 1, 740, 2960, 256, 302.8, 3.4523, 3.0587, 19600], # 2.7h\n",
    "\n",
    "[ 30, 5, 1, 720, 2880, 336, 310.1, 3.4626, 3.0556, 19600], # 3.0h\n",
    "[ 30, 5, 1, 720, 2880, 320, 306.0, 3.3628,+3.0522, 19600], # 2.9h\n",
    "[ 30, 5, 1, 720, 2880, 304, 301.8, 3.4707, 3.0615, 19600], # 2.9h\n",
    "[ 30, 5, 1, 720, 2880, 288, 297.7, 3.3806,+3.0498, 19600], # 2.8h  minGemma-hidden_layers30-att_heads5-kv_heads1-hidden720-intermediate2880-head_dim288-T512--2025-06-15-19-41.pth\n",
    "[ 30, 5, 1, 720, 2880, 272, 293.5, 3.3816, 3.0603, 19600], # 2.8h\n",
    "[ 30, 5, 1, 720, 2880, 256, 289.4, 3.3950,+3.0520, 19600], # 2.6h\n",
    "[ 30, 5, 1, 720, 2880, 240, 285.3, 3.3593, 3.0560, 19600],\n",
    "[ 30, 5, 1, 720, 2880, 224, 281.1, 3.5347, 3.0639, 19600], # 2.5h\n",
    "[ 30, 5, 1, 720, 2880, 192, 272.8, 3.3698, 3.0584, 19600], # 2.4h\n",
    "[ 30, 5, 1, 720, 2880, 160, 264.5, 3.3863, 3.0568, 19600], # 2.3h\n",
    "[ 30, 5, 1, 720, 2880, 128, 256.2, 3.3940, 3.0590, 19600], # 2.0h\n",
    "[ 30, 5, 1, 720, 2880,  96, 247.9, 3.5566, 3.0732, 19600], # 1.8h\n",
    "\n",
    "[ 30, 5, 1, 700, 2800, 320, 292.5, 3.3813, 3.0546, 19600], # 2.9h\n",
    "[ 30, 5, 1, 700, 2800, 288, 284.4, 3.4136, 3.0585, 19600], # 2.8h\n",
    "[ 30, 5, 1, 700, 2800, 272, 280.4, 3.3201,+3.0525, 19600], # 2.8h\n",
    "[ 30, 5, 1, 700, 2800, 256, 276.3, 3.3776,+3.0532, 19600], # 2.6h\n",
    "[ 30, 5, 1, 700, 2800, 240, 272.3, 3.3686, 3.0561, 19600], # 2.6h\n",
    "[ 30, 5, 1, 700, 2800, 176, 256.2, 3.3420, 3.0560, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 1, 680, 2720, 288, 271.4, 3.4339, 3.0568, 19600],\n",
    "[ 30, 5, 1, 680, 2720, 272, 267.4, 3.3299,+3.0501, 19600], # 2.7h 17.0G\n",
    "[ 30, 5, 1, 680, 2720, 256, 263.5, 3.3431,+3.0532, 19600], # 2.5h 17.0G\n",
    "[ 30, 5, 1, 680, 2720, 240, 259.6, 3.3514, 3.0536, 19600], # 2.5h\n",
    "[ 30, 5, 1, 680, 2720, 224, 255.7, 3.3404, 3.0575, 19600], # 2.4h\n",
    "[ 30, 5, 1, 680, 2720, 192, 247.9, 3.3513, 3.0585, 19600], # 2.3h\n",
    "[ 30, 5, 1, 680, 2720, 176, 243.9, 3.3583, 3.0535, 19600], # 2.3h\n",
    "[ 30, 5, 1, 680, 2720, 160, 240.0, 3.4630, 3.0580, 19600], # 2.2h\n",
    "[ 30, 5, 1, 680, 2720, 128, 232.2, 3.4498, 3.0575, 19600], # 1.9h\n",
    "[ 30, 5, 1, 680, 2720,  96, 224.4, 3.3838, 3.0607, 19600], # 1.7h\n",
    "\n",
    "[ 30, 5, 1, 660, 2640, 272, 254.8, 3.3152, 3.0546, 19600],\n",
    "[ 30, 5, 1, 660, 2640, 256, 251.0, 3.3693, 3.0546, 19600], # 2.5h\n",
    "[ 30, 5, 1, 660, 2640, 176, 232.0, 3.4496, 3.0584, 19600], # 2.3h\n",
    "\n",
    "[ 30, 5, 1, 640, 2560, 272, 242.5, 3.3231, 3.0550, 19600], # 2.5h\n",
    "[ 30, 5, 1, 640, 2560, 256, 238.8, 3.3305, 3.0584, 19600], # 2.3h\n",
    "[ 30, 5, 1, 640, 2560, 224, 231.4, 3.3544, 3.0561, 19600], # 2.2h\n",
    "[ 30, 5, 1, 640, 2560, 192, 224.1, 3.3839, 3.0591, 19600], # 2.1h 15.7G\n",
    "[ 30, 5, 1, 640, 2560, 176, 220.4, 3.3457,+3.0532, 19600], # 2.0h\n",
    "[ 30, 5, 1, 640, 2560, 160, 216.7, 3.3194, 3.0593, 19600], # 1.6h 15.2G\n",
    "[ 30, 5, 1, 640, 2560, 144, 213.0, 3.3429, 3.0671, 19600], # 1.6h\n",
    "[ 30, 5, 1, 640, 2560, 128, 209.3, 3.3543, 3.0632, 19600], # 1.3h\n",
    "[ 30, 5, 1, 640, 2560, 112, 205.6, 3.3628, 3.0705, 19600], # 1.3h\n",
    "[ 30, 5, 1, 640, 2560,  96, 202.0, 3.3768, 3.0705, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 1, 620, 2480, 176, 209.0, 3.3276, 3.0621, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 1, 600, 2400, 256, 215.2, 3.2990, 3.0638, 19600], # 2.2h\n",
    "[ 30, 5, 1, 600, 2400, 224, 208.3, 3.3282, 3.0570, 19600], # 2.1h\n",
    "[ 30, 5, 1, 600, 2400, 192, 201.4, 3.3335, 3.0591, 19600], # 2.0h\n",
    "[ 30, 5, 1, 600, 2400, 176, 198.0, 3.3205, 3.0565, 19600], # 2.0h\n",
    "[ 30, 5, 1, 600, 2400, 160, 194.5, 3.3683, 3.0618, 19600], # 1.9h\n",
    "[ 30, 5, 1, 600, 2400, 144, 191.1, 3.3659, 3.0629, 19600], # 1.5h\n",
    "[ 30, 5, 1, 600, 2400, 128, 187.6, 3.3189, 3.0649, 19600], # 1.2h\n",
    "[ 30, 5, 1, 600, 2400, 112, 184.1, 3.3267, 3.0677, 19600], # 1.2h\n",
    "[ 30, 5, 1, 600, 2400,  96, 180.7, 3.3310, 3.0822, 19600], # 1.1h\n",
    "\n",
    "[ 30, 5, 1, 560, 2240, 256, 192.8, 3.4028, 3.0589, 19600], # 2.1h\n",
    "[ 30, 5, 1, 560, 2240, 224, 186.4, 3.3118, 3.0637, 19600], # 2.0h\n",
    "[ 30, 5, 1, 560, 2240, 192, 179.9, 3.2912, 3.0641, 19600], # 1.9h\n",
    "[ 30, 5, 1, 560, 2240, 160, 173.5, 3.4181, 3.0673, 19600], # 1.8h\n",
    "[ 30, 5, 1, 560, 2240, 144, 170.3, 3.3355, 3.0689, 19600], # 1.5h\n",
    "[ 30, 5, 1, 560, 2240, 128, 167.0, 3.2986, 3.0749, 19600], # 1.1h\n",
    "[ 30, 5, 1, 560, 2240, 112, 163.8, 3.3302, 3.0713, 19600], # 1.1h\n",
    "[ 30, 5, 1, 560, 2240,  96, 160.6, 3.3252, 3.0808, 19600], # 1.0h\n",
    "\n",
    "[ 30, 5, 1, 520, 2080, 256, 171.6, 3.2779, 3.0708, 19600], # 2.1h\n",
    "[ 30, 5, 1, 520, 2080, 224, 165.6, 3.2791, 3.0686, 19600], # 2.0h\n",
    "[ 30, 5, 1, 520, 2080, 192, 159.6, 3.3124, 3.0675, 19600], # 1.9h 14.5G\n",
    "[ 30, 5, 1, 520, 2080, 160, 153.6, 3.3038, 3.0811, 19600], # 1.8h\n",
    "[ 30, 5, 1, 520, 2080, 144, 150.6, 3.2904, 3.0879, 19600], # 1.4h\n",
    "[ 30, 5, 1, 520, 2080, 128, 147.6, 3.2620, 3.0870, 19600], # 1.1h\n",
    "[ 30, 5, 1, 520, 2080, 112, 144.6, 3.3155, 3.0899, 19600], # 1.1h\n",
    "[ 30, 5, 1, 520, 2080,  96, 141.6, 3.3356, 3.0786, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 4, 4, 744, 2976, 256, 328.3, 3.5070, 3.0617, 19600], # 2.7h\n",
    "[ 30, 4, 4, 744, 2976, 224, 316.9, 3.5518, 3.0663, 19600], # 2.6h\n",
    "[ 30, 4, 4, 744, 2976, 192, 305.5, 3.4527, 3.0577, 19600], # 2.4h\n",
    "[ 30, 4, 4, 744, 2976, 160, 294.0, 3.5288, 3.0617, 19600], # 2.3h\n",
    "[ 30, 4, 4, 744, 2976, 128, 282.6, 3.4108, 3.0561, 19600], # 2.0h\n",
    "[ 30, 4, 4, 744, 2976,  96, 271.2, 3.4913, 3.0648, 19600], # 1.9h\n",
    "\n",
    "[ 30, 4, 4, 696, 2784, 256, 295.1, 3.4248, 3.0570, 19600], # 2.6h\n",
    "[ 30, 4, 4, 696, 2784, 224, 284.4, 3.4183, 3.0555, 19600], # 2.4h\n",
    "[ 30, 4, 4, 696, 2784, 192, 273.7, 3.4213, 3.0593, 19600],\n",
    "[ 30, 4, 4, 696, 2784, 160, 263.1, 3.4704, 3.0564, 19600], # 2.2h\n",
    "[ 30, 4, 4, 696, 2784, 128, 252.4, 3.4785, 3.0578, 19600], # 1.9h\n",
    "[ 30, 4, 4, 696, 2784,  96, 241.7, 3.4670, 3.0620, 19600], # 1.8h\n",
    "\n",
    "[ 30, 4, 4, 648, 2592, 288, 273.5, 3.4527, 3.0625, 19600], # 2.7h\n",
    "[ 30, 4, 4, 648, 2592, 272, 268.5, 3.3627, 3.0542, 19600], # 2.6h\n",
    "[ 30, 4, 4, 648, 2592, 256, 263.6, 3.3826, 3.0544, 19600], # 2.5h\n",
    "[ 30, 4, 4, 648, 2592, 240, 258.6, 3.3735, 3.0552, 19600], # 2.4h\n",
    "[ 30, 4, 4, 648, 2592, 224, 253.6, 3.3645, 3.0541, 19600], # 2.4h\n",
    "[ 30, 4, 4, 648, 2592, 208, 248.6, 3.3461, 3.0548, 19600], # 2.3h\n",
    "[ 30, 4, 4, 648, 2592, 192, 243.7, 3.3766,+3.0540, 19600], # 2.2h\n",
    "[ 30, 4, 4, 648, 2592, 176, 238.7, 3.4333, 3.0591, 19600], # 2.2h\n",
    "[ 30, 4, 4, 648, 2592, 144, 228.7, 3.3872, 3.0582, 19600], # 2.1h\n",
    "[ 30, 4, 4, 648, 2592, 128, 223.8, 3.3698, 3.0633, 19600], # 1.3h\n",
    "[ 30, 4, 4, 648, 2592, 112, 218.8, 3.3654, 3.0600, 19600], # 1.3h\n",
    "[ 30, 4, 4, 648, 2592,  96, 213.8, 3.4010, 3.0700, 19600], # 1.2h\n",
    "\n",
    "[ 30, 4, 4, 600, 2400, 256, 233.7, 3.3319, 3.0594, 19600], # 2.2h\n",
    "[ 30, 4, 4, 600, 2400, 224, 224.5, 3.3309, 3.0574, 19600], # 2.2h\n",
    "[ 30, 4, 4, 600, 2400, 192, 215.2, 3.3723, 3.0591, 19600], # 2.0h\n",
    "[ 30, 4, 4, 600, 2400, 160, 206.0, 3.3955, 3.0641, 19600], # 1.9h\n",
    "[ 30, 4, 4, 600, 2400, 144, 201.4, 3.3313, 3.0638, 19600], # 1.5h\n",
    "[ 30, 4, 4, 600, 2400, 128, 196.8, 3.3538, 3.0690, 19600], # 1.2h\n",
    "[ 30, 4, 4, 600, 2400, 112, 192.2, 3.3520, 3.0625, 19600], # 1.2h\n",
    "[ 30, 4, 4, 600, 2400,  96, 187.6, 3.3686, 3.0737, 19600], # 1.1h\n",
    "\n",
    "[ 30, 4, 4, 552, 2208, 256, 205.4, 3.3888, 3.0657, 19600], # 2.1h\n",
    "[ 30, 4, 4, 552, 2208, 224, 197.0, 3.3201, 3.0626, 19600], # 2.0h\n",
    "[ 30, 4, 4, 552, 2208, 192, 188.5, 3.3213, 3.0679, 19600], # 1.9h\n",
    "[ 30, 4, 4, 552, 2208, 160, 180.0, 3.3181, 3.0645, 19600], # 1.8h\n",
    "[ 30, 4, 4, 552, 2208, 144, 175.8, 3.2948, 3.0709, 19600], # 1.4h\n",
    "[ 30, 4, 4, 552, 2208, 128, 171.5, 3.3072, 3.0667, 19600], # 1.1h\n",
    "[ 30, 4, 4, 552, 2208, 112, 167.3, 3.3141, 3.0779, 19600], # 1.1h\n",
    "[ 30, 4, 4, 552, 2208,  96, 163.1, 3.3914, 3.0784, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 4, 2, 768, 3072, 176, 299.8, 3.4905, 3.0542, 19600], # 2.3h\n",
    "\n",
    "[ 30, 4, 2, 744, 2976, 256, 305.5, 3.5984, 3.0655, 19600], # 2.6h\n",
    "[ 30, 4, 2, 744, 2976, 224, 296.9, 3.4216, 3.0547, 19600], # 2.5h\n",
    "[ 30, 4, 2, 744, 2976, 192, 288.3, 3.4492, 3.0541, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 176, 284.1, 3.3963,+3.0526, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 160, 279.8, 3.4147, 3.0579, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 144, 275.5, 3.3991, 3.0551, 19600], # 2.2h\n",
    "[ 30, 4, 2, 744, 2976, 128, 271.2, 3.4006, 3.0539, 19600], # 2.0h\n",
    "[ 30, 4, 2, 744, 2976,  96, 262.6, 3.4483, 3.0652, 19600], # 1.8h\n",
    "\n",
    "[ 30, 4, 2, 720, 2880, 288, 297.7, 3.4547, 3.0581, 19600], # 2.7h 17.0G\n",
    "[ 30, 4, 2, 720, 2880, 272, 293.5, 3.4387, 3.0562, 19600], # 2.6h\n",
    "[ 30, 4, 2, 720, 2880, 256, 289.4, 3.5357, 3.0568, 19600], # 2.5h\n",
    "[ 30, 4, 2, 720, 2880, 240, 285.3, 3.4114, 3.0535, 19600], # 2.5h\n",
    "[ 30, 4, 2, 720, 2880, 224, 281.1, 3.3812, 3.0547, 19600], # 2.4h\n",
    "[ 30, 4, 2, 720, 2880, 208, 277.0, 3.5865,+3.0529, 19600], # 2.4h\n",
    "[ 30, 4, 2, 720, 2880, 192, 272.8, 3.4129, 3.0553, 19600], # 2.3h 16.3G\n",
    "[ 30, 4, 2, 720, 2880, 160, 264.5, 3.4269, 3.0572, 19600], # 2.2h\n",
    "[ 30, 4, 2, 720, 2880, 144, 260.4, 3.4326, 3.0532, 19600], # 2.2h\n",
    "\n",
    "[ 30, 4, 2, 696, 2784, 256, 273.7, 3.4693, 3.0546, 19600], # 2.4h\n",
    "[ 30, 4, 2, 696, 2784, 224, 265.7, 3.3820, 3.0544, 19600], # 2.3h\n",
    "[ 30, 4, 2, 696, 2784, 208, 261.7, 3.4757, 3.0571, 19600], # 2.3h 16.2G\n",
    "[ 30, 4, 2, 696, 2784, 192, 257.7, 3.4217,+3.0518, 19600], # 2.2h\n",
    "[ 30, 4, 2, 696, 2784, 176, 253.7, 3.3886,+3.0505, 19600], # 2.2h\n",
    "[ 30, 4, 2, 696, 2784, 160, 249.7, 3.4924, 3.0585, 19600], # 2.1h\n",
    "[ 30, 4, 2, 696, 2784, 128, 241.7, 3.5089, 3.0620, 19600], # 1.9h 16.0G\n",
    "[ 30, 4, 2, 696, 2784,  96, 233.7, 3.4015, 3.0658, 19600], # 1.7h\n",
    "\n",
    "[ 30, 4, 2, 672, 2688, 288, 266.2, 3.3754,+3.0522, 19600], # 2.5h\n",
    "[ 30, 4, 2, 672, 2688, 272, 262.4, 3.3835, 3.0531, 19600], # 2.5h\n",
    "[ 30, 4, 2, 672, 2688, 256, 258.5, 3.3646, 3.0522, 19600], # 2.4h\n",
    "[ 30, 4, 2, 672, 2688, 240, 254.6, 3.3848,+3.0494, 19600], # 2.3h 16.1G # 2nd BEST\n",
    "[ 30, 4, 2, 672, 2688, 240, 254.6, 3.3810, 3.0509, 19600], # again\n",
    "[ 30, 4, 2, 672, 2688, 224, 250.8, 3.4306, 3.0564, 19600], # 2.3h 16.2G\n",
    "[ 30, 4, 2, 672, 2688, 208, 246.9, 3.4401, 3.0532, 19600], # 2.2h\n",
    "[ 30, 4, 2, 672, 2688, 192, 243.0, 3.4030, 3.0552, 19600], # 2.2h\n",
    "[ 30, 4, 2, 672, 2688, 160, 235.3, 3.5064, 3.0621, 19600], # 2.1h\n",
    "[ 30, 4, 2, 672, 2688, 144, 231.4, 3.4365, 3.0534, 19600], # 2.0h\n",
    "\n",
    "[ 30, 4, 2, 648, 2592, 256, 243.7, 3.5282, 3.0651, 19600], # 2.3h\n",
    "[ 30, 4, 2, 648, 2592, 240, 239.9, 3.3948, 3.0552, 19600], # 2.3h\n",
    "[+30, 4, 2, 648, 2592, 224, 236.2, 3.3476,+3.0493, 19600], # 2.3h 16.7G  minGemma-hidden_layers30-att_heads4-kv_heads2-hidden648-intermediate2592-head_dim224-T512--2025-06-07-15-50.pth\n",
    "[ 30, 4, 2, 648, 2592, 224, 236.2, 3.4655, 3.0500, 19600], # again\n",
    "[ 30, 4, 2, 648, 2592, 208, 232.5, 3.3631,+3.0526, 19600], # 2.2h\n",
    "[ 30, 4, 2, 648, 2592, 192, 228.7, 3.3444,+3.0529, 19600], # 2.2h\n",
    "[ 30, 4, 2, 648, 2592, 176, 225.0, 3.3478, 3.0550, 19600], # 14.9h\n",
    "[ 30, 4, 2, 648, 2592, 144, 217.5, 3.3630, 3.0581, 19600], # 1.6h\n",
    "[ 30, 4, 2, 648, 2592, 128, 213.8, 3.3636, 3.0593, 19600], # 1.3h\n",
    "[ 30, 4, 2, 648, 2592, 112, 210.1, 3.3532, 3.0654, 19600], # 1.3h\n",
    "[ 30, 4, 2, 648, 2592,  96, 206.3, 3.3662, 3.0704, 19600], # 1.2h\n",
    "\n",
    "[ 30, 4, 2, 624, 2496, 288, 236.4, 3.3387, 3.0555, 19600], # 2.3h\n",
    "[ 30, 4, 2, 624, 2496, 272, 232.8, 3.3566, 3.0572, 19600], # 2.3h\n",
    "[ 30, 4, 2, 624, 2496, 256, 229.3, 3.3961,+3.0519, 19600], # 2.2h\n",
    "[ 30, 4, 2, 624, 2496, 240, 225.7, 3.3466, 3.0565, 19600], # 2.1h\n",
    "[ 30, 4, 2, 624, 2496, 224, 222.1, 3.3535, 3.0603, 19600], # 2.1h\n",
    "[ 30, 4, 2, 624, 2496, 208, 218.5, 3.3998, 3.0553, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 192, 214.9, 3.3619, 3.0551, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 176, 211.3, 3.4264, 3.0626, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 160, 207.7, 3.3471,+3.0529, 19600], # 1.9h\n",
    "[ 30, 4, 2, 624, 2496, 144, 204.1, 3.3446, 3.0577, 19600], # 1.9h\n",
    "\n",
    "[ 30, 4, 2, 600, 2400, 272, 218.7, 3.3463, 3.0565, 19600], # 2.3h\n",
    "[ 30, 4, 2, 600, 2400, 256, 215.2, 3.3483, 3.0567, 19600], # 2.1h\n",
    "[ 30, 4, 2, 600, 2400, 240, 211.8, 3.3763,+3.0527, 19600], # 2.1h\n",
    "[ 30, 4, 2, 600, 2400, 224, 208.3, 3.3332, 3.0601, 19600], # 1.6h\n",
    "[ 30, 4, 2, 600, 2400, 192, 201.4, 3.3268, 3.0593, 19600], # 1.5h\n",
    "[ 30, 4, 2, 600, 2400, 160, 194.5, 3.3324, 3.0613, 19600], # 1.5h\n",
    "[ 30, 4, 2, 600, 2400, 128, 187.6, 3.3435, 3.0633, 19600], # 1.2h\n",
    "[ 30, 4, 2, 600, 2400, 112, 184.1, 3.4804, 3.0720, 19600], # 1.2h\n",
    "[ 30, 4, 2, 600, 2400,  96, 180.7, 3.4408, 3.0762, 19600], # 1.1h\n",
    "\n",
    "[ 30, 4, 2, 576, 2304, 240, 198.3, 3.3021, 3.0568, 19600], # 2.0h\n",
    "\n",
    "[ 30, 4, 2, 552, 2208, 288, 194.8, 3.3074, 3.0585, 19600], # 2.2h 15.1G\n",
    "[ 30, 4, 2, 552, 2208, 256, 188.5, 3.3132, 3.0607, 19600], # 2.0h\n",
    "[ 30, 4, 2, 552, 2208, 240, 185.3, 3.3064, 3.0645, 19600], # 1.6h\n",
    "[ 30, 4, 2, 552, 2208, 208, 179.0, 3.2810, 3.0668, 19600],\n",
    "[ 30, 4, 2, 552, 2208, 176, 172.6, 3.2951, 3.0700, 19600], # 1.4h\n",
    "[ 30, 4, 2, 552, 2208, 144, 166.2, 3.3499, 3.0692, 19600], # 1.4h\n",
    "[ 30, 4, 2, 552, 2208, 128, 163.1, 3.3455, 3.0768, 19600], # 1.1h\n",
    "[ 30, 4, 2, 552, 2208, 112, 159.9, 3.3103, 3.0785, 19600], # 1.1h\n",
    "[ 30, 4, 2, 552, 2208,  96, 156.7, 3.3366, 3.0854, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 3, 3, 744, 2976, 256, 305.5, 3.4562, 3.0603, 19600], # 2.4h\n",
    "[ 30, 3, 3, 744, 2976, 224, 296.9, 3.4309, 3.0558, 19600], # 2.3h\n",
    "[ 30, 3, 3, 744, 2976, 192, 288.3, 3.4363, 3.0584, 19600], # 2.2h\n",
    "[ 30, 3, 3, 744, 2976, 160, 279.8, 3.4955, 3.0554, 19600], # 2.2h\n",
    "[ 30, 3, 3, 744, 2976, 128, 271.2, 3.4472, 3.0570, 19600], # 1.9h\n",
    "[ 30, 3, 3, 744, 2976,  96, 262.6, 3.4764, 3.0643, 19600], # 1.8h\n",
    "\n",
    "[ 30, 3, 3, 696, 2784, 272, 277.8, 3.4326, 3.0586, 19600], # 2.4h\n",
    "[ 30, 3, 3, 696, 2784, 256, 273.7, 3.4425,+3.0542, 19600], # 2.3h 16.7G\n",
    "[ 30, 3, 3, 696, 2784, 240, 269.7, 3.4506, 3.0576, 19600], # 2.3h\n",
    "[ 30, 3, 3, 696, 2784, 224, 265.7, 3.3916, 3.0597, 19600], # 2.2h\n",
    "[ 30, 3, 3, 696, 2784, 192, 257.7, 3.4272, 3.0578, 19600], # 2.1h\n",
    "[ 30, 3, 3, 696, 2784, 160, 249.7, 3.4733, 3.0598, 19600], # 2.0h\n",
    "[ 30, 3, 3, 696, 2784, 128, 241.7, 3.4498, 3.0583, 19600], # 1.8h\n",
    "[ 30, 3, 3, 696, 2784,  96, 233.7, 3.4212, 3.0630, 19600], # 1.7h\n",
    "\n",
    "[ 30, 3, 3, 648, 2592, 256, 243.7, 3.4881, 3.0622, 19600], # 2.2h\n",
    "[ 30, 3, 3, 648, 2592, 224, 236.2, 3.4027, 3.0580, 19600], # 2.1h\n",
    "[ 30, 3, 3, 648, 2592, 208, 232.5, 3.4521, 3.0634, 19600], # 1.6h\n",
    "[ 30, 3, 3, 648, 2592, 176, 225.0, 3.4189, 3.0618, 19600], # 1.5h 15.3G\n",
    "[ 30, 3, 3, 648, 2592, 144, 217.5, 3.3768, 3.0614, 19600], # 1.5h 15.5G\n",
    "[ 30, 3, 3, 648, 2592, 128, 213.8, 3.3747, 3.0598, 19600], # 1.3h\n",
    "[ 30, 3, 3, 648, 2592, 112, 210.1, 3.4036, 3.0695, 19600], # 1.3h\n",
    "[ 30, 3, 3, 648, 2592,  96, 206.3, 3.4314, 3.0673, 19600], # 1.2h\n",
    "\n",
    "[ 30, 3, 3, 600, 2400, 256, 215.2, 3.3999, 3.0619, 19600], # 2.0h 15.8G\n",
    "[ 30, 3, 3, 600, 2400, 224, 208.3, 3.5221, 3.0700, 19600], # 1.9h \n",
    "[ 30, 3, 3, 600, 2400, 208, 204.9, 3.3432, 3.0638, 19600], # 1.5h 14.9G\n",
    "[ 30, 3, 3, 600, 2400, 176, 198.0, 3.3879, 3.0648, 19600], # 1.4h 14.8G\n",
    "[ 30, 3, 3, 600, 2400, 144, 191.1, 3.3947, 3.0616, 19600], # 1.3h\n",
    "[ 30, 3, 3, 600, 2400, 128, 187.6, 3.4213, 3.0613, 19600], # 1.1h\n",
    "[ 30, 3, 3, 600, 2400, 112, 184.1, 3.3592, 3.0644, 19600], # 1.1h 14.4G\n",
    "[ 30, 3, 3, 600, 2400,  96, 180.7, 3.5229, 3.0751, 19600], # 1.1h\n",
    "\n",
    "[ 30, 3, 3, 552, 2208, 256, 188.5, 3.3234, 3.0605, 19600], # 1.9h\n",
    "[ 30, 3, 3, 552, 2208, 224, 182.1, 3.3561, 3.0695, 19600], # 1.8h\n",
    "[ 30, 3, 3, 552, 2208, 208, 179.0, 3.3546, 3.0735, 19600], # 1.4h 14.1G\n",
    "[ 30, 3, 3, 552, 2208, 176, 172.6, 3.3689, 3.0672, 19600], # 1.3h\n",
    "[ 30, 3, 3, 552, 2208, 144, 166.2, 3.3299, 3.0734, 19600], # 1.3h\n",
    "[ 30, 3, 3, 552, 2208, 128, 163.1, 3.3513, 3.0766, 19600], # 1.1h\n",
    "[ 30, 3, 3, 552, 2208, 112, 159.9, 3.3357, 3.0823, 19600], # 1.0h\n",
    "[ 30, 3, 3, 552, 2208,  96, 156.7, 3.3515, 3.0824, 19600], # 1.0h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f7c48899-34f2-4c71-8b26-bf979ee6a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att4 kv_heads2 hidden648 intermediate2592 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:18:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.347600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 4, 2, 648, 2592, 224, 236.2, 3.3476, 3.0493, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c6b0941b-a43f-4297-98e2-182bee855d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att5 kv_heads1 hidden720 intermediate2880 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:52:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.380600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 5, 1, 720, 2880, 288, 297.7, 3.3806, 3.0498, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 5 # 4 # G16 G8\n",
    "num_key_value_heads = 1 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*144 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49bda719-6f20-44e2-b508-509c5d9b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att6 kv_heads3 hidden720 intermediate2880 head_dim208 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:50:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.404700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 6, 3, 720, 2880, 208, 303.9, 3.4047, 3.0500, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*120 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 208 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a744e-9d7f-4591-b105-1af96ef79b69",
   "metadata": {},
   "source": [
    "# L28 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3078f96-fed1-4f36-b64a-ed357b010064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L28 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 28, 6, 3, 672, 2688, 192, 250.7, 3.4478, 3.0530, 19600], # 1.9h 15.4G\n",
    "[ 28, 6, 3, 672, 2688, 160, 239.9, 3.3624, 3.0524, 19600], # 1.8h 15.3G\n",
    "[ 28, 6, 3, 672, 2688, 128, 229.1, 3.3520, 3.0544, 19600], # 1.4h\n",
    "[+28, 6, 3, 648, 2592, 240, 252.2, 3.3411,+3.0502, 19600], # 2.1h 15.6G\n",
    "[ 28, 6, 3, 648, 2592, 224, 247.0, 3.3187, 3.0566, 19600], # 2.0h 15.6G\n",
    "[ 28, 6, 3, 648, 2592, 160, 226.1, 3.3505, 3.0544, 19600], # 1.8h\n",
    "[ 28, 6, 3, 648, 2592, 128, 215.7, 3.4224, 3.0643, 19600], # 1.3h\n",
    "[ 28, 6, 3, 648, 2592,  96, 205.2, 3.3543, 3.0629, 19600], # 1.2h\n",
    "[ 28, 6, 3, 612, 2448, 192, 216.0, 3.3135, 3.0522, 19600], # 1.8h\n",
    "[ 28, 6, 3, 576, 2304, 192, 196.3, 3.3502, 3.0646, 19600], # 1.7h\n",
    "[ 28, 4, 2, 648, 2592, 240, 226.1, 3.3823,+3.0515, 19600], # 1.7h 15.2G\n",
    "[ 28, 4, 2, 648, 2592, 224, 222.6, 3.4075, 3.0567, 19600], # 1.7h 15.1G\n",
    "[ 28, 4, 2, 616, 2464, 240, 208.3, 3.3324, 3.0567, 19600], # 1.6h 14.8G\n",
    "[ 28, 3, 3, 648, 2592, 240, 226.1, 3.4360, 3.0578, 19600], # 1.6h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a7e224-acbc-4ef2-99f3-9cdd2cd3e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\anaconda3\\envs\\minGemma\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L28 att6 kv_heads3 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\AppData\\Local\\Temp\\ipykernel_6572\\1931068396.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:06:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.341100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28, 6, 3, 648, 2592, 240, 252.2, 3.3411, 3.0502, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  28 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*108 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0502:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7e4813-54ac-42da-bb70-8bb733e58ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L28 att4 kv_heads2 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:44:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.382300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28, 4, 2, 648, 2592, 240, 226.1, 3.3823, 3.0515, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  28 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Linear Single Epoch # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0502:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0be87-cc3e-4021-abe8-e405467921f8",
   "metadata": {},
   "source": [
    "# L26 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759f375-5189-43dd-9baf-fcbd8b9dfc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L26 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 26, 6, 3, 648, 2592, 240, 236.5, 3.3565, 3.0574, 19600], # 1.9h\n",
    "[+26, 4, 4, 648, 2592, 240, 228.5, 3.3902,+3.0504, 19600], # 1.7h\n",
    "[ 26, 4, 2, 648, 2592, 240, 212.3, 3.3869, 3.0558, 19600], # 1.6h\n",
    "[ 26, 3, 3, 648, 2592, 240, 212.3, 3.4225, 3.0616, 19600], # 1.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d253e43-998d-4917-8f38-f96c091c7dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L26 att4 kv_heads4 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:42:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.390200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 26, 4, 4, 648, 2592, 240, 228.5, 3.3902, 3.0504, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  26 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0504:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fd481-14c7-455b-97cd-c15daba1d1ac",
   "metadata": {},
   "source": [
    "# L24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb319f-4601-4259-9160-1d8547e73812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L24 Normal (default: x4 B12 lr13.5e-4 WD1)\n",
    "\n",
    "[ 24, 8, 4, 800, 3200, 320, 372.2, 3.6540, 3.0723, 19600], # 3.5h\n",
    "[ 24, 8, 4, 800, 3200, 272, 350.1, 3.5770, 3.0645, 19600], # 3.3h\n",
    "[ 24, 8, 4, 800, 3200, 256, 342.7, 3.4613, 3.0607, 19600], # 3.1h 18.5G\n",
    "[ 24, 8, 4, 800, 3200, 224, 328.0, 3.4424, 3.0589, 19600], # 2.9h 17.4G\n",
    "[ 24, 8, 4, 800, 3200, 192, 313.2, 3.5217, 3.0613, 19600], # 2.8h\n",
    "[ 24, 8, 4, 800, 3200, 144, 291.1, 3.4425, 3.0573, 19600],\n",
    "[ 24, 8, 4, 800, 3200, 128, 283.7, 3.4433, 3.0566, 19600],\n",
    "\n",
    "[ 24, 8, 4, 768, 3072, 320, 350.2, 3.5420, 3.0610, 19600], # 3.2h\n",
    "[ 24, 8, 4, 768, 3072, 272, 329.0, 3.4347, 3.0608, 19600], # 3.0h\n",
    "[ 24, 8, 4, 768, 3072, 256, 321.9, 3.4499, 3.0583, 19600], # 2.8h\n",
    "[ 24, 8, 4, 768, 3072, 224, 307.8, 3.3925, 3.0604, 19600], # 2.7h 17.0G\n",
    "[ 24, 8, 4, 768, 3072, 192, 293.6, 3.4537, 3.0622, 19600], # 2.5h\n",
    "[ 24, 8, 4, 768, 3072, 160, 279.4, 3.4423, 3.0590, 19600], # 2.4h\n",
    "[ 24, 8, 4, 768, 3072, 144, 272.4, 3.4194,+3.0517, 19600],\n",
    "[ 24, 8, 4, 768, 3072, 128, 265.3, 3.4590, 3.0554, 19600], # 1.4h\n",
    "\n",
    "[ 24, 8, 4, 736, 2944, 320, 328.9, 3.4044, 3.0587, 19600], # 3.2h\n",
    "[ 24, 8, 4, 736, 2944, 272, 308.5, 3.4012, 3.0594, 19600], # 3.0h\n",
    "[ 24, 8, 4, 736, 2944, 256, 301.7, 3.4340, 3.0602, 19600], # 2.8h\n",
    "[ 24, 8, 4, 736, 2944, 224, 288.2, 3.4336, 3.0599, 19600], # 2.6h\n",
    "[ 24, 8, 4, 736, 2944, 192, 274.6, 3.3734, 3.0591, 19600], # 2.5h\n",
    "[ 24, 8, 4, 736, 2944, 176, 267.8, 3.4050, 3.0589, 19600], # 2.4h\n",
    "[ 24, 8, 4, 736, 2944, 160, 261.0, 3.4018,+3.0498, 19600], # 2.3h\n",
    "[ 24, 8, 4, 736, 2944, 144, 254.2, 3.3894,+3.0528, 19600], # 1.8h\n",
    "[ 24, 8, 4, 736, 2944, 128, 247.5, 3.4029, 3.0610, 19600], # 1.4h\n",
    "\n",
    "[ 24, 8, 4, 704, 2816, 320, 308.1, 3.4448, 3.0578, 19600], # 3.1h\n",
    "[ 24, 8, 4, 704, 2816, 272, 288.6, 3.3675, 3.0554, 19600], # 2.9h\n",
    "[ 24, 8, 4, 704, 2816, 256, 282.1, 3.3660, 3.0572, 19600], # 2.7h\n",
    "[ 24, 8, 4, 704, 2816, 224, 269.1, 3.3478, 3.0596, 19600], # 2.6h 16.4G\n",
    "[ 24, 8, 4, 704, 2816, 192, 256.2, 3.3814, 3.0598, 19600], # 1.9h 15.2G\n",
    "[ 24, 8, 4, 704, 2816, 176, 249.7, 3.3853, 3.0561, 19600], # 2.3h\n",
    "[ 24, 8, 4, 704, 2816, 160, 243.2, 3.3646,+3.0527, 19600], # 1.8h\n",
    "[+24, 8, 4, 704, 2816, 144, 236.7, 3.3452,+3.0515, 19600], # 1.7h\n",
    "[ 24, 8, 4, 704, 2816, 128, 230.2, 3.3975, 3.0556, 19600], # 1.3h 14.7G\n",
    "\n",
    "[ 24, 8, 4, 672, 2688, 320, 287.9, 3.3538, 3.0566, 19600], # 3.1h\n",
    "[ 24, 8, 4, 672, 2688, 272, 269.3, 3.3433, 3.0598, 19600], # 2.9h\n",
    "[ 24, 8, 4, 672, 2688, 256, 263.1, 3.3727, 3.0554, 19600], # 2.7h 15.8G\n",
    "[ 24, 8, 4, 672, 2688, 224, 250.7, 3.3521, 3.0551, 19600], # 2.5h\n",
    "[ 24, 8, 4, 672, 2688, 208, 244.5, 3.3670, 3.0586, 19600], # 2.4h\n",
    "[ 24, 8, 4, 672, 2688, 192, 238.3, 3.3503,+3.0520, 19600], # 1.9h\n",
    "[ 24, 8, 4, 672, 2688, 176, 232.1, 3.3628, 3.0581, 19600], # 2.3h\n",
    "[ 24, 8, 4, 672, 2688, 160, 225.9, 3.3548, 3.0585, 19600], # 1.8h\n",
    "[ 24, 8, 4, 672, 2688, 144, 219.7, 3.3664, 3.0584, 19600], # 1.7h\n",
    "[ 24, 8, 4, 672, 2688, 128, 213.6, 3.3528, 3.0557, 19600], # 1.3h\n",
    "\n",
    "[ 24, 8, 4, 640, 2560, 320, 268.3, 3.3287, 3.0555, 19600], # 2.9h\n",
    "[ 24, 8, 4, 640, 2560, 304, 262.4, 3.3549, 3.0547, 19600], # 2.8h\n",
    "[ 24, 8, 4, 640, 2560, 288, 256.5, 3.3230,+3.0523, 19600], # 2.8h\n",
    "[ 24, 8, 4, 640, 2560, 272, 250.6, 3.3622,+3.0534, 19600], # 2.7h\n",
    "[ 24, 8, 4, 640, 2560, 256, 244.7, 3.3319,+3.0516, 19600], # 2.5h\n",
    "[ 24, 8, 4, 640, 2560, 240, 238.8, 3.3859, 3.0585, 19600], # 2.4h\n",
    "[ 24, 8, 4, 640, 2560, 224, 232.9, 3.3401, 3.0583, 19600], # 2.4h\n",
    "[ 24, 8, 4, 640, 2560, 192, 221.1, 3.3441, 3.0573, 19600], # 1.8h\n",
    "[ 24, 8, 4, 640, 2560, 160, 209.3, 3.3388, 3.0612, 19600], # 1.7h\n",
    "[ 24, 8, 4, 640, 2560, 144, 203.4, 3.3549, 3.0545, 19600], # 1.6h\n",
    "[ 24, 8, 4, 640, 2560, 128, 197.5, 3.3770, 3.0578, 19600], # 1.2h 13.8G\n",
    "\n",
    "[ 24, 8, 4, 608, 2432, 320, 249.2, 3.3141, 3.0548, 19600], # 2.8h\n",
    "[ 24, 8, 4, 608, 2432, 272, 232.4, 3.3081, 3.0609, 19600], # 2.6h\n",
    "[ 24, 8, 4, 608, 2432, 256, 226.8, 3.3168, 3.0577, 19600], # 2.4h\n",
    "[ 24, 8, 4, 608, 2432, 224, 215.6, 3.3135, 3.0598, 19600], # 1.9h\n",
    "[ 24, 8, 4, 608, 2432, 192, 204.4, 3.2980, 3.0605, 19600], # 1.8h\n",
    "[ 24, 8, 4, 608, 2432, 160, 193.2, 3.3319, 3.0623, 19600], # 1.6h\n",
    "[ 24, 8, 4, 608, 2432, 144, 187.6, 3.3112, 3.0579, 19600], # 1.6h\n",
    "[ 24, 8, 4, 608, 2432, 128, 182.0, 3.3391, 3.0607, 19600], # 1.2h\n",
    "\n",
    "[ 24, 8, 4, 576, 2304, 256, 209.6, 3.2889, 3.0586, 19600], # 2.0h\n",
    "[ 24, 8, 4, 576, 2304, 224, 199.0, 3.3299, 3.0592, 19600], # 1.9h\n",
    "[ 24, 8, 4, 576, 2304, 192, 188.4, 3.3344, 3.0551, 19600], # 1.7h\n",
    "[ 24, 8, 4, 576, 2304, 160, 177.7, 3.3130, 3.0621, 19600], # 1.6h\n",
    "[ 24, 8, 4, 576, 2304, 144, 172.4, 3.3262, 3.0668, 19600], # 1.5h\n",
    "[ 24, 8, 4, 576, 2304, 128, 167.1, 3.2887, 3.0672, 19600], # 1.1h 13.2G\n",
    "\n",
    "\n",
    "[ 24, 6, 6, 648, 2592, 256, 249.2, 3.3943, 3.0618, 19600], # 2.0h 15.6G\n",
    "[ 24, 6, 6, 648, 2592, 240, 243.3, 3.3821, 3.0564, 19600], # 1.9h\n",
    "[ 24, 6, 6, 648, 2592, 224, 237.3, 3.3719, 3.0586, 19600], # 1.8h\n",
    "\n",
    "[ 24, 6, 6, 624, 2496, 256, 235.7, 3.3694, 3.0589, 19600], # 1.8h 15.4G\n",
    "[ 24, 6, 6, 624, 2496, 240, 229.9, 3.3713,+3.0537, 19600], # 1.8h\n",
    "[ 24, 6, 6, 624, 2496, 224, 224.2, 3.3490, 3.0601, 19600], # 1.8h\n",
    "\n",
    "[ 24, 6, 6, 600, 2400, 256, 222.5, 3.3210, 3.0604, 19600], # 8.9h\n",
    "[ 24, 6, 6, 600, 2400, 240, 216.9, 3.3594, 3.0566, 19600], # 1.7h\n",
    "[ 24, 6, 6, 600, 2400, 224, 211.4, 3.2984, 3.0590, 19600], # 1.7h\n",
    "\n",
    "[ 24, 6, 6, 576, 2304, 256, 209.6, 3.2962, 3.0636, 19600], # 1.8h\n",
    "[ 24, 6, 6, 576, 2304, 240, 204.3, 3.3545, 3.0605, 19600], # 1.7h\n",
    "[ 24, 6, 6, 576, 2304, 224, 199.0, 3.3353, 3.0612, 19600], # 1.7h\n",
    "\n",
    "\n",
    "[ 24, 6, 3, 768, 3072, 160, 261.8, 3.4131, 3.0563, 19600], # 1.7h 15.0G\n",
    "[ 24, 6, 3, 768, 3072, 144, 256.4, 3.4177, 3.0535, 19600], # 1.6h\n",
    "\n",
    "[ 24, 6, 3, 744, 2976, 240, 274.1, 3.3991, 3.0550, 19600], # 16.6h 15.6G+\n",
    "[ 24, 6, 3, 744, 2976, 224, 269.0, 3.4468, 3.0542, 19600], # 1.9h 15.5G\n",
    "[ 24, 6, 3, 744, 2976, 192, 258.7, 3.4382, 3.0554, 19600], # 1.8h 15.2G\n",
    "\n",
    "[ 24, 6, 3, 720, 2880, 256, 265.3, 3.4394, 3.0589, 18800], # 1.9h 15.4G\n",
    "[ 24, 6, 3, 720, 2880, 240, 260.3, 3.4129, 3.0564, 19600], # 1.9h 15.1G\n",
    "[ 24, 6, 3, 720, 2880, 224, 255.3, 3.3935,+3.0487, 19600], # 21.2h 14.9G\n",
    "[ 24, 6, 3, 720, 2880, 224, 255.3, 3.3921, 3.0549, 19600], # again\n",
    "[ 24, 6, 3, 720, 2880, 224, 255.3, 3.4070, 3.0544, 18800], # again with small N 1.7h 15.2G\n",
    "[ 24, 6, 3, 720, 2880, 224, 255.3, 3.3851, 3.0531, 20400], # again with large N 1.9h 15.2G\n",
    "[ 24, 6, 3, 720, 2880, 208, 250.4, 3.3837, 3.0547, 19600], # 2.2h\n",
    "[ 24, 6, 3, 720, 2880, 192, 245.4, 3.4315,+3.0498, 19600], # 1.7h\n",
    "[ 24, 6, 3, 720, 2880, 192, 245.4, 3.4086, 3.0538, 19600], # again\n",
    "[ 24, 6, 3, 720, 2880, 160, 235.4, 3.5644, 3.0579, 19600], # 1.6h\n",
    "[ 24, 6, 3, 720, 2880, 144, 230.5, 3.3816, 3.0597, 19600], # 1.6h\n",
    "\n",
    "[ 24, 6, 3, 696, 2784, 240, 246.8, 3.4036, 3.0558, 19600], # 1.8h\n",
    "[ 24, 6, 3, 696, 2784, 224, 242.0, 3.3857, 3.0542, 19600], # 1.9h 15.1G\n",
    "[ 24, 6, 3, 696, 2784, 192, 232.4, 3.4338, 3.0591, 19600], # 1.7h\n",
    "\n",
    "[ 24, 6, 3, 672, 2688, 272, 243.0, 3.3820, 3.0572, 19600], # 2.5h\n",
    "[ 24, 6, 3, 672, 2688, 256, 238.3, 3.3560, 3.0522, 19600], # 1.9h\n",
    "[ 24, 6, 3, 672, 2688, 240, 233.7, 3.3809, 3.0530, 19600], # 1.8h\n",
    "[ 24, 6, 3, 672, 2688, 224, 229.0, 3.3757,+3.0506, 19600], # 1.8h\n",
    "[ 24, 6, 3, 672, 2688, 224, 229.0, 3.3691,+3.0505, 19600], # again\n",
    "[ 24, 6, 3, 672, 2688, 224, 229.0, 3.4358, 3.0514, 18800], # again with small N 1.7h\n",
    "[ 24, 6, 3, 672, 2688, 224, 229.0, 3.3431, 3.0508, 20400], # again with large N 1.9h\n",
    "[ 24, 6, 3, 672, 2688, 208, 224.4, 3.4290, 3.0581, 19600], # 2.2h 14.9G\n",
    "[ 24, 6, 3, 672, 2688, 192, 219.7, 3.3768, 3.0546, 19600], # 1.6h\n",
    "[ 24, 6, 3, 672, 2688, 160, 210.5, 3.3760, 3.0562, 19600], # 1.5h 14.0G\n",
    "[ 24, 6, 3, 672, 2688, 144, 205.8, 3.3620, 3.0577, 19600], # 1.5h\n",
    "\n",
    "[ 24, 6, 3, 648, 2592, 320, 243.3, 3.3153, 3.0534, 19600], # 2.1h 15.1G\n",
    "[ 24, 6, 3, 648, 2592, 288, 234.3, 3.4198, 3.0532, 19600], # 2.0h\n",
    "[ 24, 6, 3, 648, 2592, 256, 225.3, 3.4490, 3.0566, 19600], # 1.8h 14.8G\n",
    "[+24, 6, 3, 648, 2592, 240, 220.9, 3.3493,+3.0484, 19600], # 1.8h  minGemma-hidden_layers24-att_heads6-kv_heads3-hidden648-intermediate2592-head_dim240-T512--2025-05-16-05-52.pth\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3494, 3.0524, 19600], # again\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3620, 3.0513, 19600], # again\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3990, 3.0550, 18800], # again with small N 1.7h\n",
    "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3216, 3.0536, 20400], # again with large N 1.8h\n",
    "[ 24, 6, 3, 648, 2592, 224, 216.4, 3.3620, 3.0523, 19600], # 1.7h 14.6G\n",
    "[ 24, 6, 3, 648, 2592, 208, 211.9, 3.3950, 3.0544, 19600], # 2.1h\n",
    "[ 24, 6, 3, 648, 2592, 192, 207.4, 3.3543, 3.0602, 19600], # 1.6h\n",
    "\n",
    "[ 24, 6, 3, 624, 2496, 288, 221.3, 3.3320, 3.0506, 19600], # 2.0h\n",
    "[ 24, 6, 3, 624, 2496, 256, 212.7, 3.3274, 3.0557, 19600], # 1.8h\n",
    "[ 24, 6, 3, 624, 2496, 240, 208.4, 3.4206, 3.0541, 19600], # 1.7h\n",
    "[ 24, 6, 3, 624, 2496, 224, 204.0, 3.3274, 3.0604, 19600], # 1.7h\n",
    "[ 24, 6, 3, 624, 2496, 160, 186.8, 3.3284, 3.0571, 19600], # 1.4h\n",
    "[ 24, 6, 3, 624, 2496, 144, 182.5, 3.3571, 3.0648, 19600], # 1.4h\n",
    "\n",
    "[ 24, 6, 3, 600, 2400, 320, 216.9, 3.3183, 3.0581, 19600], # 2.0h\n",
    "[ 24, 6, 3, 600, 2400, 288, 208.6, 3.3264, 3.0561, 19600], # 1.9h\n",
    "[ 24, 6, 3, 600, 2400, 240, 196.2, 3.3806, 3.0584, 19600], # 1.7h\n",
    "[ 24, 6, 3, 600, 2400, 192, 183.8, 3.3205, 3.0536, 19600], # 1.5h\n",
    "\n",
    "[ 24, 6, 3, 576, 2304, 320, 204.3, 3.3123, 3.0600, 19600], # 2.0h\n",
    "[ 24, 6, 3, 576, 2304, 288, 196.3, 3.3086, 3.0598, 19600], # 1.9h\n",
    "[ 24, 6, 3, 576, 2304, 256, 188.4, 3.3131, 3.0586, 19600], # 1.7h\n",
    "[ 24, 6, 3, 576, 2304, 240, 184.4, 3.3204, 3.0601, 19600], # 1.6h\n",
    "[ 24, 6, 3, 576, 2304, 224, 180.4, 3.2905, 3.0582, 19600], # 1.6h\n",
    "[ 24, 6, 3, 576, 2304, 192, 172.4, 3.3323, 3.0633, 19600], # 1.4h\n",
    "[ 24, 6, 3, 576, 2304, 160, 164.5, 3.3377, 3.0601, 19600], # 1.3h\n",
    "[ 24, 6, 3, 576, 2304, 144, 160.5, 3.3245, 3.0718, 19600], # 1.3h\n",
    "\n",
    "\n",
    "[ 24, 6, 2, 648, 2592, 272, 221.4, 3.4127, 3.0551, 19600], # 2.4h 15.0G\n",
    "[ 24, 6, 2, 648, 2592, 256, 217.4, 3.3309,+3.0532, 19600], # 1.8h\n",
    "[ 24, 6, 2, 648, 2592, 240, 213.4, 3.3537, 3.0613, 19600], # 1.7h\n",
    "[ 24, 6, 2, 648, 2592, 224, 209.4, 3.3775, 3.0590, 19600], # 1.7h\n",
    "\n",
    "[ 24, 6, 2, 624, 2496, 256, 205.0, 3.3065, 3.0603, 19600], # 1.7h\n",
    "[ 24, 6, 2, 624, 2496, 240, 201.2, 3.3541, 3.0590, 19600], # 1.7h\n",
    "[ 24, 6, 2, 624, 2496, 224, 197.3, 3.3338, 3.0575, 19600], # 1.6h\n",
    "\n",
    "[ 24, 6, 2, 600, 2400, 256, 193.0, 3.3026, 3.0600, 19600], # 1.7h\n",
    "[ 24, 6, 2, 600, 2400, 240, 189.3, 3.3347, 3.0620, 19600], # 1.6h\n",
    "[ 24, 6, 2, 600, 2400, 224, 185.6, 3.3635, 3.0592, 19600], # 1.6h\n",
    "\n",
    "[ 24, 6, 2, 576, 2304, 256, 181.3, 3.3103, 3.0602, 19600], # 1.6h 13.8G\n",
    "[ 24, 6, 2, 576, 2304, 240, 177.7, 3.3317, 3.0589, 19600], # 1.6h\n",
    "[ 24, 6, 2, 576, 2304, 224, 174.2, 3.3051, 3.0625, 19600], # 1.5h\n",
    "\n",
    "\n",
    "[ 24, 5, 1, 800, 3200, 272, 287.4, 3.4445, 3.0557, 19600], # 2.5h\n",
    "[ 24, 5, 1, 800, 3200, 256, 283.7, 3.4492, 3.0598, 19600], # 2.4h\n",
    "[ 24, 5, 1, 800, 3200, 240, 280.0, 3.4316, 3.0600, 19600], # 2.4h\n",
    "[ 24, 5, 1, 800, 3200, 224, 276.3, 3.4309, 3.0552, 19600], # 2.3h 15.9G\n",
    "[ 24, 5, 1, 800, 3200, 208, 272.7, 3.4655, 3.0603, 19600], # 2.3h\n",
    "[ 24, 5, 1, 800, 3200, 192, 269.0, 3.4391, 3.0533, 19600], # 2.2h\n",
    "[ 24, 5, 1, 800, 3200, 176, 265.3, 3.4471, 3.0564, 19600], # 2.2h\n",
    "\n",
    "[ 24, 5, 1, 780, 3120, 224, 264.9, 3.4490, 3.0542, 19600], # 2.3h\n",
    "\n",
    "[ 24, 5, 1, 760, 3040, 272, 264.3, 3.4284, 3.0501, 19600], # 2.3h\n",
    "[ 24, 5, 1, 760, 3040, 256, 260.8, 3.4413, 3.0561, 19600], # 2.2h\n",
    "[ 24, 5, 1, 760, 3040, 240, 257.3, 3.4007, 3.0591, 19600], # 2.2h\n",
    "[+24, 5, 1, 760, 3040, 224, 253.8, 3.4110,+3.0484, 19600], # 2.1h  BEST tie (not saved)\n",
    "[ 24, 5, 1, 760, 3040, 224, 253.8, 3.4209, 3.0539, 19600], # again\n",
    "[ 24, 5, 1, 760, 3040, 208, 250.3, 3.4485, 3.0528, 19600], # 2.1h 15.3G\n",
    "[ 24, 5, 1, 760, 3040, 192, 246.8, 3.3914, 3.0537, 19600], # 2.0h\n",
    "[ 24, 5, 1, 760, 3040, 176, 243.3, 3.3813, 3.0590, 19600], # 2.0h 15.2G\n",
    "\n",
    "[ 24, 5, 1, 740, 2960, 224, 242.8, 3.4712, 3.0608, 19600], # 2.1h\n",
    "\n",
    "[ 24, 5, 1, 720, 2880, 272, 242.1, 3.3799, 3.0553, 19600], # 2.3h\n",
    "[ 24, 5, 1, 720, 2880, 256, 238.8, 3.3887, 3.0581, 19600], # 2.1h\n",
    "[ 24, 5, 1, 720, 2880, 240, 235.4, 3.4724, 3.0576, 19600], # 2.1h\n",
    "[ 24, 5, 1, 720, 2880, 224, 232.1, 3.4695, 3.0613, 19600], # 2.0h\n",
    "[ 24, 5, 1, 720, 2880, 208, 228.8, 3.3670, 3.0540, 19600], # 2.0h\n",
    "[ 24, 5, 1, 720, 2880, 192, 225.5, 3.3767, 3.0639, 19600], # 2.0h\n",
    "[ 24, 5, 1, 720, 2880, 176, 222.2, 3.3915, 3.0568, 19600], # 1.9h\n",
    "\n",
    "[ 24, 5, 1, 700, 2800, 208, 218.4, 3.3519, 3.0561, 19600], # 2.0h\n",
    "\n",
    "[ 24, 5, 1, 680, 2720, 272, 220.8, 3.4186, 3.0566, 19600], # 2.2h\n",
    "[ 24, 5, 1, 680, 2720, 256, 217.7, 3.3798, 3.0578, 19600], # 2.1h\n",
    "[ 24, 5, 1, 680, 2720, 240, 214.5, 3.3766, 3.0525, 19600], # 2.1h\n",
    "[ 24, 5, 1, 680, 2720, 224, 211.4, 3.3376, 3.0609, 19600], # 2.0h\n",
    "[ 24, 5, 1, 680, 2720, 208, 208.3, 3.4418,+3.0498, 19600], # 1.9h\n",
    "[ 24, 5, 1, 680, 2720, 208, 208.3, 3.4472, 3.0577, 19600], # again\n",
    "[ 24, 5, 1, 680, 2720, 192, 205.1, 3.3546, 3.0541, 19600], # 1.9h\n",
    "[ 24, 5, 1, 680, 2720, 176, 202.0, 3.3742, 3.0619, 19600], # 1.8h\n",
    "\n",
    "[ 24, 5, 1, 660, 2640, 208, 198.3, 3.3579, 3.0546, 19600], # 1.9h 14.3G\n",
    "\n",
    "[ 24, 5, 1, 640, 2560, 272, 200.4, 3.3795, 3.0560, 19600], # 2.0h\n",
    "[ 24, 5, 1, 640, 2560, 256, 197.5, 3.4147, 3.0564, 19600], # 1.9h\n",
    "[ 24, 5, 1, 640, 2560, 240, 194.5, 3.3611, 3.0565, 19600], # 1.8h\n",
    "[ 24, 5, 1, 640, 2560, 224, 191.6, 3.3936, 3.0618, 19600], # 1.8h\n",
    "[ 24, 5, 1, 640, 2560, 208, 188.6, 3.3472, 3.0589, 19600], # 1.8h\n",
    "[ 24, 5, 1, 640, 2560, 192, 185.7, 3.3431, 3.0585, 19600], # 1.7h\n",
    "[ 24, 5, 1, 640, 2560, 176, 182.7, 3.3996, 3.0530, 19600], # 1.7h\n",
    "[ 24, 5, 1, 640, 2560, 160, 179.8, 3.3565, 3.0656, 19600], # 1.6h 13.9G\n",
    "\n",
    "\n",
    "[ 24, 4, 4, 800, 3200, 240, 298.5, 3.5770, 3.0630, 19600], # 2.4h 16.8G\n",
    "[ 24, 4, 4, 800, 3200, 208, 288.6, 3.4899, 3.0590, 19600], # 2.3h\n",
    "[ 24, 4, 4, 800, 3200, 176, 278.8, 3.5062, 3.0595, 19600], # 2.2h\n",
    "[ 24, 4, 4, 800, 3200, 128, 264.1, 3.5951, 3.0633, 19600], # 1.4h 15.2G\n",
    "\n",
    "[ 24, 4, 4, 768, 3072, 240, 279.4, 3.5161, 3.0589, 19600], # 2.2h\n",
    "[ 24, 4, 4, 768, 3072, 208, 270.0, 3.4918, 3.0645, 19600], # 2.1h\n",
    "[ 24, 4, 4, 768, 3072, 176, 260.6, 3.4323, 3.0573, 19600], # 2.0h\n",
    "[ 24, 4, 4, 768, 3072, 128, 246.4, 3.4482, 3.0588, 19600], # 1.3h\n",
    "\n",
    "[ 24, 4, 4, 736, 2944, 240, 261.0, 3.4809, 3.0564, 19600], # 2.1h\n",
    "[ 24, 4, 4, 736, 2944, 208, 252.0, 3.4556, 3.0551, 19600], # 2.0h\n",
    "[ 24, 4, 4, 736, 2944, 176, 242.9, 3.5019, 3.0610, 19600], # 1.9h 15.4G\n",
    "[ 24, 4, 4, 736, 2944, 144, 233.9, 3.4087, 3.0589, 19600], # 1.4h\n",
    "[ 24, 4, 4, 736, 2944, 128, 229.4, 3.4385, 3.0554, 19600], # 1.2h\n",
    "\n",
    "[ 24, 4, 4, 704, 2816, 256, 247.5, 3.4356, 3.0561, 19600], # 1.7h\n",
    "[ 24, 4, 4, 704, 2816, 224, 238.9, 3.5633, 3.0658, 19600], # 1.6h\n",
    "[ 24, 4, 4, 704, 2816, 192, 230.2, 3.4339, 3.0559, 19600], # 1.5h\n",
    "[ 24, 4, 4, 704, 2816, 160, 221.6, 3.6056, 3.0875, 19600], # 1.4h\n",
    "[ 24, 4, 4, 704, 2816, 144, 217.2, 3.5125, 3.0617, 19600], # 1.4h\n",
    "[ 24, 4, 4, 704, 2816, 128, 212.9, 3.3923, 3.0603, 19600], # 1.2h\n",
    "\n",
    "[ 24, 4, 4, 672, 2688, 256, 230.1, 3.3990, 3.0584, 19600], # 1.6h\n",
    "[ 24, 4, 4, 672, 2688, 224, 221.8, 3.3888, 3.0567, 19600], # 1.5h\n",
    "[ 24, 4, 4, 672, 2688, 192, 213.6, 3.3940,+3.0542, 19600], # 1.4h\n",
    "[ 24, 4, 4, 672, 2688, 160, 205.3, 3.4370, 3.0586, 19600], # 1.4h\n",
    "[ 24, 4, 4, 672, 2688, 144, 201.2, 3.3909, 3.0631, 19600], # 1.3h\n",
    "[ 24, 4, 4, 672, 2688, 128, 197.0, 3.3920, 3.0608, 19600], # 1.1h\n",
    "\n",
    "[ 24, 4, 4, 640, 2560, 288, 221.1, 3.4232, 3.0622, 19600], # 1.6h\n",
    "[ 24, 4, 4, 640, 2560, 256, 213.2, 3.4184, 3.0608, 19600], # 1.5h\n",
    "[ 24, 4, 4, 640, 2560, 224, 205.3, 3.3699,+3.0553, 19600], # 1.4h\n",
    "[ 24, 4, 4, 640, 2560, 192, 197.5, 3.3685, 3.0580, 19600], # 1.3h\n",
    "[ 24, 4, 4, 640, 2560, 160, 189.6, 3.3455, 3.0611, 19600], # 1.3h\n",
    "[ 24, 4, 4, 640, 2560, 144, 185.7, 3.3695, 3.0629, 19600], # 1.3h\n",
    "[ 24, 4, 4, 640, 2560, 128, 181.8, 3.4085, 3.0708, 19600], # 1.0h\n",
    "\n",
    "[ 24, 4, 4, 608, 2432, 320, 211.9, 3.3533, 3.0583, 19600], # 1.6h\n",
    "[ 24, 4, 4, 608, 2432, 288, 204.4, 3.4273, 3.0607, 19600], # 1.6h\n",
    "[ 24, 4, 4, 608, 2432, 256, 196.9, 3.3529, 3.0579, 19600], # 1.4h\n",
    "[ 24, 4, 4, 608, 2432, 224, 189.5, 3.3583, 3.0600, 19600], # 1.4h\n",
    "[ 24, 4, 4, 608, 2432, 192, 182.0, 3.3653, 3.0619, 19600], # 1.3h\n",
    "[ 24, 4, 4, 608, 2432, 160, 174.5, 3.3424, 3.0644, 19600], # 1.2h\n",
    "[ 24, 4, 4, 608, 2432, 144, 170.8, 3.3567, 3.0639, 19600], # 1.2h\n",
    "[ 24, 4, 4, 608, 2432, 128, 167.1, 3.4717, 3.0673, 19600], # 1.0h\n",
    "\n",
    "[ 24, 4, 4, 576, 2304, 320, 195.4, 3.3312, 3.0591, 19600], # 1.6h 14.2G\n",
    "[ 24, 4, 4, 576, 2304, 288, 188.4, 3.3492, 3.0605, 19600], # 1.5h\n",
    "[ 24, 4, 4, 576, 2304, 256, 181.3, 3.3444, 3.0601, 19600], # 1.4h\n",
    "[ 24, 4, 4, 576, 2304, 224, 174.2, 3.3461, 3.0642, 19600], # 1.3h\n",
    "[ 24, 4, 4, 576, 2304, 192, 167.1, 3.3360, 3.0590, 19600], # 1.3h\n",
    "[ 24, 4, 4, 576, 2304, 160, 160.0, 3.3262, 3.0680, 19600], # 1.2h\n",
    "[ 24, 4, 4, 576, 2304, 144, 156.5, 3.3371, 3.0691, 19600], # 1.2h\n",
    "[ 24, 4, 4, 576, 2304, 128, 153.0, 3.3501, 3.0705, 19600], # 1.0h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c10ad8dc-e13b-4bfd-b5d2-3afcaa3d2fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L24 att6 kv_heads3 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:49:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.349300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24, 6, 3, 648, 2592, 240, 220.9, 3.3493, 3.0484, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  24 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*108 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0484:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54412b3f-514f-48ab-a2fe-e50fee24aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L24 att5 kv_heads1 hidden760 intermediate3040 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:09:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.411000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24, 5, 1, 760, 3040, 224, 253.8, 3.4110, 3.0484, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  24 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 5 # 4 # G16 G8\n",
    "num_key_value_heads = 1 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*152 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0484:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b30b3-d325-46fa-bf2b-77ce985e8406",
   "metadata": {},
   "source": [
    "# L22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c742be1-f3a5-4d7e-bd98-958008ae341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L22 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "\n",
    "[ 22, 8, 4, 832, 3328, 320, 365.3, 3.7763, 3.0932, 19600], # 3.3h\n",
    "[ 22, 8, 4, 832, 3328, 272, 344.3, 3.5172, 3.0683, 19600], # 3.1h\n",
    "[ 22, 8, 4, 832, 3328, 256, 337.2, 3.5426, 3.0666, 19600], # 2.9h\n",
    "[ 22, 8, 4, 832, 3328, 224, 323.2, 3.5843, 3.0605, 19600], # 2.7h\n",
    "[ 22, 8, 4, 832, 3328, 128, 281.0, 3.4893, 3.0626, 19600], # 1.6h\n",
    "\n",
    "[ 22, 8, 4, 816, 3264, 144, 279.0, 3.4644, 3.0569, 19600], # 12.4h\n",
    "\n",
    "[ 22, 8, 4, 800, 3200, 256, 317.5, 3.4839, 3.0604, 19600], # 2.8h\n",
    "[ 22, 8, 4, 800, 3200, 192, 290.5, 3.4523, 3.0572, 19600], # 23.0h\n",
    "[ 22, 8, 4, 800, 3200, 128, 263.4, 3.4650, 3.0591, 19600], # 1.5h\n",
    "\n",
    "[ 22, 8, 4, 784, 3136, 256, 307.8, 3.4136, 3.0600, 19600], # 2.8h 16.9G\n",
    "[ 22, 8, 4, 784, 3136, 144, 261.5, 3.4719, 3.0588, 19600], # 1.9h\n",
    "\n",
    "[ 22, 8, 4, 768, 3072, 320, 324.3, 3.6332, 3.0798, 19600], # 3.0h\n",
    "[ 22, 8, 4, 768, 3072, 272, 304.8, 3.4640, 3.0600, 19600], # 2.8h\n",
    "[ 22, 8, 4, 768, 3072, 256, 298.3, 3.4033,+3.0524, 19600], # 2.6h\n",
    "[ 22, 8, 4, 768, 3072, 240, 291.8, 3.3528, 3.0543, 19600], # 2.6h\n",
    "[ 22, 8, 4, 768, 3072, 224, 285.3, 3.4431, 3.0602, 19600], # 2.5h\n",
    "[ 22, 8, 4, 768, 3072, 192, 272.4, 3.4474, 3.0558, 19600], # 1.9h\n",
    "[ 22, 8, 4, 768, 3072, 128, 246.4, 3.4254, 3.0518, 19600], # 1.3h\n",
    "\n",
    "[ 22, 8, 4, 736, 2944, 320, 304.5, 3.4382, 3.0626, 19600], # 2.9h\n",
    "[ 22, 8, 4, 736, 2944, 272, 285.9, 3.5311, 3.0553, 19600], # 2.7h\n",
    "[ 22, 8, 4, 736, 2944, 256, 279.7, 3.3683, 3.0582, 19600], # 2.6h\n",
    "[ 22, 8, 4, 736, 2944, 224, 267.2, 3.3977, 3.0586, 19600], # 2.4h\n",
    "[ 22, 8, 4, 736, 2944, 224, 267.2, 3.4471, 3.0562, 19600], # 2.0h\n",
    "[ 22, 8, 4, 736, 2944, 192, 254.8, 3.3951, 3.0568, 19600], # 1.9h\n",
    "[ 22, 8, 4, 736, 2944, 144, 236.1, 3.4118, 3.0568, 19600], # 1.7h\n",
    "[ 22, 8, 4, 736, 2944, 128, 229.9, 3.4359, 3.0556, 19600], # 1.3h\n",
    "\n",
    "[ 22, 8, 4, 704, 2816, 320, 285.3, 3.3915, 3.0569, 19600], # 2.9h\n",
    "[ 22, 8, 4, 704, 2816, 272, 267.5, 3.4223, 3.0549, 19600], # 2.7h\n",
    "[ 22, 8, 4, 704, 2816, 256, 261.6, 3.3880, 3.0563, 19600], # 2.5h\n",
    "[ 22, 8, 4, 704, 2816, 224, 249.7, 3.3706, 3.0561, 19600], # 9.5h\n",
    "[ 22, 8, 4, 704, 2816, 208, 243.7, 3.3505, 3.0526, 19600], # 2.3h\n",
    "[+22, 8, 4, 704, 2816, 192, 237.8, 3.3583,+3.0505, 19600], # 1.8h  minGemma-hidden_layers22-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim192-T512--2025-05-10-14-35.pth\n",
    "[ 22, 8, 4, 704, 2816, 176, 231.8, 3.3767, 3.0536, 19600], # 2.2h\n",
    "[ 22, 8, 4, 704, 2816, 160, 225.9, 3.3726, 3.0533, 19600], # 2.1h\n",
    "[ 22, 8, 4, 704, 2816, 144, 219.9, 3.3917, 3.0540, 19600], # 1.6h\n",
    "[ 22, 8, 4, 704, 2816, 128, 214.0, 3.3756, 3.0600, 19600], # 1.2h\n",
    "\n",
    "[ 22, 8, 4, 672, 2688, 272, 249.7, 3.3773, 3.0583, 19600], # 2.7h\n",
    "[ 22, 8, 4, 672, 2688, 256, 244.0, 3.3553, 3.0536, 19600], # 2.0h\n",
    "[ 22, 8, 4, 672, 2688, 224, 232.6, 3.3506, 3.0604, 19600], # 1.9h\n",
    "[ 22, 8, 4, 672, 2688, 192, 221.3, 3.3776, 3.0558, 19600], # 1.8h\n",
    "[ 22, 8, 4, 672, 2688, 144, 204.2, 3.3895, 3.0560, 19600], # 1.6h\n",
    "[ 22, 8, 4, 672, 2688, 128, 198.6, 3.3516, 3.0587, 19600], # 1.2h\n",
    "\n",
    "[ 22, 8, 4, 640, 2560, 320, 248.6, 3.3510, 3.0576, 19600], # 2.4h\n",
    "[ 22, 8, 4, 640, 2560, 272, 232.4, 3.3695, 3.0589, 19600], # 2.1h\n",
    "[ 22, 8, 4, 640, 2560, 256, 227.0, 3.3440, 3.0534, 19600], # 2.0h\n",
    "[ 22, 8, 4, 640, 2560, 240, 221.6, 3.3259, 3.0569, 19600], # 2.3h\n",
    "[ 22, 8, 4, 640, 2560, 224, 216.1, 3.3703, 3.0615, 19600], # 1.8h\n",
    "[ 22, 8, 4, 640, 2560, 192, 205.3, 3.3356, 3.0556, 19600], # 1.7h\n",
    "[ 22, 8, 4, 640, 2560, 144, 189.1, 3.3409, 3.0633, 19600], # 1.5h\n",
    "[ 22, 8, 4, 640, 2560, 128, 183.7, 3.3553, 3.0563, 19600], # 1.1h\n",
    "\n",
    "[ 22, 8, 4, 608, 2432, 320, 231.0, 3.3107, 3.0561, 19600], # 2.3h\n",
    "[ 22, 8, 4, 608, 2432, 272, 215.6, 3.3171, 3.0612, 19600], # 2.1h\n",
    "[ 22, 8, 4, 608, 2432, 256, 210.5, 3.3692, 3.0535, 19600], # 1.9h\n",
    "[ 22, 8, 4, 608, 2432, 240, 205.3, 3.3087, 3.0603, 19600], # 2.2h\n",
    "[ 22, 8, 4, 608, 2432, 224, 200.2, 3.3054, 3.0631, 19600], # 1.8h\n",
    "[ 22, 8, 4, 608, 2432, 192, 189.9, 3.3495, 3.0565, 19600], # 1.7h\n",
    "[ 22, 8, 4, 608, 2432, 144, 174.5, 3.3318, 3.0639, 19600], # 1.4h\n",
    "[ 22, 8, 4, 608, 2432, 128, 169.4, 3.3043, 3.0651, 19600], # 1.1h\n",
    "\n",
    "[ 22, 8, 4, 576, 2304, 320, 214.0, 3.3096, 3.0619, 19600], # 2.3h\n",
    "[ 22, 8, 4, 576, 2304, 272, 199.4, 3.3268, 3.0592, 19600], # 2.1h\n",
    "[ 22, 8, 4, 576, 2304, 256, 194.5, 3.3012, 3.0631, 19600], # 1.8h\n",
    "[ 22, 8, 4, 576, 2304, 224, 184.8, 3.2853, 3.0644, 19600], # 1.7h\n",
    "[ 22, 8, 4, 576, 2304, 192, 175.1, 3.3510, 3.0585, 19600], # 1.6h\n",
    "[ 22, 8, 4, 576, 2304, 144, 160.5, 3.3194, 3.0616, 19600], # 1.4h\n",
    "[ 22, 8, 4, 576, 2304, 128, 155.6, 3.3252, 3.0640, 19600], # 1.0h\n",
    "\n",
    "[ 22, 8, 4, 544, 2176, 320, 197.5, 3.2819, 3.0652, 19600], # 2.2h\n",
    "[ 22, 8, 4, 544, 2176, 272, 183.7, 3.2774, 3.0629, 19600], # 2.0h\n",
    "[ 22, 8, 4, 544, 2176, 256, 179.1, 3.2799, 3.0672, 19600], # 1.8h\n",
    "[ 22, 8, 4, 544, 2176, 224, 169.9, 3.3311, 3.0632, 19600], # 1.7h\n",
    "[ 22, 8, 4, 544, 2176, 192, 160.7, 3.2941, 3.0675, 19600], # 1.6h\n",
    "[ 22, 8, 4, 544, 2176, 144, 147.0, 3.3270, 3.0724, 19600], # 1.4h\n",
    "[ 22, 8, 4, 544, 2176, 128, 142.4, 3.3151, 3.0758, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 22, 7, 7, 756, 3024, 256, 308.3, 3.4348, 3.0607, 19600], # 2.7h 15.3G\n",
    "[ 22, 7, 7, 756, 3024, 240, 300.8, 3.4119, 3.0559, 19600], # 25.5h\n",
    "[ 22, 7, 7, 756, 3024, 224, 293.4, 3.4749, 3.0560, 19600], # 24.5h\n",
    "[ 22, 7, 7, 756, 3024, 208, 285.9, 3.4062, 3.0582, 19600], # 18.0h\n",
    "[ 22, 7, 7, 756, 3024, 192, 278.5, 3.4419, 3.0652, 19600], # 14.6h\n",
    "[ 22, 7, 7, 756, 3024, 176, 271.0, 3.4142, 3.0552, 19600], # 1.8h\n",
    "\n",
    "[ 22, 7, 7, 700, 2800, 256, 275.1, 3.3542, 3.0551, 19600], # 2.6h 16.3G\n",
    "[ 22, 7, 7, 700, 2800, 240, 268.2, 3.3862, 3.0586, 19600], # 2.5h 16.1G\n",
    "[ 22, 7, 7, 700, 2800, 224, 261.3, 3.3476, 3.0606, 19600], # 2.4h\n",
    "[ 22, 7, 7, 700, 2800, 208, 254.4, 3.3919, 3.0576, 19600], # 1.9h 15.3G\n",
    "[ 22, 7, 7, 700, 2800, 192, 247.5, 3.4134,+3.0542, 19600], # 1.8h\n",
    "[ 22, 7, 7, 700, 2800, 176, 240.6, 3.3938, 3.0600, 19600], # 1.7h\n",
    "\n",
    "[ 22, 7, 7, 644, 2576, 272, 249.9, 3.3094, 3.0552, 19600], # 2.7h\n",
    "[ 22, 7, 7, 644, 2576, 256, 243.6, 3.3932, 3.0578, 19600], # 2.5h\n",
    "[ 22, 7, 7, 644, 2576, 240, 237.2, 3.3359, 3.0575, 19600], # 2.0h 15.1G\n",
    "[ 22, 7, 7, 644, 2576, 224, 230.9, 3.3546, 3.0603, 19600], # 1.9h\n",
    "[ 22, 7, 7, 644, 2576, 208, 224.5, 3.3484, 3.0556, 19600], # 1.8h\n",
    "[ 22, 7, 7, 644, 2576, 192, 218.2, 3.3782, 3.0576, 19600], # 1.7h\n",
    "[ 22, 7, 7, 644, 2576, 176, 211.8, 3.3753, 3.0588, 19600], # 1.6h\n",
    "\n",
    "[ 22, 7, 1, 756, 3024, 256, 257.2, 3.3832, 3.0650, 19600], # 2.3h\n",
    "[ 22, 7, 1, 756, 3024, 240, 252.9, 3.3358, 3.0621, 19600], # 2.3h\n",
    "[ 22, 7, 1, 756, 3024, 224, 248.7, 3.4824, 3.0576, 19600], # 1.8h 14.9G\n",
    "[ 22, 7, 1, 756, 3024, 208, 244.4, 3.3610, 3.0615, 19600], # 1.8h\n",
    "[ 22, 7, 1, 756, 3024, 192, 240.2, 3.3783, 3.0627, 19600], # 1.7h\n",
    "[ 22, 7, 1, 756, 3024, 176, 235.9, 3.3933, 3.0601, 19600], # 1.6h\n",
    "\n",
    "[ 22, 7, 1, 700, 2800, 256, 227.8, 3.3275, 3.0604, 19600], # 2.2h\n",
    "[ 22, 7, 1, 700, 2800, 240, 223.8, 3.3213, 3.0568, 19600], # 2.2h\n",
    "[ 22, 7, 1, 700, 2800, 224, 219.9, 3.3391, 3.0602, 19600], # 1.7h\n",
    "[ 22, 7, 1, 700, 2800, 208, 216.0, 3.3320, 3.0608, 19600], # 1.7h 14.7G\n",
    "[ 22, 7, 1, 700, 2800, 192, 212.0, 3.3350, 3.0619, 19600], # 1.6h 14.3G\n",
    "[ 22, 7, 1, 700, 2800, 176, 208.1, 3.3431, 3.0559, 19600], # 1.6h\n",
    "\n",
    "[ 22, 7, 1, 644, 2576, 272, 203.7, 3.2885, 3.0667, 19600], # 2.3h 14.6G\n",
    "[ 22, 7, 1, 644, 2576, 256, 200.0, 3.3256, 3.0614, 19600], # 2.2h 14.5G\n",
    "[ 22, 7, 1, 644, 2576, 240, 196.4, 3.2930, 3.0578, 19600], # 1.7h\n",
    "[ 22, 7, 1, 644, 2576, 224, 192.8, 3.3889, 3.0664, 19600], # 1.6h\n",
    "[ 22, 7, 1, 644, 2576, 208, 189.2, 3.3332, 3.0624, 19600], # 1.6h\n",
    "[ 22, 7, 1, 644, 2576, 192, 185.5, 3.3236, 3.0607, 19600], # 1.5h\n",
    "[ 22, 7, 1, 644, 2576, 176, 181.9, 3.3311, 3.0625, 19600], # 1.5h\n",
    "\n",
    "\n",
    "[ 22, 6, 3, 792, 3168, 272, 290.9, 3.4683, 3.0607, 19600], # 2.7h\n",
    "[ 22, 6, 3, 792, 3168, 256, 285.9, 3.4731, 3.0569, 19600], # 2.5h\n",
    "[ 22, 6, 3, 792, 3168, 240, 280.9, 3.4357, 3.0616, 19600], # 2.4h\n",
    "[ 22, 6, 3, 792, 3168, 224, 275.8, 3.4391, 3.0583, 19600], # 2.4h 15.5G\n",
    "[ 22, 6, 3, 792, 3168, 208, 270.8, 3.4426, 3.0537, 19600], # 1.9h 15.2G\n",
    "[ 22, 6, 3, 792, 3168, 192, 265.8, 3.5367, 3.0568, 19600], # 1.8h 15.0G\n",
    "[ 22, 6, 3, 792, 3168, 176, 260.8, 3.4386, 3.0542, 19600], # 1.7h\n",
    "\n",
    "[ 22, 6, 3, 756, 3024, 272, 270.5, 3.3738, 3.0549, 19600], # 2.5h\n",
    "[ 22, 6, 3, 756, 3024, 256, 265.7, 3.4879, 3.0601, 19600], # 1.9h 15.0G\n",
    "[ 22, 6, 3, 756, 3024, 240, 260.9, 3.4863, 3.0567, 19600], # 1.8h\n",
    "[ 22, 6, 3, 756, 3024, 224, 256.1, 3.5312, 3.0642, 19600], # 1.7h\n",
    "[ 22, 6, 3, 756, 3024, 208, 251.3, 3.4494, 3.0543, 19600], # 1.7h\n",
    "[ 22, 6, 3, 756, 3024, 192, 246.5, 3.4164, 3.0552, 19600], # 1.6h 14.7G\n",
    "[ 22, 6, 3, 756, 3024, 176, 241.8, 3.5533, 3.0573, 19600], # 1.6h\n",
    "\n",
    "[ 22, 6, 3, 738, 2952, 272, 260.5, 3.3960, 3.0564, 19600], # 2.4h\n",
    "[ 22, 6, 3, 738, 2952, 256, 255.9, 3.3789, 3.0550, 19600], # 1.8h\n",
    "[ 22, 6, 3, 738, 2952, 224, 246.5, 3.3962, 3.0544, 19600], # 1.7h\n",
    "[ 22, 6, 3, 738, 2952, 192, 237.2, 3.4250, 3.0542, 19600], # 1.6h\n",
    "[ 22, 6, 3, 738, 2952, 176, 232.5, 3.4145, 3.0544, 19600], # 1.6h\n",
    "\n",
    "[ 22, 6, 3, 720, 2880, 288, 255.3, 3.3942, 3.0571, 19600], # 2.4h\n",
    "[ 22, 6, 3, 720, 2880, 272, 250.8, 3.4051,+3.0528, 19600], # 2.0h\n",
    "[ 22, 6, 3, 720, 2880, 256, 246.2, 3.4092,+3.0530, 19600], # 1.8h\n",
    "[ 22, 6, 3, 720, 2880, 240, 241.6, 3.4072, 3.0552, 19600], # 1.8h\n",
    "[ 22, 6, 3, 720, 2880, 224, 237.1, 3.4072,+3.0532, 19600], # 1.7h 14.6G\n",
    "[ 22, 6, 3, 720, 2880, 208, 232.5, 3.4317, 3.0552, 19600], # 1.6h\n",
    "[ 22, 6, 3, 720, 2880, 192, 228.0, 3.5637,+3.0520, 19600], # 1.6h\n",
    "[ 22, 6, 3, 720, 2880, 192, 228.0, 3.3994,+3.0522, 19600], # again\n",
    "[ 22, 6, 3, 720, 2880, 176, 223.4, 3.3774, 3.0540, 19600], # 1.6h 14.1G\n",
    "[ 22, 6, 3, 720, 2880, 160, 218.8, 3.4027, 3.0568, 19600], # 1.5h\n",
    "\n",
    "[ 22, 6, 3, 702, 2808, 272, 241.2, 3.3520, 3.0561, 19600], # 2.4h\n",
    "[ 22, 6, 3, 702, 2808, 256, 236.7, 3.3785,+3.0519, 19600], # 1.8h\n",
    "[ 22, 6, 3, 702, 2808, 256, 236.7, 3.3586, 3.0574, 19600], # again\n",
    "[ 22, 6, 3, 702, 2808, 224, 227.8, 3.3923, 3.0561, 19600], # 1.7h\n",
    "[ 22, 6, 3, 702, 2808, 192, 218.9, 3.3751,+3.0529, 19600], # 1.6h\n",
    "[ 22, 6, 3, 702, 2808, 176, 214.5, 3.3614, 3.0584, 19600], # 1.5h\n",
    "\n",
    "[ 22, 6, 3, 684, 2736, 272, 231.7, 3.3486, 3.0537, 19600], # 2.3h\n",
    "[ 22, 6, 3, 684, 2736, 256, 227.4, 3.4056, 3.0604, 19600], # 1.7h\n",
    "[ 22, 6, 3, 684, 2736, 240, 223.1, 3.3747, 3.0579, 19600], # 1.7h\n",
    "[ 22, 6, 3, 684, 2736, 224, 218.7, 3.4054, 3.0576, 19600], # 1.6h\n",
    "[ 22, 6, 3, 684, 2736, 208, 214.4, 3.3653,+3.0531, 19600], # 1.6h\n",
    "[ 22, 6, 3, 684, 2736, 192, 210.1, 3.3777, 3.0564, 19600], # 1.5h\n",
    "[ 22, 6, 3, 684, 2736, 176, 205.7, 3.4114, 3.0553, 19600], # 1.5h\n",
    "\n",
    "[ 22, 6, 3, 648, 2592, 272, 213.4, 3.4873, 3.0665, 19600], # 1.8h\n",
    "[ 22, 6, 3, 648, 2592, 256, 209.3, 3.3698, 3.0541, 19600], # 1.7h\n",
    "[ 22, 6, 3, 648, 2592, 240, 205.2, 3.3685, 3.0540, 19600], # 1.7h\n",
    "[ 22, 6, 3, 648, 2592, 224, 201.1, 3.3687, 3.0566, 19600], # 1.6h\n",
    "[ 22, 6, 3, 648, 2592, 208, 197.0, 3.3350, 3.0557, 19600], # 1.6h\n",
    "[ 22, 6, 3, 648, 2592, 192, 192.8, 3.3306, 3.0567, 19600], # 1.5h\n",
    "[ 22, 6, 3, 648, 2592, 176, 188.7, 3.3348, 3.0569, 19600], # 1.4h\n",
    "\n",
    "[ 22, 6, 3, 612, 2448, 272, 195.7, 3.3531, 3.0547, 19600], # 1.8h\n",
    "[ 22, 6, 3, 612, 2448, 256, 191.8, 3.3428, 3.0592, 19600], # 1.6h\n",
    "[ 22, 6, 3, 612, 2448, 240, 188.0, 3.3628, 3.0566, 19600], # 1.6h\n",
    "[ 22, 6, 3, 612, 2448, 224, 184.1, 3.3589, 3.0597, 19600], # 1.5h\n",
    "[ 22, 6, 3, 612, 2448, 208, 180.2, 3.3223, 3.0636, 19600], # 1.5h\n",
    "[ 22, 6, 3, 612, 2448, 192, 176.3, 3.3364, 3.0602, 19600], # 1.4h\n",
    "[ 22, 6, 3, 612, 2448, 176, 172.4, 3.3377, 3.0648, 19600], # 1.4h\n",
    "\n",
    "\n",
    "[ 22, 5, 5, 800, 3200, 288, 310.7, 3.5321, 3.0708, 19600], # 2.6h\n",
    "[ 22, 5, 5, 800, 3200, 272, 305.1, 3.4871, 3.0616, 19600], # 2.6h\n",
    "[ 22, 5, 5, 800, 3200, 256, 299.5, 3.4711, 3.0589, 19600], # 2.4h\n",
    "[ 22, 5, 5, 800, 3200, 240, 293.8, 3.5131, 3.0650, 19600], # 2.4h\n",
    "[ 22, 5, 5, 800, 3200, 224, 288.2, 3.5010, 3.0687, 19600], # 2.3h\n",
    "[ 22, 5, 5, 800, 3200, 208, 282.6, 3.5205, 3.0643, 19600], # 2.3h\n",
    "[ 22, 5, 5, 800, 3200, 192, 276.9, 3.5383, 3.0549, 19600], # 2.2h\n",
    "[ 22, 5, 5, 800, 3200, 176, 271.3, 3.5130, 3.0584, 19600], # 2.2h\n",
    "\n",
    "[ 22, 5, 5, 760, 3040, 288, 287.2, 3.4413, 3.0634, 19600], # 2.4h\n",
    "[ 22, 5, 5, 760, 3040, 272, 281.8, 3.4455, 3.0630, 19600], # 2.4h\n",
    "[ 22, 5, 5, 760, 3040, 256, 276.5, 3.4583, 3.0629, 19600], # 2.2h\n",
    "[ 22, 5, 5, 760, 3040, 240, 271.1, 3.4748, 3.0662, 19600], # 2.2h\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4648,+3.0507, 19600], # 1.7h 15.3G\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4902, 3.0637, 19600], # again\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4904, 3.0614, 19600], # again\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4466, 3.0675, 19600], # again\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.5142, 3.0692, 19600], # again\n",
    "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4376, 3.0642, 19600], # again\n",
    "[ 22, 5, 5, 760, 3040, 208, 260.4, 3.4316, 3.0566, 19600], # 1.6h\n",
    "[ 22, 5, 5, 760, 3040, 192, 255.1, 3.4342, 3.0561, 19600], # 1.6h\n",
    "[ 22, 5, 5, 760, 3040, 176, 249.7, 3.4263, 3.0574, 19600], # 1.6h\n",
    "\n",
    "[ 22, 5, 5, 720, 2880, 288, 264.5, 3.4361, 3.0563, 19600], # 2.3h\n",
    "[ 22, 5, 5, 720, 2880, 272, 259.4, 3.4997, 3.0633, 19600], # 2.3h\n",
    "[ 22, 5, 5, 720, 2880, 256, 254.3, 3.4398, 3.0627, 19600], # 2.2h\n",
    "[ 22, 5, 5, 720, 2880, 240, 249.2, 3.4339, 3.0562, 19600], # 1.7h\n",
    "[ 22, 5, 5, 720, 2880, 224, 244.2, 3.4074, 3.0567, 19600], # 1.6h 15.0G\n",
    "[ 22, 5, 5, 720, 2880, 208, 239.1, 3.4350, 3.0619, 19600], # 1.6h\n",
    "[ 22, 5, 5, 720, 2880, 192, 234.0, 3.3762, 3.0542, 19600], # 1.5h\n",
    "[ 22, 5, 5, 720, 2880, 176, 229.0, 3.4175, 3.0554, 19600], # 1.5h\n",
    "\n",
    "[ 22, 5, 5, 680, 2720, 288, 242.6, 3.3811, 3.0616, 19600], # 2.3h\n",
    "[ 22, 5, 5, 680, 2720, 272, 237.8, 3.4843, 3.0621, 19600], # 2.2h\n",
    "[ 22, 5, 5, 680, 2720, 256, 233.0, 3.3982, 3.0614, 19600], # 1.7h 14.6G\n",
    "[ 22, 5, 5, 680, 2720, 240, 228.2, 3.4236,+3.0533, 19600], # 1.6h\n",
    "[ 22, 5, 5, 680, 2720, 224, 223.4, 3.3797, 3.0543, 19600], # 1.6h\n",
    "[ 22, 5, 5, 680, 2720, 208, 218.6, 3.4178, 3.0604, 19600], # 1.5h\n",
    "[ 22, 5, 5, 680, 2720, 192, 213.9, 3.4374, 3.0576, 19600], # 1.5h\n",
    "[ 22, 5, 5, 680, 2720, 176, 209.1, 3.3960, 3.0569, 19600], # 1.4h\n",
    "\n",
    "[ 22, 5, 5, 640, 2560, 288, 221.6, 3.3868, 3.0649, 19600], # 2.1h\n",
    "[ 22, 5, 5, 640, 2560, 272, 217.0, 3.3785, 3.0574, 19600], # 2.1h\n",
    "[ 22, 5, 5, 640, 2560, 256, 212.5, 3.3878, 3.0620, 19600], # 1.6h\n",
    "[ 22, 5, 5, 640, 2560, 240, 208.0, 3.3841, 3.0578, 19600], # 1.5h\n",
    "[ 22, 5, 5, 640, 2560, 224, 203.5, 3.3555, 3.0617, 19600], # 1.5h\n",
    "[ 22, 5, 5, 640, 2560, 208, 199.0, 3.3681, 3.0618, 19600], # 1.4h\n",
    "[ 22, 5, 5, 640, 2560, 192, 194.5, 3.3598, 3.0563, 19600], # 1.4h\n",
    "[ 22, 5, 5, 640, 2560, 176, 190.0, 3.3847, 3.0608, 19600], # 1.3h\n",
    "\n",
    "[ 22, 5, 5, 600, 2400, 288, 201.4, 3.3220, 3.0622, 19600], # 2.1h\n",
    "[ 22, 5, 5, 600, 2400, 272, 197.1, 3.3371, 3.0629, 19600], # 2.0h\n",
    "[ 22, 5, 5, 600, 2400, 256, 192.9, 3.3058, 3.0598, 19600], # 1.9h\n",
    "[ 22, 5, 5, 600, 2400, 240, 188.7, 3.3571, 3.0638, 19600], # 1.8h\n",
    "[ 22, 5, 5, 600, 2400, 224, 184.5, 3.3359, 3.0627, 19600], # 1.8h\n",
    "[ 22, 5, 5, 600, 2400, 208, 180.3, 3.3304, 3.0592, 19600], # 1.8h\n",
    "[ 22, 5, 5, 600, 2400, 192, 176.0, 3.3101, 3.0602, 19600], # 1.7h\n",
    "[ 22, 5, 5, 600, 2400, 176, 171.8, 3.3680, 3.0700, 19600], # 1.7h\n",
    "\n",
    "\n",
    "[ 22, 5, 1, 800, 3200, 288, 270.2, 3.4215, 3.0615, 19600], # 2.4h\n",
    "[ 22, 5, 1, 800, 3200, 272, 266.8, 3.4592, 3.0557, 19600], # 2.3h\n",
    "[ 22, 5, 1, 800, 3200, 256, 263.4, 3.4562, 3.0626, 19600], # 2.2h\n",
    "[ 22, 5, 1, 800, 3200, 240, 260.0, 3.4421, 3.0700, 19600], # 2.2h\n",
    "[ 22, 5, 1, 800, 3200, 224, 256.7, 3.4155, 3.0681, 19600], # 2.2h\n",
    "[ 22, 5, 1, 800, 3200, 208, 253.3, 3.4043, 3.0596, 19600], # 2.1h\n",
    "[ 22, 5, 1, 800, 3200, 192, 249.9, 3.4755, 3.0639, 19600], # 2.1h\n",
    "[ 22, 5, 1, 800, 3200, 176, 246.5, 3.4220, 3.0554, 19600], # 2.0h\n",
    "\n",
    "[ 22, 5, 1, 760, 3040, 304, 251.9, 3.3972, 3.0579, 19600], # 2.2h\n",
    "[ 22, 5, 1, 760, 3040, 288, 248.7, 3.3855,+3.0531, 19600], # 2.2h\n",
    "[ 22, 5, 1, 760, 3040, 272, 245.4, 3.4222, 3.0625, 19600], # 2.2h\n",
    "[ 22, 5, 1, 760, 3040, 256, 242.2, 3.3929, 3.0598, 19600], # 2.0h\n",
    "[ 22, 5, 1, 760, 3040, 240, 239.0, 3.4069, 3.0581, 19600], # 1.6h\n",
    "[ 22, 5, 1, 760, 3040, 224, 235.8, 3.3965, 3.0556, 19600], # 1.6h\n",
    "[ 22, 5, 1, 760, 3040, 208, 232.6, 3.4181, 3.0625, 19600], # 1.5h\n",
    "[ 22, 5, 1, 760, 3040, 192, 229.4, 3.3993, 3.0566, 19600], # 1.5h\n",
    "[ 22, 5, 1, 760, 3040, 176, 226.2, 3.4060, 3.0618, 19600], # 1.9h\n",
    "\n",
    "[ 22, 5, 1, 720, 2880, 304, 231.0, 3.3691, 3.0628, 19600], # 2.2h\n",
    "[ 22, 5, 1, 720, 2880, 288, 228.0, 3.3564,+3.0516, 19600], # 2.1h\n",
    "[ 22, 5, 1, 720, 2880, 272, 224.9, 3.3926, 3.0574, 19600], # 2.1h\n",
    "[ 22, 5, 1, 720, 2880, 256, 221.9, 3.3783, 3.0618, 19600], # 2.0h\n",
    "[ 22, 5, 1, 720, 2880, 240, 218.8, 3.4436, 3.0594, 19600], # 1.6h 14.3G\n",
    "[ 22, 5, 1, 720, 2880, 224, 215.8, 3.4216, 3.0551, 19600], # 1.5h\n",
    "[ 22, 5, 1, 720, 2880, 208, 212.8, 3.4722, 3.0562, 19600], # 1.5h\n",
    "[ 22, 5, 1, 720, 2880, 192, 209.7, 3.3830, 3.0598, 19600], # 1.4h\n",
    "[ 22, 5, 1, 720, 2880, 176, 206.7, 3.3783, 3.0564, 19600], # 1.8h\n",
    "\n",
    "[ 22, 5, 1, 680, 2720, 288, 208.1, 3.3678, 3.0635, 19600], # 2.1h\n",
    "[ 22, 5, 1, 680, 2720, 272, 205.2, 3.3476, 3.0550, 19600], # 1.6h\n",
    "[ 22, 5, 1, 680, 2720, 256, 202.4, 3.3522, 3.0552, 19600], # 1.5h\n",
    "[ 22, 5, 1, 680, 2720, 240, 199.5, 3.3717, 3.0549, 19600], # 1.5h\n",
    "[ 22, 5, 1, 680, 2720, 224, 196.6, 3.3659, 3.0582, 19600], # 1.5h\n",
    "[ 22, 5, 1, 680, 2720, 208, 193.8, 3.4083, 3.0598, 19600], # 1.4h\n",
    "[ 22, 5, 1, 680, 2720, 192, 190.9, 3.4170, 3.0576, 19600], # 1.4h\n",
    "[ 22, 5, 1, 680, 2720, 176, 188.0, 3.3509, 3.0577, 19600], # 1.7h\n",
    "\n",
    "[ 22, 5, 1, 640, 2560, 288, 189.1, 3.3494, 3.0586, 19600], # 1.9h\n",
    "[ 22, 5, 1, 640, 2560, 272, 186.4, 3.3304, 3.0669, 19600], # 1.9h\n",
    "[ 22, 5, 1, 640, 2560, 256, 183.7, 3.3679, 3.0590, 19600], # 1.7h 13.9G\n",
    "[ 22, 5, 1, 640, 2560, 240, 181.0, 3.3146, 3.0584, 19600], # 1.4h\n",
    "[ 22, 5, 1, 640, 2560, 224, 178.3, 3.3534, 3.0576, 19600], # 1.4h\n",
    "[ 22, 5, 1, 640, 2560, 208, 175.6, 3.3415, 3.0608, 19600], # 1.3h\n",
    "[ 22, 5, 1, 640, 2560, 192, 172.9, 3.3454, 3.0599, 19600], # 1.3h\n",
    "[ 22, 5, 1, 640, 2560, 176, 170.2, 3.3859, 3.0646, 19600], # 1.6h\n",
    "\n",
    "[ 22, 5, 1, 600, 2400, 288, 171.0, 3.3335, 3.0589, 19600], # 1.8h\n",
    "[ 22, 5, 1, 600, 2400, 272, 168.4, 3.3141, 3.0663, 19600], # 1.8h\n",
    "[ 22, 5, 1, 600, 2400, 256, 165.9, 3.2964, 3.0733, 19600], # 1.7h\n",
    "[ 22, 5, 1, 600, 2400, 240, 163.4, 3.3153, 3.0649, 19600], # 1.7h\n",
    "[ 22, 5, 1, 600, 2400, 224, 160.8, 3.3168, 3.0691, 19600], # 1.6h\n",
    "[ 22, 5, 1, 600, 2400, 208, 158.3, 3.3118, 3.0655, 19600], # 1.6h\n",
    "[ 22, 5, 1, 600, 2400, 192, 155.8, 3.3824, 3.0779, 19600], # 1.6h\n",
    "[ 22, 5, 1, 600, 2400, 176, 153.2, 3.2930, 3.0693, 19600], # 1.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fe4330-d9e1-4b99-92dd-8911dd84c6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\anaconda3\\envs\\minGemma\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L22 att5 kv_heads5 hidden760 intermediate3040 head_dim224 T512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\AppData\\Local\\Temp\\ipykernel_12820\\3208739530.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:45:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.464800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22, 5, 5, 760, 3040, 224, 265.8, 3.4648, 3.0507, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  22 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 5 # 4 # G16 G8\n",
    "num_key_value_heads = 5 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*152 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0505:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8533f8-a081-4d67-995f-28d2d0defca3",
   "metadata": {},
   "source": [
    "# L20 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cbb8d-c12c-4b9f-8f56-772bca57136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L20 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 20, 8, 4, 928, 3712, 128, 310.5, 3.5399, 3.0653, 19600], # 2.0h\n",
    "[ 20, 8, 4, 864, 3456, 128, 275.9, 3.5633, 3.0599, 19600], # 1.5h\n",
    "[ 20, 8, 4, 800, 3200, 128, 243.1, 3.5120, 3.0582, 19600], # 1.4h\n",
    "[ 20, 8, 4, 848, 3392, 192, 293.5, 3.4913, 3.0596, 19600], # 20.9h\n",
    "[ 20, 8, 4, 800, 3200, 192, 267.7, 3.4751, 3.0610, 19600], # 1.9h\n",
    "[ 20, 8, 4, 736, 2944, 192, 235.0, 3.4224, 3.0582, 19600], # 1.7h\n",
    "[ 20, 8, 4, 704, 2816, 192, 219.4, 3.3811, 3.0561, 19600], # 1.6h\n",
    "[ 20, 8, 4, 768, 3072, 256, 274.7, 3.4261, 3.0589, 19600], # 2.0h\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.3451, 3.0546, 20800], # 2.0h\n",
    "[+20, 8, 4, 704, 2816, 272, 246.4, 3.4022,+3.0508, 19600], # 2.1h  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-03-23-24.pth\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.3796, 3.0536, 19600], # 1.9h\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.4142, 3.0549, 18400], # 1.8h\n",
    "[ 20, 8, 4, 640, 2560, 256, 209.3, 3.3627, 3.0575, 19600], # 1.8h\n",
    "\n",
    "[ 20, 6, 3, 792, 3168, 256, 263.5, 3.4414, 3.0612, 19600], # 1.9h 14.7G\n",
    "[ 20, 6, 3, 768, 3072, 240, 246.7, 3.4773, 3.0552, 19600], # 1.7h 14.2G\n",
    "[ 20, 6, 3, 696, 2784, 192, 199.5, 3.4094, 3.0554, 19600], # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df660d90-a0d1-4371-b4d7-c67b09bf98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L20 att8 kv_heads4 hidden704 intermediate2816 head_dim272 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:08:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.402200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.4022, 3.0508, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  20 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 272 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0508: #3.0484:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a810b8b-e27b-49b1-95c0-e5b6c9d0ada8",
   "metadata": {},
   "source": [
    "# L18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f7057-01e1-4736-b560-5d18fbd9c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L18 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "\n",
    "[ 18,12, 4, 792, 3168, 256, 292.2, 3.4836, 3.0652, 19600], # 2.5h 15.4G\n",
    "[ 18,12, 4, 744, 2976, 256, 266.8, 3.3787, 3.0570, 19600], # 2.3h\n",
    "[ 18,12, 4, 696, 2784, 256, 242.4, 3.3779, 3.0572, 19600], # 2.2h\n",
    "[ 18,12, 4, 648, 2592, 256, 218.9, 3.3269, 3.0615, 19600], # 2.1h\n",
    "[ 18,12, 4, 600, 2400, 256, 196.5, 3.2975, 3.0620, 19600], # 2.0h\n",
    "\n",
    "\n",
    "[ 18, 9, 3, 792, 3168, 256, 263.0, 3.4692, 3.0574, 19600], # 2.1h 14.6G\n",
    "[ 18, 9, 3, 792, 3168, 224, 252.1, 3.4288, 3.0596, 19600], # 1.9h\n",
    "[ 18, 9, 3, 792, 3168, 192, 241.1, 3.5182, 3.0571, 19600], # 1.8h\n",
    "[ 18, 9, 3, 756, 3024, 272, 250.4, 3.4079, 3.0598, 19600], # 2.1h\n",
    "[ 18, 9, 3, 756, 3024, 256, 245.2, 3.4225, 3.0548, 19600], # 1.9h\n",
    "[ 18, 9, 3, 756, 3024, 240, 240.0, 3.4291, 3.0566, 19600], # 1.9h\n",
    "[ 18, 9, 3, 756, 3024, 224, 234.8, 3.4086, 3.0547, 19600], # 1.8h\n",
    "[ 18, 9, 3, 756, 3024, 208, 229.5, 3.4232, 3.0600, 19600], # 1.7h\n",
    "[ 18, 9, 3, 756, 3024, 192, 224.3, 3.3821, 3.0591, 19600], # 1.6h\n",
    "[ 18, 9, 3, 720, 2880, 256, 227.9, 3.3842, 3.0579, 19600], # 1.9h\n",
    "[ 18, 9, 3, 720, 2880, 224, 218.0, 3.3829, 3.0601, 19600], # 1.7h\n",
    "[ 18, 9, 3, 720, 2880, 208, 213.0, 3.4078,+3.0531, 19600], # 1.7h\n",
    "[ 18, 9, 3, 720, 3600, 192, 236.0, 3.5293, 3.0575, 19600], # 1.7h 14.5G x5\n",
    "[ 18, 9, 3, 720, 2880, 192, 208.0, 3.4005,+3.0519, 19600], # 1.6h  minGemma-hidden_layers18-att_heads9-kv_heads3-hidden720-intermediate2880-head_dim192-T512--2025-06-27-07-35.pth\n",
    "[ 18, 9, 3, 720, 2880, 176, 203.0, 3.3865, 3.0576, 19600], # 1.6h\n",
    "[ 18, 9, 3, 684, 2736, 272, 215.9, 3.3567, 3.0562, 19600], # 2.0h\n",
    "[ 18, 9, 3, 684, 2736, 256, 211.2, 3.3431, 3.0543, 19600], # 1.8h\n",
    "[ 18, 9, 3, 684, 2736, 240, 206.5, 3.3505, 3.0586, 19600], # 1.8h\n",
    "[ 18, 9, 3, 684, 2736, 224, 201.8, 3.4041, 3.0602, 19600], # 1.7h\n",
    "[ 18, 9, 3, 684, 2736, 192, 192.3, 3.3641, 3.0604, 19600], # 1.5h\n",
    "[ 18, 9, 3, 648, 2592, 272, 199.5, 3.3438, 3.0561, 19600], # 2.0h \n",
    "[ 18, 9, 3, 648, 2592, 256, 195.1, 3.3260,+3.0540, 19600], # 1.8h\n",
    "[ 18, 9, 3, 648, 2592, 240, 190.6, 3.3112, 3.0577, 19600], # 1.7h\n",
    "[ 18, 9, 3, 648, 2592, 224, 186.1, 3.3340, 3.0642, 19600], # 1.6h\n",
    "[ 18, 9, 3, 648, 2592, 192, 177.1, 3.3516, 3.0623, 19600], # 1.5h\n",
    "[ 18, 9, 3, 612, 2448, 256, 179.5, 3.3227, 3.0595, 19600], # 1.7h\n",
    "[ 18, 9, 3, 612, 2448, 224, 171.0, 3.3277, 3.0611, 19600], # 1.6h\n",
    "[ 18, 9, 3, 612, 2448, 192, 162.5, 3.3382, 3.0638, 19600], # 1.5h\n",
    "\n",
    "\n",
    "[ 18, 8, 4, 960, 3840, 144, 307.2, 3.5791, 3.0738, 19600], # 1.9h\n",
    "[ 18, 8, 4, 912, 3648, 192, 301.3, 3.6117, 3.0670, 19600], #18.9h\n",
    "[ 18, 8, 4, 896, 3584, 144, 274.4, 3.5357, 3.0632, 19600], # 1.7h\n",
    "[ 18, 8, 4, 864, 3456, 192, 276.5, 3.5424, 3.0611, 19600], # 1.8h\n",
    "[ 18, 8, 4, 848, 3392, 256, 291.9, 3.6446, 3.0676, 19600], #16.9h\n",
    "[ 18, 8, 4, 832, 3328, 144, 243.3, 3.4639, 3.0592, 19600], # 1.6h\n",
    "[ 18, 8, 4, 816, 3264, 256, 275.2, 3.5260, 3.0607, 20800], # 2.1h\n",
    "[ 18, 8, 4, 816, 3264, 256, 275.2, 3.4915, 3.0554, 19600], # 2.0h\n",
    "[ 18, 8, 4, 816, 3264, 224, 264.0, 3.4727, 3.0597, 19600], # 1.9h\n",
    "[ 18, 8, 4, 816, 3264, 192, 252.7, 3.5275, 3.0552, 19600], # 1.7h\n",
    "[ 18, 8, 4, 800, 3200, 192, 245.0, 3.4583, 3.0574, 19600], # 1.7h\n",
    "\n",
    "[ 18, 8, 4, 768, 3072, 256, 251.1, 3.4846, 3.0616, 19600], # 1.8h\n",
    "[ 18, 8, 4, 768, 3072, 224, 240.5, 3.4745, 3.0615, 19600], # 1.7h\n",
    "[ 18, 8, 4, 768, 3072, 144, 213.9, 3.4209, 3.0596, 19600], # 1.4h\n",
    "\n",
    "[ 18, 8, 4, 736, 2944, 256, 235.5, 3.4929, 3.0549, 19600], # 1.8h\n",
    "[ 18, 8, 4, 736, 2944, 224, 225.4, 3.3994, 3.0570, 19600], # 1.7h\n",
    "[ 18, 8, 4, 736, 2944, 192, 215.2, 3.4236, 3.0604, 19600], # 1.6h\n",
    "[ 18, 8, 4, 736, 2944, 144, 199.9, 3.3939, 3.0553, 19600], # 1.4h 13.1G\n",
    "\n",
    "[ 18, 8, 4, 704, 2816, 288, 230.2, 3.4114, 3.0583, 19600], # 1.9h\n",
    "[ 18, 8, 4, 704, 2816, 272, 225.3, 3.4582, 3.0586, 19600], # 1.9h\n",
    "[+18, 8, 4, 704, 2816, 256, 220.4, 3.4075,+3.0537, 19600], # 1.7h\n",
    "[ 18, 8, 4, 704, 2816, 240, 215.6, 3.4002, 3.0548, 19600], # 1.7h\n",
    "[ 18, 8, 4, 704, 2816, 224, 210.7, 3.3961, 3.0546, 19600], # 1.6h 13.5G\n",
    "[ 18, 8, 4, 704, 2816, 208, 205.8, 3.4050, 3.0583, 19600], # 1.6h\n",
    "[ 18, 8, 4, 704, 2816, 192, 201.0, 3.4084, 3.0612, 19600], # 1.5h\n",
    "[ 18, 8, 4, 704, 2816, 144, 186.4, 3.3926, 3.0567, 19600], # 1.4h 12.7G\n",
    "\n",
    "[ 18, 8, 4, 672, 2688, 256, 205.8, 3.3864, 3.0562, 19600], # 1.7h\n",
    "[ 18, 8, 4, 672, 2688, 224, 196.5, 3.3632, 3.0571, 19600], # 1.6h\n",
    "[ 18, 8, 4, 672, 2688, 192, 187.2, 3.4178, 3.0613, 19600], # 1.5h\n",
    "[ 18, 8, 4, 672, 2688, 144, 173.3, 3.3879, 3.0580, 19600], # 1.3h\n",
    "\n",
    "[ 18, 8, 4, 640, 2560, 256, 191.5, 3.4145, 3.0594, 19600], # 1.6h\n",
    "[ 18, 8, 4, 640, 2560, 224, 182.7, 3.3567, 3.0590, 19600], # 1.5h\n",
    "[ 18, 8, 4, 640, 2560, 192, 173.8, 3.3544, 3.0605, 19600], # 1.4h 12.6G\n",
    "[ 18, 8, 4, 640, 2560, 144, 160.6, 3.3712, 3.0659, 19600], # 1.2h\n",
    "\n",
    "[ 18, 8, 4, 608, 2432, 256, 177.8, 3.3245, 3.0602, 19600], # 1.6h\n",
    "[ 18, 8, 4, 608, 2432, 224, 169.4, 3.3441, 3.0615, 19600], # 1.5h\n",
    "[ 18, 8, 4, 608, 2432, 192, 161.0, 3.3319, 3.0638, 19600], # 1.4h\n",
    "[ 18, 8, 4, 608, 2432, 144, 148.3, 3.3211, 3.0661, 19600], # 1.2h 10.9G\n",
    "\n",
    "\n",
    "[ 18, 8, 2, 800, 3200, 208, 238.5, 3.4431, 3.0633, 19600], # 1.7h\n",
    "[ 18, 8, 2, 800, 3200, 192, 233.9, 3.4250, 3.0613, 19600], # 1.7h\n",
    "[ 18, 8, 2, 800, 3200, 176, 229.3, 3.4283, 3.0596, 19600], # 1.6h\n",
    "[ 18, 8, 2, 768, 3072, 288, 245.8, 3.4939, 3.0638, 19600], # 2.0h\n",
    "[ 18, 8, 2, 768, 3072, 256, 236.9, 3.4335, 3.0614, 19600], # 1.8h\n",
    "[ 18, 8, 2, 768, 3072, 224, 228.1, 3.3652, 3.0579, 19600], # 1.7h\n",
    "[ 18, 8, 2, 768, 3072, 208, 223.7, 3.4075, 3.0595, 19600], # 1.6h\n",
    "[ 18, 8, 2, 768, 3072, 192, 219.2, 3.4672,+3.0542, 19600], # 1.5h\n",
    "[ 18, 8, 2, 768, 3072, 176, 214.8, 3.4474, 3.0632, 19600], # 1.5h\n",
    "[ 18, 8, 2, 768, 3072, 160, 210.4, 3.4459, 3.0571, 19600], # 1.4h\n",
    "[ 18, 8, 2, 736, 2944, 256, 222.0, 3.3920, 3.0571, 19600], # 1.7h\n",
    "[ 18, 8, 2, 736, 2944, 224, 213.5, 3.3924, 3.0568, 19600], # 1.6h\n",
    "[ 18, 8, 2, 736, 2944, 208, 209.3, 3.4053, 3.0580, 19600], # 1.6h\n",
    "[ 18, 8, 2, 736, 2944, 192, 205.0, 3.4079, 3.0640, 19600], # 1.5h\n",
    "[ 18, 8, 2, 736, 2944, 176, 200.8, 3.3875, 3.0565, 19600], # 1.5h\n",
    "[ 18, 8, 2, 736, 2944, 160, 196.5, 3.4298, 3.0637, 19600], # 1.4h\n",
    "[ 18, 8, 2, 704, 2816, 288, 215.6, 3.3755, 3.0566, 19600], # 1.9h\n",
    "[ 18, 8, 2, 704, 2816, 256, 207.5, 3.3993, 3.0600, 19600], # 1.7h\n",
    "[ 18, 8, 2, 704, 2816, 224, 199.3, 3.4104, 3.0646, 19600], # 1.6h\n",
    "[ 18, 8, 2, 704, 2816, 192, 191.2, 3.3632, 3.0624, 19600], # 1.5h\n",
    "[ 18, 8, 2, 704, 2816, 160, 183.1, 3.3830, 3.0620, 19600], # 1.4h\n",
    "[ 18, 8, 2, 672, 2688, 256, 193.4, 3.3795, 3.0586, 19600], # 1.6h\n",
    "[ 18, 8, 2, 672, 2688, 240, 189.5, 3.4028,+3.0542, 19600], # 1.6h\n",
    "[ 18, 8, 2, 672, 2688, 224, 185.6, 3.3633, 3.0612, 19600], # 1.5h\n",
    "[ 18, 8, 2, 672, 2688, 208, 181.8, 3.3608, 3.0585, 19600], # 1.5h\n",
    "[ 18, 8, 2, 672, 2688, 192, 177.9, 3.3694, 3.0642, 19600], # 1.4h\n",
    "[ 18, 8, 2, 672, 2688, 176, 174.0, 3.3653, 3.0579, 19600], # 1.4h\n",
    "[ 18, 8, 2, 640, 2560, 256, 179.7, 3.3572, 3.0669, 19600], # 1.6h\n",
    "[ 18, 8, 2, 640, 2560, 240, 176.1, 3.3432, 3.0619, 19600], # 1.5h\n",
    "[ 18, 8, 2, 640, 2560, 224, 172.4, 3.3485, 3.0605, 19600], # 1.5h\n",
    "[ 18, 8, 2, 640, 2560, 208, 168.7, 3.3448, 3.0656, 19600], # 1.4h\n",
    "[ 18, 8, 2, 640, 2560, 192, 165.0, 3.3153, 3.0617, 19600], # 1.4h\n",
    "[ 18, 8, 2, 640, 2560, 176, 161.3, 3.3769, 3.0576, 19600], # 1.3h\n",
    "\n",
    "\n",
    "[ 18, 7, 7, 784, 3136, 256, 273.5, 3.4935, 3.0612, 19600], # 1.9h\n",
    "[ 18, 7, 7, 784, 3136, 224, 260.8, 3.4884, 3.0623, 19600], # 1.9h\n",
    "[ 18, 7, 7, 784, 3136, 192, 248.2, 3.4954, 3.0611, 19600], # 1.7h\n",
    "[ 18, 7, 7, 728, 2912, 256, 245.1, 3.5636, 3.0689, 19600], # 1.8h\n",
    "[ 18, 7, 7, 728, 2912, 224, 233.4, 3.4266,+3.0553, 19600], # 1.6h\n",
    "[ 18, 7, 7, 728, 2912, 192, 221.7, 3.4250, 3.0609, 19600], # 1.5h\n",
    "[ 18, 7, 7, 672, 2688, 256, 218.2, 3.4327, 3.0589, 19600], # 1.7h\n",
    "[ 18, 7, 7, 672, 2688, 224, 207.3, 3.4002, 3.0609, 19600], # 1.6h\n",
    "[ 18, 7, 7, 672, 2688, 192, 196.5, 3.3750, 3.0562, 19600], # 1.4h\n",
    "[ 18, 7, 7, 616, 2464, 256, 192.5, 3.3483, 3.0592, 19600], # 1.6h\n",
    "[ 18, 7, 7, 616, 2464, 224, 182.6, 3.3655, 3.0625, 19600], # 1.5h\n",
    "[ 18, 7, 7, 616, 2464, 192, 172.7, 3.3502, 3.0566, 19600], # 1.4h\n",
    "\n",
    "\n",
    "[ 18, 7, 1, 784, 3136, 256, 230.1, 3.4027, 3.0614, 19600], # 1.7h\n",
    "[ 18, 7, 1, 784, 3136, 224, 222.9, 3.4335,+3.0560, 19600], # 1.6h\n",
    "[ 18, 7, 1, 784, 3136, 192, 215.7, 3.4065, 3.0639, 19600], # 1.5h\n",
    "[ 18, 7, 1, 728, 2912, 256, 204.9, 3.3801, 3.0582, 19600], # 1.5h\n",
    "[ 18, 7, 1, 728, 2912, 224, 198.2, 3.3786, 3.0618, 19600], # 1.5h\n",
    "[ 18, 7, 1, 728, 2912, 192, 191.5, 3.3600, 3.0592, 19600], # 1.4h\n",
    "[ 18, 7, 1, 672, 2688, 256, 181.0, 3.4220, 3.0643, 19600], # 1.5h\n",
    "[ 18, 7, 1, 672, 2688, 224, 174.8, 3.4895, 3.0636, 19600], # 1.4h\n",
    "[ 18, 7, 1, 672, 2688, 192, 168.6, 3.3798, 3.0629, 19600], # 1.3h\n",
    "[ 18, 7, 1, 616, 2464, 256, 158.5, 3.3368, 3.0709, 19600], # 1.4h\n",
    "[ 18, 7, 1, 616, 2464, 224, 152.8, 3.4722, 3.0753, 19600], # 1.3h\n",
    "[ 18, 7, 1, 616, 2464, 192, 147.1, 3.3360, 3.0661, 19600], # 1.2h\n",
    "\n",
    "\n",
    "[ 18, 6, 3, 816, 3264, 256, 252.7, 3.5178, 3.0612, 19600], # 1.7h\n",
    "[ 18, 6, 3, 816, 3264, 240, 248.4, 3.4537, 3.0579, 19600], # 2.1h\n",
    "[ 18, 6, 3, 816, 3264, 224, 244.2, 3.4968, 3.0584, 19600], # 1.6h\n",
    "[ 18, 6, 3, 816, 3264, 192, 235.8, 3.5066, 3.0616, 19600], # 1.5h\n",
    "\n",
    "[ 18, 6, 3, 768, 3072, 288, 237.8, 3.5033, 3.0607, 19600], # 1.7h\n",
    "[ 18, 6, 3, 768, 3072, 272, 233.8, 3.4317, 3.0611, 19600], # 1.7h\n",
    "[ 18, 6, 3, 768, 3072, 256, 229.9, 3.4561, 3.0541, 19600], # 1.5h 13.5G\n",
    "[ 18, 6, 3, 768, 3072, 240, 225.9, 3.4607, 3.0537, 19600], # 1.5h\n",
    "[ 18, 6, 3, 768, 3072, 224, 221.9, 3.4849, 3.0557, 19600], # 1.5h\n",
    "[ 18, 6, 3, 768, 3072, 192, 213.9, 3.4424, 3.0605, 19600], # 1.4h\n",
    "[ 18, 6, 3, 768, 3072, 176, 209.9, 3.4007, 3.0658, 19600], # 1.7h\n",
    "\n",
    "[ 18, 6, 3, 744, 2976, 272, 222.7, 3.4427, 3.0555, 19600], # 2.0h\n",
    "[ 18, 6, 3, 744, 2976, 256, 218.8, 3.4641, 3.0550, 19600], # 1.5h\n",
    "[ 18, 6, 3, 744, 2976, 240, 215.0, 3.4758, 3.0583, 19600], # 1.5h\n",
    "[ 18, 6, 3, 744, 2976, 224, 211.1, 3.4213, 3.0546, 19600], # 1.5h\n",
    "[ 18, 6, 3, 744, 2976, 208, 207.2, 3.3880, 3.0588, 19600], # 1.8h\n",
    "[ 18, 6, 3, 744, 2976, 192, 203.4, 3.3816, 3.0551, 19600], # 1.7h\n",
    "[ 18, 6, 3, 744, 2976, 176, 199.5, 3.3719, 3.0592, 19600], # 1.7h\n",
    "\n",
    "[ 18, 6, 3, 720, 2880, 304, 219.2, 3.3965, 3.0554, 19600], # 2.1h\n",
    "[ 18, 6, 3, 720, 2880, 288, 215.5, 3.4322, 3.0597, 19600], # 1.6h\n",
    "[ 18, 6, 3, 720, 2880, 272, 211.8, 3.4414, 3.0515, 19600], # 1.6h\n",
    "[ 18, 6, 3, 720, 2880, 256, 208.0, 3.4473, 3.0538, 19600], # 1.5h\n",
    "[ 18, 6, 3, 720, 2880, 240, 204.3, 3.4070, 3.0512, 19600], # 1.5h\n",
    "[+18, 6, 3, 720, 2880, 224, 200.6, 3.4125,+3.0504, 19600], # 1.4h    BEST (not saved)\n",
    "[ 18, 6, 3, 720, 2880, 224, 200.6, 3.4594, 3.0541, 19600], # again\n",
    "[ 18, 6, 3, 720, 2880, 224, 200.6, 3.4334, 3.0556, 19600], # again\n",
    "[ 18, 6, 3, 720, 2880, 224, 200.6, 3.4043, 3.0561, 19600], # again\n",
    "[ 18, 6, 3, 720, 2880, 208, 196.8, 3.4140, 3.0597, 19600], # 1.4h\n",
    "[ 18, 6, 3, 720, 2880, 192, 193.1, 3.3948, 3.0563, 19600], # 1.3h\n",
    "[ 18, 6, 3, 720, 2880, 176, 189.4, 3.3908, 3.0558, 19600], # 1.6h\n",
    "\n",
    "[ 18, 6, 3, 696, 2784, 272, 201.1, 3.4697, 3.0632, 19600], # 1.9h\n",
    "[ 18, 6, 3, 696, 2784, 256, 197.5, 3.3834, 3.0555, 19600], # 1.5h\n",
    "[ 18, 6, 3, 696, 2784, 240, 193.9, 3.4331, 3.0595, 19600], # 1.4h\n",
    "[ 18, 6, 3, 696, 2784, 224, 190.3, 3.4553, 3.0574, 19600], # 1.4h\n",
    "[ 18, 6, 3, 696, 2784, 208, 186.7, 3.3617, 3.0567, 19600], # 1.7h\n",
    "[ 18, 6, 3, 696, 2784, 192, 183.0, 3.3772, 3.0654, 19600], # 1.6h 12.9G\n",
    "[ 18, 6, 3, 696, 2784, 176, 179.4, 3.3958, 3.0600, 19600], # 1.6h 13.0G\n",
    "\n",
    "[ 18, 6, 3, 672, 2688, 256, 187.2, 3.4093, 3.0564, 19600], # 1.4h 13.2G\n",
    "[ 18, 6, 3, 672, 2688, 224, 180.2, 3.4897, 3.0594, 19600], # 1.3h\n",
    "[ 18, 6, 3, 672, 2688, 192, 173.3, 3.3773, 3.0574, 19600], # 1.2h\n",
    "\n",
    "[ 18, 6, 3, 624, 2496, 256, 167.3, 3.3447, 3.0583, 19600], # 1.3h\n",
    "[ 18, 6, 3, 624, 2496, 224, 160.9, 3.3517, 3.0620, 19600], # 1.3h\n",
    "[ 18, 6, 3, 624, 2496, 192, 154.4, 3.3389, 3.0583, 19600], # 1.2h\n",
    "\n",
    "[ 18, 6, 3, 528, 2112, 128, 108.8, 3.3236, 3.0796, 19600], # 0.8h\n",
    "\n",
    "\n",
    "[ 18, 6, 2, 816, 3264, 256, 245.2, 3.4788, 3.0561, 19600], # 1.7h\n",
    "[ 18, 6, 2, 816, 3264, 224, 237.6, 3.4507,+3.0554, 19600], # 1.6h\n",
    "[ 18, 6, 2, 816, 3264, 192, 230.1, 3.5086, 3.0606, 19600], # 1.5h\n",
    "[ 18, 6, 2, 768, 3072, 256, 222.8, 3.4830, 3.0601, 19600], # 1.5h\n",
    "[ 18, 6, 2, 768, 3072, 224, 215.7, 3.4395, 3.0562, 19600], # 1.4h\n",
    "[ 18, 6, 2, 768, 3072, 192, 208.6, 3.5248, 3.0594, 19600], # 1.3h\n",
    "[ 18, 6, 2, 720, 2880, 256, 201.4, 3.4027, 3.0567, 19600], # 1.4h\n",
    "[ 18, 6, 2, 720, 2880, 224, 194.8, 3.3945, 3.0563, 19600], # 1.4h\n",
    "[ 18, 6, 2, 720, 2880, 192, 188.1, 3.4160, 3.0574, 19600], # 1.3h\n",
    "[ 18, 6, 2, 672, 2688, 256, 181.0, 3.3960, 3.0585, 19600], # 1.4h\n",
    "[ 18, 6, 2, 672, 2688, 224, 174.8, 3.5027, 3.0620, 19600], # 1.3h\n",
    "[ 18, 6, 2, 672, 2688, 192, 168.6, 3.4028, 3.0585, 19600], # 1.2h\n",
    "[ 18, 6, 2, 624, 2496, 256, 161.6, 3.3549, 3.0625, 19600], # 1.3h\n",
    "[ 18, 6, 2, 624, 2496, 224, 155.8, 3.3462, 3.0610, 19600], # 1.2h\n",
    "[ 18, 6, 2, 624, 2496, 192, 150.1, 3.3330, 3.0656, 19600], # 1.2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f6881f9-5a68-4740-a945-0d6b11602225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L18 att6 kv_heads3 hidden720 intermediate2880 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:27:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.412500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18, 6, 3, 720, 2880, 224, 200.6, 3.4125, 3.0504, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  18 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*120 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0519:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3aa9e-dd90-47e1-ad3f-3eda9d082cbf",
   "metadata": {},
   "source": [
    "# L16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1600dad-6725-4a5d-b199-a0b098e77bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L16 Normal Model (default: B12 x4 lr13.5e-4 WD1)\n",
    "[ 16,12, 6, 696, 2784, 224, 217.9, 3.3786, 3.0553, 19600], # 1.9h 13.7G\n",
    "[ 16,12, 4, 696, 2784, 224, 207.9, 3.3824, 3.0603, 19600], # 1.8h 13.7G\n",
    "[ 16,12, 3, 696, 2784, 224, 202.9, 3.3565, 3.0611, 19600], # 1.8h\n",
    "[ 16,10, 5, 680, 2720, 224, 196.2, 3.3722, 3.0608, 19600], # 1.7h\n",
    "[ 16,10, 2, 680, 2720, 224, 181.6, 3.3138, 3.0634, 19600], # 1.6h\n",
    "[ 16,10, 1, 680, 2720, 224, 176.7, 3.3655, 3.0689, 19600], # 1.6h\n",
    "[ 16, 9, 9, 756, 3024, 256, 259.3, 3.4331, 3.0571, 19600], # 2.0h\n",
    "[ 16, 9, 9, 684, 2736, 224, 212.6, 3.3968, 3.0588, 19600], # 1.7h 14.0G\n",
    "\n",
    "[ 16, 9, 3, 792, 3168, 288, 248.0, 3.4878, 3.0605, 19600], # 2.1h\n",
    "[ 16, 9, 3, 792, 3168, 256, 238.2, 3.4353, 3.0605, 19600], # 1.9h\n",
    "[ 16, 9, 3, 792, 3168, 224, 228.5, 3.4456, 3.0610, 19600], # 1.7h\n",
    "[ 16, 9, 3, 792, 3168, 192, 218.8, 3.5141, 3.0611, 19600], # 1.6h\n",
    "[ 16, 9, 3, 792, 3168, 160, 209.0, 3.4412, 3.0646, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 304, 236.1, 3.4335, 3.0568, 19600], # 2.0h\n",
    "[ 16, 9, 3, 756, 3024, 288, 231.5, 3.4106,+3.0552, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 272, 226.8, 3.4141, 3.0610, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 256, 222.2, 3.4238, 3.0567, 19600], # 1.7h 13.3G\n",
    "[ 16, 9, 3, 756, 3024, 224, 212.9, 3.4077, 3.0592, 19600], # 1.6h\n",
    "[ 16, 9, 3, 756, 3024, 192, 203.6, 3.3903, 3.0588, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 160, 194.3, 3.4151, 3.0577, 19600], # 1.4h 13.1G\n",
    "[ 16, 9, 3, 720, 2880, 320, 224.3, 3.3521, 3.0561, 19600], # 2.0h\n",
    "[ 16, 9, 3, 720, 2880, 304, 219.9, 3.4003, 3.0572, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 288, 215.5, 3.3564,+3.0556, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 272, 211.0, 3.3431, 3.0589, 19600], # 1.8h\n",
    "[ 16, 9, 3, 720, 2880, 256, 206.6, 3.3858, 3.0575, 19600], # 1.7h\n",
    "[ 16, 9, 3, 720, 2880, 224, 197.8, 3.4157, 3.0565, 19600], # 1.6h\n",
    "[ 16, 9, 3, 720, 2880, 192, 188.9, 3.4725, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 720, 2880, 160, 180.1, 3.4023, 3.0613, 19600], # 1.3h\n",
    "[ 16, 9, 3, 702, 2808, 224, 190.4, 3.3924, 3.0624, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 288, 200.0, 3.3698, 3.0613, 19600], # 1.8h\n",
    "[ 16, 9, 3, 684, 2736, 256, 191.6, 3.3788, 3.0613, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 240, 187.4, 3.3933, 3.0585, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 3420, 224, 205.6, 3.4053, 3.0570, 19600], # 1.6h x5\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3528,+3.0538, 19600], # 1.5h # local min\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3641, 3.0562, 19600], # again\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3679,+3.0545, 19600], # again\n",
    "[ 16, 9, 3, 684, 2052, 224, 160.7, 3.3504, 3.0636, 19600], # 1.4h x3\n",
    "[ 16, 9, 3, 684, 2736, 208, 179.0, 3.4294, 3.0651, 19600], # 1.5h\n",
    "[ 16, 9, 3, 684, 2736, 192, 174.8, 3.3822, 3.0578, 19600], # 1.4h\n",
    "[ 16, 9, 3, 684, 2736, 160, 166.4, 3.3546, 3.0708, 19600], # 1.3h\n",
    "[ 16, 9, 3, 666, 2664, 224, 176.0, 3.3408, 3.0619, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 288, 185.0, 3.4168, 3.0575, 19600], # 1.8h\n",
    "[ 16, 9, 3, 648, 2592, 256, 177.0, 3.3494, 3.0597, 19600], # 1.6h\n",
    "[ 16, 9, 3, 648, 2592, 224, 169.0, 3.3636, 3.0596, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 208, 165.1, 3.3476, 3.0566, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 192, 161.1, 3.3674,+3.0552, 19600], # 1.4h\n",
    "[ 16, 9, 3, 648, 2592, 176, 157.1, 3.3518, 3.0571, 19600], # 1.3h\n",
    "[ 16, 9, 3, 648, 2592, 160, 153.1, 3.4071, 3.0621, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 288, 170.5, 3.3219, 3.0604, 19600], # 1.7h\n",
    "[ 16, 9, 3, 612, 2448, 256, 162.9, 3.3793, 3.0589, 19600], # 1.5h\n",
    "[ 16, 9, 3, 612, 2448, 224, 155.4, 3.3291, 3.0650, 19600], # 1.4h\n",
    "[ 16, 9, 3, 612, 2448, 192, 147.9, 3.3395, 3.0629, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 160, 140.4, 3.3432, 3.0705, 19600], # 1.2h\n",
    "[ 16, 9, 3, 576, 2304, 288, 156.5, 3.3139, 3.0602, 19600], # 1.7h\n",
    "[ 16, 9, 3, 576, 2304, 256, 149.4, 3.3302, 3.0645, 19600], # 1.5h\n",
    "[ 16, 9, 3, 576, 2304, 224, 142.3, 3.3453, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 576, 2304, 192, 135.2, 3.3139, 3.0647, 19600], # 1.3h\n",
    "[ 16, 9, 3, 576, 2304, 160, 128.1, 3.3197, 3.0732, 19600], # 1.2h\n",
    "\n",
    "[ 16, 9, 1, 684, 2736, 224, 173.4, 3.3315, 3.0629, 19600], # 1.5h\n",
    "\n",
    "[ 16, 8, 4, 960, 3840, 144, 278.4, 3.5831, 3.0678, 19600], # 1.7h\n",
    "[ 16, 8, 4, 912, 3648, 192, 272.9, 3.5625, 3.0690, 19600], # 1.8h\n",
    "[ 16, 8, 4, 896, 3584, 144, 248.9, 3.5718, 3.0678, 19600], # 1.5h\n",
    "[ 16, 8, 4, 880, 3520, 256, 279.6, 3.5187, 3.0604, 19600], # 1.9h\n",
    "[ 16, 8, 4, 864, 3456, 192, 250.6, 3.5129, 3.0600, 19600], # 1.6h\n",
    "[ 16, 8, 4, 832, 3328, 256, 256.7, 3.5653, 3.0695, 19600], # 1.8h\n",
    "[ 16, 8, 4, 832, 3328, 144, 220.9, 3.4968, 3.0645, 19600], # 1.4h\n",
    "[ 16, 8, 4, 800, 3200, 256, 241.9, 3.4717, 3.0610, 19600], # 1.7h\n",
    "[ 16, 8, 4, 800, 3200, 224, 232.0, 3.4724, 3.0602, 19600], # 1.6h\n",
    "[ 16, 8, 4, 800, 3200, 192, 222.2, 3.4555, 3.0601, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 288, 236.9, 3.4780, 3.0595, 19600], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4080, 3.0599, 20800], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4434, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 224, 218.0, 3.4582, 3.0578, 19600], # 1.5h\n",
    "[ 16, 8, 4, 768, 3072, 192, 208.6, 3.4559, 3.0593, 19600], # 1.4h\n",
    "[ 16, 8, 4, 736, 2944, 256, 213.5, 3.3695, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 736, 2944, 224, 204.4, 3.3880, 3.0584, 19600], # 1.5h\n",
    "[ 16, 8, 4, 736, 2944, 192, 195.4, 3.4246, 3.0605, 19600], # 1.4h 13.1G\n",
    "[ 16, 8, 4, 704, 2816, 256, 199.9, 3.4143, 3.0660, 19600], # 1.6h\n",
    "[ 16, 8, 4, 704, 2816, 224, 191.2, 3.4081, 3.0591, 19600], # 1.5h\n",
    "[ 16, 8, 4, 704, 2816, 192, 182.6, 3.3957, 3.0584, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 256, 186.7, 3.3549, 3.0560, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 240, 182.5, 3.4249, 3.0599, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 224, 178.4, 3.3611,+3.0553, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 208, 174.3, 3.3694,+3.0556, 19600], # 1.4h\n",
    "[+16, 8, 4, 672, 2688, 192, 170.1, 3.3666,+3.0526, 19600], # 1.3h   minGemma-hidden_layers16-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-07-15-16-14.pth\n",
    "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3690, 3.0573, 19600], # again\n",
    "[ 16, 8, 4, 672, 2688, 176, 166.0, 3.3938, 3.0599, 19600], # 1.3h\n",
    "[ 16, 8, 4, 640, 2560, 256, 173.8, 3.3768, 3.0603, 19600], # 1.5h\n",
    "[ 16, 8, 4, 640, 2560, 224, 166.0, 3.3484, 3.0612, 19600], # 1.4h\n",
    "[ 16, 8, 4, 640, 2560, 192, 158.1, 3.3972, 3.0658, 19600], # 1.3h\n",
    "\n",
    "[ 16, 6, 3, 768, 3072, 288, 215.7, 3.5378, 3.0657, 19600], # 1.5h\n",
    "[ 16, 6, 3, 768, 3072, 240, 205.1, 3.5022,+3.0585, 19600], # 1.4h\n",
    "[ 16, 6, 3, 768, 3072, 192, 194.4, 3.4339, 3.0600, 19600],\n",
    "[ 16, 6, 3, 768, 3072, 144, 183.8, 3.4668, 3.0630, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 192, 166.6, 3.4332, 3.0602, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 144, 157.0, 3.4218, 3.0633, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 192, 124.6, 3.3380, 3.0774, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 144, 116.6, 3.3442, 3.0790, 19600],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2f3060a-cc17-40ac-9528-b250529bfdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L16 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:22:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.366600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3666, 3.0526, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "# 208,224,240,256\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0538:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13be91-8a0d-488f-88f0-80499e8981e2",
   "metadata": {},
   "source": [
    "# L14 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fea3bc-0a55-4509-9e59-b2c787e441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L14 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 14, 8, 4, 768, 3072, 240, 199.7, 3.4702, 3.0625, 20800],\n",
    "[ 14, 8, 4, 768, 3072, 192, 187.4, 3.4227, 3.0584, 20800],\n",
    "[ 14, 8, 4, 768, 3072, 144, 175.0, 3.3769, 3.0621, 20800], # 1.2h\n",
    "[ 14, 8, 4, 736, 2944, 192, 175.6, 3.3751, 3.0596, 20800], # 1.3h\n",
    "[ 14, 8, 4, 704, 2816, 240, 175.5, 3.4039, 3.0631, 20800],\n",
    "[ 14, 8, 4, 704, 2816, 216, 169.8, 3.3530, 3.0576, 20800], # 1.4h\n",
    "[ 14, 8, 4, 704, 3520, 192, 185.0, 3.3541, 3.0583, 20800], # 1.4h x5\n",
    "[+14, 8, 4, 704, 2816, 192, 164.2, 3.3670,+3.0553, 20800],\n",
    "[ 14, 8, 4, 704, 2816, 192, 164.2, 3.3564, 3.0626, 19600], # 1.2h\n",
    "[ 14, 8, 4, 704, 2816, 168, 158.5, 3.3300, 3.0610, 20800], # 1.2h\n",
    "[ 14, 8, 4, 704, 2816, 144, 152.8, 3.3884, 3.0668, 20800], # 1.2h\n",
    "[ 14, 8, 4, 672, 2688, 192, 153.1, 3.3537, 3.0588, 20800], # 1.3h\n",
    "[ 14, 8, 4, 640, 2560, 240, 152.7, 3.3119, 3.0687, 20800], # 1.3h\n",
    "[ 14, 8, 4, 640, 2560, 192, 142.4, 3.3184, 3.0596, 20800], # 1.2h\n",
    "[ 14, 8, 4, 640, 2560, 144, 132.0, 3.3049, 3.0674, 20800], # 1.1h\n",
    "\n",
    "# search N_steps\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3734, 3.0636, 21600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3264, 3.0617, 21200]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3558,+3.0599, 20800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3716, 3.0676, 20400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3671, 3.0644, 20000]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3863, 3.0621, 19800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4194, 3.0608, 19600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3920, 3.0630, 19400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3783, 3.0640, 19200]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4281, 3.0671, 18800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4434, 3.0638, 18400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4765, 3.0712, 18000]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.5028, 3.0658, 17600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4935, 3.0633, 17200]\n",
    "\n",
    "# search x4 ~ x8 for FFN\n",
    "[ 14, 6, 3, 768, 3072, 240, 184.3, 3.4371, 3.0605, 20800], # x4\n",
    "[ 14, 6, 3, 768, 3072, 192, 175.0, 3.4229, 3.0628, 20800], # x4\n",
    "[ 14, 6, 3, 696, 3480, 192, 170.5, 3.3813, 3.0639, 20800], # x5\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3558,+3.0599, 20800], # x4\n",
    "[ 14, 6, 3, 696, 4176, 192, 190.9, 3.4417, 3.0702, 19600], # x6\n",
    "[ 14, 6, 3, 696, 3480, 192, 170.5, 3.3835,+3.0595, 19600], # x5 # x4 is equally good:3.0599. So use x4 from now on.\n",
    "[ 14, 6, 3, 696, 4176, 144, 182.5, 3.4328, 3.0709, 19600], # x6\n",
    "[ 14, 6, 3, 696, 2784, 144, 141.7, 3.4093, 3.0697, 19600], # x4\n",
    "[ 14, 6, 3, 576, 2304, 192, 112.6, 3.3262, 3.0709, 19600], # x4\n",
    "[ 14, 6, 3, 576, 4608, 144, 161.5, 3.3518, 3.0762, 19600], # x8\n",
    "[ 14, 6, 3, 576, 4032, 144, 147.5, 3.3395, 3.0767, 19600], # x7\n",
    "[ 14, 6, 3, 576, 3456, 144, 133.6, 3.3417, 3.0783, 19600], # x6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df324c1-ab37-437a-b07c-365b0ff18076",
   "metadata": {},
   "source": [
    "# L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f2fb2-be6b-4955-988a-09f363470dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L12 Normal Model (default: B18800, B12, lr13.5e-4)\n",
    "\n",
    "[ 12, 16, 4, 768, 3072, 256, 218.0, 3.3989, 3.0601, 18800], # 1.8h 13.6G\n",
    "\n",
    "[ 12, 15, 3, 780, 3120, 256, 213.2, 3.4514, 3.0599, 18800], # 1.8h\n",
    "\n",
    "[ 12, 14, 7, 840, 3360, 256, 252.3, 3.5258, 3.0646, 18800], # 1.9h\n",
    "[ 12, 14, 7, 784, 3136, 256, 229.2, 3.4706, 3.0625, 18800], # 1.9h\n",
    "[ 12, 14, 7, 728, 2912, 256, 206.9, 3.4426, 3.0605, 18800], # 1.7h\n",
    "[ 12, 14, 7, 672, 2688, 256, 185.6, 3.4660, 3.0636, 18800], # 1.7h\n",
    "[ 12, 14, 7, 616, 2464, 256, 165.2, 3.3811, 3.0632, 18800], # 1.6h 12.3G\n",
    "\n",
    "[ 12, 14, 2, 840, 3360, 256, 226.5, 3.4694, 3.0690, 18800], # 1.8h\n",
    "[ 12, 14, 2, 784, 3136, 256, 205.1, 3.4314, 3.0661, 18800], # 1.7h\n",
    "[ 12, 14, 2, 728, 2912, 256, 184.6, 3.4126, 3.0678, 18800], # 1.6h\n",
    "[ 12, 14, 2, 672, 2688, 256, 165.0, 3.3625, 3.0679, 18800], # 1.5h 12.5G?\n",
    "[ 12, 14, 2, 616, 2464, 256, 146.2, 3.3818, 3.0681, 18800], # 1.5h\n",
    "\n",
    "[ 12, 12,12, 864, 3456, 288, 294.4, 3.8193, 3.0891, 18800], # 2.2h 15.3G\n",
    "[ 12, 12,12, 816, 3264, 320, 287.4, 3.6647, 3.0717, 18800], # 2.2h 15.4G\n",
    "[ 12, 12,12, 816, 3264, 256, 257.3, 3.6130, 3.0698, 18800], # 1.9h 14.0G\n",
    "[ 12, 12,12, 816, 3264, 224, 242.3, 3.5798, 3.0669, 18800], # 1.7h 13.5G\n",
    "[ 12, 12,12, 816, 3264, 192, 227.2, 3.5402, 3.0658, 18800], # 1.6h\n",
    "[ 12, 12,12, 792, 3168, 192, 217.8, 3.5099, 3.0645, 18800], # 1.6h\n",
    "[ 12, 12,12, 768, 3072, 256, 236.9, 3.5022, 3.0612, 18800], # 1.8h\n",
    "[ 12, 12,12, 768, 3072, 224, 222.7, 3.4811,+3.0594, 18800], # 1.6h\n",
    "[ 12, 12,12, 768, 3072, 208, 215.6, 3.4870, 3.0617, 18800], # 1.6h\n",
    "[ 12, 12,12, 768, 3072, 192, 208.6, 3.4734, 3.0629, 18800], # 1.5h 12.7G\n",
    "[ 12, 12,12, 768, 3072, 176, 201.5, 3.4626, 3.0616, 18800], # 1.4h\n",
    "[ 12, 12,12, 768, 3072, 160, 194.4, 3.5065, 3.0717, 18800], # 1.4h\n",
    "[ 12, 12,12, 744, 2976, 192, 199.5, 3.4640, 3.0611, 18800], # 1.5h\n",
    "[ 12, 12,12, 720, 2880, 192, 190.6, 3.5042, 3.0624, 18800], # 1.5h\n",
    "\n",
    "[ 12, 12, 6, 816, 3264, 320, 249.8, 3.5150, 3.0642, 18800], # 2.0h 14.1G\n",
    "[ 12, 12, 6, 816, 3264, 288, 238.5, 3.4971, 3.0610, 18800], # 1.9h\n",
    "[ 12, 12, 6, 816, 3264, 256, 227.2, 3.6264, 3.0686, 18800], # 1.7h\n",
    "[ 12, 12, 6, 816, 3264, 224, 216.0, 3.5363, 3.0637, 18800], # 1.6h\n",
    "[ 12, 12, 6, 816, 3264, 192, 204.7, 3.5030, 3.0616, 18800], # 1.5h\n",
    "[ 12, 12, 6, 792, 3168, 320, 239.7, 3.6240, 3.0704, 18800], # 2.0h 14.0G\n",
    "[ 12, 12, 6, 768, 3072, 352, 240.4, 3.4771, 3.0613, 18800], # 2.0h 14.1G\n",
    "[ 12, 12, 6, 768, 3072, 336, 235.1, 3.5655, 3.0702, 18800], # 2.0h\n",
    "[ 12, 12, 6, 768, 3072, 320, 229.8, 3.4336,+3.0565, 18800], # 1.9h\n",
    "[ 12, 12, 6, 768, 3072, 304, 224.5, 3.6113, 3.0675, 18800], # 1.8h\n",
    "[ 12, 12, 6, 768, 3072, 288, 219.2, 3.4753, 3.0656, 18800], # 1.8h\n",
    "[ 12, 12, 6, 768, 3072, 256, 208.6, 3.4828, 3.0636, 18800], # 1.6h\n",
    "[ 12, 12, 6, 768, 3072, 224, 198.0, 3.4475, 3.0636, 18800], # 1.5h\n",
    "[ 12, 12, 6, 768, 3072, 192, 187.3, 3.4762, 3.0637, 18800], # 1.4h\n",
    "[ 12, 12, 6, 744, 2976, 320, 220.0, 3.4917, 3.0592, 18800], # 1.9h\n",
    "[ 12, 12, 6, 720, 2880, 320, 210.5, 3.4434, 3.0638, 18800], # 1.8h\n",
    "[ 12, 12, 6, 720, 2880, 288, 200.5, 3.5165, 3.0599, 18800], # 1.7h\n",
    "[ 12, 12, 6, 720, 2880, 256, 190.6, 3.4140, 3.0597, 18800], # 1.5h\n",
    "[ 12, 12, 6, 720, 2880, 224, 180.6, 3.4428, 3.0681, 18800], # 1.4h\n",
    "[ 12, 12, 6, 720, 2880, 192, 170.7, 3.4995, 3.0635, 18800], # 1.3h\n",
    "[ 12, 12, 6, 672, 2688, 320, 191.8, 3.3924, 3.0637, 18800], # 1.8h\n",
    "[ 12, 12, 6, 672, 2688, 288, 182.5, 3.4011, 3.0659, 18800], # 1.7h\n",
    "[ 12, 12, 6, 672, 2688, 256, 173.2, 3.4060, 3.0620, 18800], # 1.5h\n",
    "[ 12, 12, 6, 672, 2688, 224, 163.9, 3.4040, 3.0677, 18800], # 1.4h\n",
    "[ 12, 12, 6, 672, 2688, 192, 154.6, 3.3942, 3.0626, 18800], # 1.3h\n",
    "\n",
    "[ 12, 12, 4, 912, 3648, 320, 277.8, 3.6521, 3.0822, 18800], # 2.1h 14.7G\n",
    "[ 12, 12, 4, 912, 3648, 288, 266.6, 3.6737, 3.0806, 18800], # 2.1h\n",
    "[ 12, 12, 4, 912, 3648, 256, 255.4, 3.6088, 3.0651, 18800], # 1.8h\n",
    "[ 12, 12, 4, 912, 3648, 224, 244.2, 3.5961, 3.0687, 18800], # 1.7h\n",
    "[ 12, 12, 4, 864, 3456, 320, 257.2, 3.4472, 3.0616, 18800], # 2.0h\n",
    "[ 12, 12, 4, 864, 3456, 288, 246.6, 3.5722, 3.0726, 18800], # 1.9h\n",
    "[ 12, 12, 4, 864, 3456, 256, 236.0, 3.5841, 3.0657, 18800], # 1.7h\n",
    "[ 12, 12, 4, 864, 3456, 224, 225.4, 3.5894, 3.0626, 18800], # 1.6h\n",
    "[ 12, 12, 4, 840, 3360, 320, 247.2, 3.7348, 3.0803, 18800], # 2.0h\n",
    "[ 12, 12, 4, 816, 3264, 352, 247.3, 3.5454, 3.0711, 18800], # 2.1h\n",
    "[ 12, 12, 4, 816, 3264, 336, 242.3, 3.5427, 3.0673, 18800], # 2.0h\n",
    "[ 12, 12, 4, 816, 3264, 320, 237.3, 3.4851,+3.0569, 18800], # 1.9h\n",
    "[ 12, 12, 4, 816, 3264, 304, 232.3, 3.5381, 3.0694, 18800], # 1.9h\n",
    "[ 12, 12, 4, 816, 3264, 288, 227.2, 3.5863, 3.0629, 18800], # 1.8h 13.8G\n",
    "[ 12, 12, 4, 816, 3264, 256, 217.2, 3.5497, 3.0617, 18800], # 1.6h\n",
    "[ 12, 12, 4, 816, 3264, 224, 207.2, 3.4825, 3.0582, 18800], # 1.5h\n",
    "[ 12, 12, 4, 816, 3264, 192, 197.2, 3.4755, 3.0646, 18800], # 1.4h 12.4G\n",
    "[ 12, 12, 4, 816, 3264, 160, 187.1, 3.4845, 3.0642, 18800], # 1.3h 12.1G\n",
    "[ 12, 12, 4, 816, 3264, 128, 177.1, 3.4870, 3.0616, 18800], # 1.0h\n",
    "[ 12, 12, 4, 792, 3168, 320, 227.6, 3.4834, 3.0616, 18800], # 1.9h\n",
    "[ 12, 12, 4, 792, 3168, 192, 188.6, 3.4551, 3.0590, 18800], # 1.4h\n",
    "[ 12, 12, 4, 768, 3072, 320, 218.0, 3.4480, 3.0599, 18800], # 1.8h\n",
    "[ 12, 12, 4, 768, 3072, 288, 208.6, 3.4913, 3.0626, 18800], # 1.7h\n",
    "[ 12, 12, 4, 768, 3072, 256, 199.1, 3.4792, 3.0663, 18800], # 1.5h\n",
    "[ 12, 12, 4, 768, 3072, 224, 189.7, 3.4769, 3.0628, 18800], # 1.4h\n",
    "[ 12, 12, 4, 768, 3072, 208, 185.0, 3.4560, 3.0631, 18800], # 1.4h\n",
    "[+12, 12, 4, 768, 3072, 192, 180.3, 3.4810,+3.0559, 18800], # 1.3h\n",
    "[ 12, 12, 4, 768, 3072, 176, 175.5, 3.4378, 3.0633, 18800], # 1.3h\n",
    "[ 12, 12, 4, 768, 3072, 160, 170.8, 3.4852, 3.0688, 18800], # 1.2h\n",
    "[ 12, 12, 4, 768, 3072, 128, 161.4, 3.4619, 3.0642, 18800], # 0.9h\n",
    "[ 12, 12, 4, 768, 3072,  96, 151.9, 3.4861, 3.0712, 18800], # 0.8h\n",
    "[ 12, 12, 4, 768, 3072,  64, 142.5, 3.4848, 3.0828, 18800], # 0.7h\n",
    "[ 12, 12, 4, 744, 2976, 192, 172.1, 3.4371, 3.0646, 18800], # 1.3h\n",
    "[ 12, 12, 4, 720, 2880, 288, 190.6, 3.4115, 3.0561, 18800], # 1.7h\n",
    "[ 12, 12, 4, 720, 2880, 256, 181.7, 3.4442, 3.0631, 18800], # 1.5h\n",
    "[ 12, 12, 4, 720, 2880, 224, 172.9, 3.4431, 3.0591, 18800], # 1.4h\n",
    "[ 12, 12, 4, 720, 2880, 192, 164.0, 3.4240, 3.0636, 18800], # 1.3h\n",
    "[ 12, 12, 4, 720, 2880, 160, 155.2, 3.5028, 3.0646, 18800], # 1.2h\n",
    "[ 12, 12, 4, 720, 2880, 128, 146.3, 3.4307, 3.0665, 18800], # 0.8h\n",
    "[ 12, 12, 4, 720, 2880,  96, 137.5, 3.4292, 3.0705, 18800], # 0.7h\n",
    "[ 12, 12, 4, 720, 2880,  64, 128.6, 3.4500, 3.0850, 18800], # 0.7h\n",
    "[ 12, 12, 4, 672, 2688, 288, 173.2, 3.3863, 3.0648, 18800], # 1.6h\n",
    "[ 12, 12, 4, 672, 2688, 256, 165.0, 3.3917, 3.0625, 18800], # 1.4h\n",
    "[ 12, 12, 4, 672, 2688, 224, 156.7, 3.4182, 3.0626, 18800], # 1.4h\n",
    "[ 12, 12, 4, 624, 2496, 256, 148.9, 3.3509, 3.0701, 18800], # 1.4h\n",
    "\n",
    "[ 12, 12, 3, 816, 3264, 320, 231.0, 3.5877, 3.0664, 18800], # 1.9h\n",
    "[ 12, 12, 3, 816, 3264, 256, 212.2, 3.5403, 3.0678, 18800], # 1.6h\n",
    "[ 12, 12, 3, 816, 3264, 224, 202.8, 3.4625, 3.0656, 18800], # 1.5h 12.6G\n",
    "[ 12, 12, 3, 816, 3264, 192, 193.4, 3.4989, 3.0616, 18800], # 1.4h\n",
    "[ 12, 12, 3, 792, 3168, 256, 203.2, 3.5969, 3.0726, 18800], # 1.6h\n",
    "[ 12, 12, 3, 768, 3072, 288, 203.3, 3.4027, 3.0665, 18800], # 1.7h\n",
    "[ 12, 12, 3, 768, 3072, 272, 198.8, 3.3851,+3.0592, 18800], # 1.7h\n",
    "[ 12, 12, 3, 768, 3072, 256, 194.4, 3.4539,+3.0605, 18800], # 1.5h\n",
    "[ 12, 12, 3, 768, 3072, 240, 190.0, 3.4223, 3.0651, 18800], # 1.5h\n",
    "[ 12, 12, 3, 768, 3072, 224, 185.6, 3.4472, 3.0652, 18800], # 1.4h\n",
    "[ 12, 12, 3, 768, 3072, 208, 181.1, 3.4370, 3.0656, 18800], # 1.4h\n",
    "[ 12, 12, 3, 768, 3072, 192, 176.7, 3.4445,+3.0605, 18800], # 1.3h\n",
    "[ 12, 12, 3, 768, 3072, 176, 172.3, 3.4425, 3.0657, 18800], # 1.3h\n",
    "[ 12, 12, 3, 768, 3072, 160, 167.9, 3.4345, 3.0644, 18800], # 1.2h\n",
    "[ 12, 12, 3, 720, 2880, 288, 185.6, 3.4034, 3.0632, 18800], # 1.7h\n",
    "[ 12, 12, 3, 720, 2880, 272, 181.4, 3.4034, 3.0623, 18800], # 1.6h\n",
    "[ 12, 12, 3, 720, 2880, 256, 177.3, 3.4204, 3.0620, 18800], # 1.5h\n",
    "[ 12, 12, 3, 720, 2880, 240, 173.1, 3.4321, 3.0678, 18800], # 1.4h\n",
    "[ 12, 12, 3, 720, 2880, 224, 169.0, 3.4098, 3.0689, 18800], # 1.4h\n",
    "[ 12, 12, 3, 720, 2880, 208, 164.8, 3.4699, 3.0619, 18800], # 1.3h\n",
    "[ 12, 12, 3, 720, 2880, 192, 160.7, 3.4051, 3.0655, 18800], # 1.3h\n",
    "[ 12, 12, 3, 720, 2880, 176, 156.5, 3.4429, 3.0609, 18800], # 1.2h\n",
    "[ 12, 12, 3, 720, 2880, 160, 152.4, 3.4350, 3.0669, 18800], # 1.2h\n",
    "[ 12, 12, 3, 672, 2688, 288, 168.6, 3.3750, 3.0597, 18800], # 1.6h\n",
    "[ 12, 12, 3, 672, 2688, 272, 164.7, 3.5514, 3.0751, 18800], # 1.6h\n",
    "[ 12, 12, 3, 672, 2688, 256, 160.8, 3.4028, 3.0637, 18800], # 1.4h\n",
    "[ 12, 12, 3, 672, 2688, 240, 157.0, 3.4181, 3.0695, 18800], # 1.4h\n",
    "[ 12, 12, 3, 672, 2688, 224, 153.1, 3.4004, 3.0686, 18800], # 1.3h\n",
    "[ 12, 12, 3, 672, 2688, 208, 149.2, 3.4014, 3.0704, 18800], # 1.3h\n",
    "[ 12, 12, 3, 672, 2688, 192, 145.3, 3.3984, 3.0695, 18800], # 1.2h\n",
    "[ 12, 12, 3, 672, 2688, 176, 141.5, 3.4039, 3.0636, 18800], # 1.2h\n",
    "[ 12, 12, 3, 672, 2688, 160, 137.6, 3.3805, 3.0643, 18800], # 1.1h\n",
    "[ 12, 12, 3, 624, 2496, 288, 152.2, 3.3894, 3.0682, 18800], # 1.6h\n",
    "[ 12, 12, 3, 624, 2496, 272, 148.6, 3.3643, 3.0726, 18800], # 1.5h\n",
    "[ 12, 12, 3, 624, 2496, 256, 145.0, 3.3676, 3.0699, 18800], # 1.4h\n",
    "[ 12, 12, 3, 624, 2496, 240, 141.4, 3.3620, 3.0666, 18800], # 1.3h\n",
    "[ 12, 12, 3, 624, 2496, 224, 137.8, 3.3597, 3.0728, 18800], # 1.3h\n",
    "[ 12, 12, 3, 624, 2496, 208, 134.2, 3.3652, 3.0689, 18800], # 1.2h\n",
    "[ 12, 12, 3, 624, 2496, 192, 130.6, 3.3477, 3.0757, 18800], # 1.2h\n",
    "[ 12, 12, 3, 624, 2496, 176, 127.1, 3.4069, 3.0716, 18800], # 1.1h\n",
    "[ 12, 12, 3, 624, 2496, 160, 123.5, 3.3684, 3.0709, 18800], # 1.1h\n",
    "\n",
    "[ 12, 12, 2, 864, 3456, 224, 216.1, 3.5063, 3.0710, 18800], # 1.6h 13.0G\n",
    "[ 12, 12, 2, 840, 3360, 224, 207.2, 3.4873, 3.0696, 18800], # 1.5h\n",
    "[ 12, 12, 2, 816, 3264, 320, 224.7, 3.4414, 3.0662, 18800], # 1.9h\n",
    "[ 12, 12, 2, 816, 3264, 256, 207.2, 3.4975, 3.0676, 18800], # 1.6h\n",
    "[ 12, 12, 2, 816, 3264, 240, 202.8, 3.5574, 3.0739, 18800], # 1.6h\n",
    "[ 12, 12, 2, 816, 3264, 224, 198.4, 3.4658, 3.0619, 18800], # 1.5h\n",
    "[ 12, 12, 2, 816, 3264, 208, 194.0, 3.5214, 3.0668, 18800], # 1.5h\n",
    "[ 12, 12, 2, 816, 3264, 192, 189.6, 3.4978, 3.0702, 18800], # 1.4h\n",
    "[ 12, 12, 2, 816, 3264, 176, 185.3, 3.4539, 3.0638, 18800], # 1.3h\n",
    "[ 12, 12, 2, 792, 3168, 224, 189.8, 3.4532, 3.0678, 18800], # 1.5h\n",
    "[ 12, 12, 2, 768, 3072, 256, 189.7, 3.4818, 3.0643, 18800], # 1.5h\n",
    "[ 12, 12, 2, 768, 3072, 240, 185.6, 3.4448, 3.0655, 18800], # 1.4h\n",
    "[ 12, 12, 2, 768, 3072, 224, 181.4, 3.4545, 3.0669, 18800], # 1.4h\n",
    "[ 12, 12, 2, 768, 3072, 208, 177.3, 3.4458,+3.0612, 18800], # 1.4h\n",
    "[ 12, 12, 2, 768, 3072, 192, 173.2, 3.4525, 3.0678, 18800], # 1.3h\n",
    "[ 12, 12, 2, 768, 3072, 176, 169.1, 3.4245, 3.0660, 18800], # 1.2h\n",
    "[ 12, 12, 2, 720, 2880, 256, 172.9, 3.3707, 3.0643, 18800], # 1.4h\n",
    "[ 12, 12, 2, 720, 2880, 240, 169.0, 3.4050, 3.0639, 18800], # 1.4h\n",
    "[ 12, 12, 2, 720, 2880, 224, 165.1, 3.4331, 3.0619, 18800], # 1.4h\n",
    "[ 12, 12, 2, 720, 2880, 208, 161.2, 3.4166, 3.0660, 18800], # 1.3h\n",
    "[ 12, 12, 2, 720, 2880, 192, 157.4, 3.4238, 3.0729, 18800], # 1.2h\n",
    "[ 12, 12, 2, 720, 2880, 176, 153.5, 3.3972, 3.0641, 18800], # 1.2h\n",
    "[ 12, 12, 2, 672, 2688, 256, 156.7, 3.4097, 3.0633, 18800], # 1.4h\n",
    "[ 12, 12, 2, 672, 2688, 240, 153.1, 3.3999, 3.0759, 18800], # 1.4h\n",
    "[ 12, 12, 2, 672, 2688, 224, 149.5, 3.3898, 3.0643, 18800], # 1.3h\n",
    "[ 12, 12, 2, 672, 2688, 208, 145.9, 3.4072, 3.0668, 18800], # 1.3h\n",
    "[ 12, 12, 2, 672, 2688, 192, 142.2, 3.3892, 3.0699, 18800], # 1.2h\n",
    "[ 12, 12, 2, 672, 2688, 176, 138.6, 3.3833, 3.0647, 18800], # 1.2h\n",
    "[ 12, 12, 2, 624, 2496, 256, 141.2, 3.3552, 3.0680, 18800], # 1.3h\n",
    "[ 12, 12, 2, 624, 2496, 240, 137.8, 3.3756, 3.0648, 18800], # 1.3h\n",
    "[ 12, 12, 2, 624, 2496, 224, 134.5, 3.3704, 3.0715, 18800], # 1.3h\n",
    "[ 12, 12, 2, 624, 2496, 208, 131.1, 3.3771, 3.0747, 18800], # 1.2h\n",
    "[ 12, 12, 2, 624, 2496, 192, 127.8, 3.3746, 3.0719, 18800], # 1.2h\n",
    "[ 12, 12, 2, 624, 2496, 176, 124.4, 3.3604, 3.0745, 18800], # 1.1h\n",
    "\n",
    "[ 12, 12, 1, 816, 3264, 320, 218.5, 3.4646, 3.0745, 18800], # 1.9h\n",
    "[ 12, 12, 1, 816, 3264, 224, 194.0, 3.4538, 3.0709, 18800], # 1.5h\n",
    "[ 12, 12, 1, 768, 3072, 192, 169.6, 3.4169, 3.0679, 18800], # 1.3h\n",
    "\n",
    "[ 12, 11,11, 726, 2904, 320, 235.1, 3.4779, 3.0620, 18800], # 1.9h\n",
    "[ 12, 11,11, 726, 2904, 288, 222.9, 3.4881, 3.0636, 18800], # 1.8h\n",
    "[ 12, 11,11, 726, 2904, 256, 210.6, 3.4603, 3.0619, 18800], # 1.6h\n",
    "[ 12, 11,11, 726, 2904, 224, 198.3, 3.4874, 3.0622, 18800], # 1.5h\n",
    "[ 12, 11,11, 726, 2904, 192, 186.1, 3.4568, 3.0581, 18800], # 1.4h\n",
    "\n",
    "[ 12, 11, 1, 726, 2904, 320, 179.4, 3.4207, 3.0597, 18800], # 1.6h\n",
    "[ 12, 11, 1, 726, 2904, 288, 172.7, 3.3752, 3.0666, 18800], # 1.5h\n",
    "[ 12, 11, 1, 726, 2904, 256, 166.0, 3.4465, 3.0626, 18800], # 1.4h\n",
    "[ 12, 11, 1, 726, 2904, 224, 159.3, 3.3959, 3.0649, 18800], # 1.3h\n",
    "[ 12, 11, 1, 726, 2904, 192, 152.6, 3.3867, 3.0703, 18800], # 1.2h\n",
    "\n",
    "\n",
    "[ 12, 10,10, 840, 3360, 256, 247.2, 3.7332, 3.0742, 18800], # 1.7h\n",
    "[ 12, 10,10, 800, 3200, 256, 230.8, 3.6421, 3.0684, 18800], # 1.6h\n",
    "[ 12, 10,10, 760, 3040, 256, 214.9, 3.4856, 3.0651, 18800], # 1.5h\n",
    "[ 12, 10,10, 720, 2880, 256, 199.4, 3.4637, 3.0606, 18800], # 1.5h\n",
    "[ 12, 10,10, 680, 2720, 256, 184.4, 3.4006, 3.0654, 18800], # 1.5h\n",
    "[ 12, 10,10, 640, 2560, 256, 169.9, 3.4217, 3.0667, 18800], # 1.4h\n",
    "\n",
    "[ 12, 10, 5, 840, 3360, 320, 240.7, 3.6001, 3.0743, 18800], # 1.8h\n",
    "[ 12, 10, 5, 840, 3360, 288, 231.0, 3.6040, 3.0652, 18800], # 1.7h\n",
    "[ 12, 10, 5, 840, 3360, 256, 221.3, 3.5737, 3.0646, 18800], # 1.5h\n",
    "[ 12, 10, 5, 840, 3360, 224, 211.7, 3.5232, 3.0679, 18800], # 1.5h\n",
    "[ 12, 10, 5, 840, 3360, 208, 206.8, 3.5132, 3.0611, 18800], # 1.4h\n",
    "[ 12, 10, 5, 840, 3360, 192, 202.0, 3.5081, 3.0617, 18800], # 1.3h\n",
    "[ 12, 10, 5, 840, 3360, 176, 197.2, 3.5270, 3.0596, 18800], # 1.3h\n",
    "[ 12, 10, 5, 840, 3360, 160, 192.3, 3.5607, 3.0679, 18800], # 1.3h\n",
    "\n",
    "[ 12, 10, 5, 800, 3200, 320, 224.6, 3.5620, 3.0613, 18800], # 1.8h\n",
    "[ 12, 10, 5, 800, 3200, 304, 220.0, 3.5099, 3.0654, 18800], # 1.7h\n",
    "[ 12, 10, 5, 800, 3200, 288, 215.4, 3.5230, 3.0634, 18800], # 1.7h\n",
    "[ 12, 10, 5, 800, 3200, 272, 210.8, 3.5523, 3.0602, 18800], # 1.6h\n",
    "[ 12, 10, 5, 800, 3200, 256, 206.2, 3.4958, 3.0613, 18800], # 1.5h\n",
    "[ 12, 10, 5, 800, 3200, 240, 201.6, 3.5599, 3.0661, 18800], # 1.4h\n",
    "[ 12, 10, 5, 800, 3200, 224, 197.0, 3.5175, 3.0644, 18800], # 1.4h\n",
    "[ 12, 10, 5, 800, 3200, 208, 192.4, 3.5221, 3.0611, 18800], # 1.4h\n",
    "[ 12, 10, 5, 800, 3200, 192, 187.8, 3.4884, 3.0696, 18800], # 1.3h\n",
    "[ 12, 10, 5, 800, 3200, 176, 183.2, 3.4876, 3.0643, 18800], # 1.3h\n",
    "[ 12, 10, 5, 800, 3200, 160, 178.6, 3.4899, 3.0614, 18800], # 1.2h\n",
    "\n",
    "[ 12, 10, 5, 760, 3040, 320, 209.0, 3.4996, 3.0654, 18800], # 1.6h\n",
    "[ 12, 10, 5, 760, 3040, 304, 204.6, 3.4638, 3.0626, 18800], # 1.6h\n",
    "[ 12, 10, 5, 760, 3040, 288, 200.3, 3.5392, 3.0591, 18800], # 1.6h\n",
    "[ 12, 10, 5, 760, 3040, 272, 195.9, 3.4761, 3.0661, 18800], # 1.5h\n",
    "[ 12, 10, 5, 760, 3040, 256, 191.5, 3.5185, 3.0673, 18800], # 1.4h\n",
    "[ 12, 10, 5, 760, 3040, 240, 187.1, 3.4806, 3.0642, 18800], # 1.3h\n",
    "[ 12, 10, 5, 760, 3040, 224, 182.8, 3.4477,+3.0562, 18800], # 1.3h\n",
    "[ 12, 10, 5, 760, 3040, 224, 182.8, 3.4549, 3.0605, 18800], # again\n",
    "[ 12, 10, 5, 760, 3040, 224, 182.8, 3.4534, 3.0591, 18800], # again\n",
    "[ 12, 10, 5, 760, 3040, 208, 178.4, 3.5351, 3.0618, 18800], # 1.3h\n",
    "[ 12, 10, 5, 760, 3040, 192, 174.0, 3.4557, 3.0668, 18800], # 1.2h\n",
    "[ 12, 10, 5, 760, 3040, 176, 169.6, 3.4609, 3.0597, 18800], # 1.2h\n",
    "[ 12, 10, 5, 760, 3040, 160, 165.2, 3.4688, 3.0658, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 5, 740, 2960, 320, 201.4, 3.4450, 3.0593, 18800], # 1.6h\n",
    "[ 12, 10, 5, 740, 2960, 304, 197.1, 3.4709, 3.0608, 18800], # 1.6h\n",
    "[ 12, 10, 5, 740, 2960, 288, 192.9, 3.4963, 3.0628, 18800], # 1.6h\n",
    "[ 12, 10, 5, 740, 2960, 272, 188.6, 3.4389, 3.0611, 18800], # 1.5h\n",
    "[ 12, 10, 5, 740, 2960, 256, 184.3, 3.4365, 3.0639, 18800], # 1.4h\n",
    "[ 12, 10, 5, 740, 2960, 240, 180.1, 3.4443, 3.0600, 18800], # 1.3h\n",
    "[ 12, 10, 5, 740, 2960, 224, 175.8, 3.4061, 3.0624, 18800], # 1.3h\n",
    "[ 12, 10, 5, 740, 2960, 208, 171.6, 3.4438, 3.0589, 18800], # 1.3h\n",
    "[ 12, 10, 5, 740, 2960, 176, 163.0, 3.4245, 3.0625, 18800], # 1.2h\n",
    "[ 12, 10, 5, 740, 2960, 192, 167.3, 3.4438, 3.0633, 18800], # 1.2h\n",
    "[ 12, 10, 5, 740, 2960, 160, 158.8, 3.4279, 3.0630, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 5, 720, 2880, 320, 193.9, 3.4730, 3.0625, 18800], # 1.6h\n",
    "[ 12, 10, 5, 720, 2880, 304, 189.7, 3.4184, 3.0582, 18800], # 1.6h\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.3697, 3.0603, 21200], # 1.7h\n",
    "[+12, 10, 5, 720, 2880, 288, 185.6, 3.4425,+3.0543, 18800], # 1.5h    BEST (not saved)\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4394, 3.0587, 18800], # again\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4671, 3.0598, 18800], # again\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4291, 3.0582, 18800], # again\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4471, 3.0627, 18800], # again\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4834, 3.0608, 18800], # again\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4388, 3.0648, 19600], # 1.6h again with larger N\n",
    "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4278, 3.0590, 20400], # 1.7h again with larger N\n",
    "[ 12, 10, 5, 720, 2880, 272, 181.4, 3.4303, 3.0633, 18800], # 1.5h\n",
    "[ 12, 10, 5, 720, 2880, 256, 177.3, 3.4436, 3.0586, 18800], # 1.4h\n",
    "[ 12, 10, 5, 720, 2880, 240, 173.1, 3.4581, 3.0631, 18800], # 1.3h\n",
    "[ 12, 10, 5, 720, 2880, 224, 169.0, 3.4911, 3.0603, 18800], # 1.3h\n",
    "[ 12, 10, 5, 720, 2880, 208, 164.8, 3.5196, 3.0664, 18800], # 1.2h\n",
    "[ 12, 10, 5, 720, 2880, 192, 160.7, 3.4483, 3.0648, 18800], # 1.2h\n",
    "[ 12, 10, 5, 720, 2880, 176, 156.5, 3.4512, 3.0613, 18800], # 1.1h\n",
    "[ 12, 10, 5, 720, 2880, 160, 152.4, 3.4276, 3.0617, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 5, 700, 2800, 320, 186.5, 3.4251, 3.0654, 18800], # 1.6h\n",
    "[ 12, 10, 5, 700, 2800, 304, 182.4, 3.5124, 3.0646, 18800], # 1.6h\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4566, 3.0613, 20400], # 1.6h again with larger N\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4121, 3.0628, 19600], # 1.6h again with larger N\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4503,+3.0552, 18800], # 1.5h\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4392, 3.0680, 18800], # again\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4096, 3.0589, 18800], # again\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4204, 3.0581, 18800], # again\n",
    "[ 12, 10, 5, 700, 2800, 288, 178.4, 3.4286, 3.0576, 18800], # again\n",
    "[ 12, 10, 5, 700, 2800, 272, 174.4, 3.4338, 3.0628, 18800], # 1.5h\n",
    "[ 12, 10, 5, 700, 2800, 256, 170.3, 3.3975, 3.0630, 18800], # 1.3h\n",
    "[ 12, 10, 5, 700, 2800, 240, 166.3, 3.4058, 3.0657, 18800], # 1.3h\n",
    "[ 12, 10, 5, 700, 2800, 224, 162.3, 3.4764, 3.0637, 18800], # 1.3h\n",
    "[ 12, 10, 5, 700, 2800, 208, 158.2, 3.4006, 3.0624, 18800], # 1.2h\n",
    "[ 12, 10, 5, 700, 2800, 192, 154.2, 3.3874, 3.0636, 18800], # 1.1h\n",
    "[ 12, 10, 5, 700, 2800, 176, 150.2, 3.3668, 3.0634, 18800], # 1.1h\n",
    "[ 12, 10, 5, 700, 2800, 160, 146.2, 3.4194, 3.0629, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 5, 680, 2720, 320, 179.2, 3.4239, 3.0626, 18800], # 1.6h\n",
    "[ 12, 10, 5, 680, 2720, 304, 175.3, 3.3970, 3.0598, 18800], # 1.5h\n",
    "[ 12, 10, 5, 680, 2720, 288, 171.4, 3.4888, 3.0645, 18800], # 1.5h\n",
    "[ 12, 10, 5, 680, 2720, 272, 167.4, 3.4061, 3.0621, 18800], # 1.4h\n",
    "[ 12, 10, 5, 680, 2720, 256, 163.5, 3.4194, 3.0615, 18800], # 1.3h\n",
    "[ 12, 10, 5, 680, 2720, 240, 159.6, 3.4476,+3.0586, 18800], # 1.3h\n",
    "[ 12, 10, 5, 680, 2720, 224, 155.7, 3.4044, 3.0598, 18800], # 1.2h\n",
    "[ 12, 10, 5, 680, 2720, 208, 151.8, 3.3949, 3.0644, 18800], # 1.2h\n",
    "[ 12, 10, 5, 680, 2720, 192, 147.9, 3.4600, 3.0632, 18800], # 1.1h\n",
    "[ 12, 10, 5, 680, 2720, 176, 143.9, 3.4647, 3.0610, 18800], # 1.1h\n",
    "[ 12, 10, 5, 680, 2720, 160, 140.0, 3.4039, 3.0668, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 5, 640, 2560, 320, 165.0, 3.3946, 3.0665, 18800], # 1.5h\n",
    "[ 12, 10, 5, 640, 2560, 304, 161.3, 3.4033, 3.0659, 18800], # 1.5h\n",
    "[ 12, 10, 5, 640, 2560, 288, 157.6, 3.4057, 3.0608, 18800], # 1.4h\n",
    "[ 12, 10, 5, 640, 2560, 272, 153.9, 3.4425, 3.0619, 18800], # 1.4h\n",
    "[ 12, 10, 5, 640, 2560, 256, 150.2, 3.3913, 3.0598, 18800], # 1.3h \n",
    "[ 12, 10, 5, 640, 2560, 240, 146.5, 3.3940, 3.0631, 18800], # 1.2h\n",
    "[ 12, 10, 5, 640, 2560, 224, 142.8, 3.3927, 3.0651, 18800], # 1.2h\n",
    "[ 12, 10, 5, 640, 2560, 208, 139.2, 3.3883, 3.0695, 18800], # 1.1h\n",
    "[ 12, 10, 5, 640, 2560, 192, 135.5, 3.3707, 3.0628, 18800], # 1.1h\n",
    "[ 12, 10, 5, 640, 2560, 176, 131.8, 3.3907, 3.0687, 18800], # 1.0h\n",
    "[ 12, 10, 5, 640, 2560, 160, 128.1, 3.3787, 3.0697, 18800], # 1.0h\n",
    "\n",
    "[ 12, 10, 2, 840, 3360, 288, 213.6, 3.5550, 3.0648, 18800], # 1.6h\n",
    "[ 12, 10, 2, 840, 3360, 256, 205.9, 3.4644, 3.0666, 18800], # 1.5h\n",
    "[ 12, 10, 2, 840, 3360, 224, 198.1, 3.5144, 3.0600, 18800], # 1.4h\n",
    "[ 12, 10, 2, 800, 3200, 288, 198.8, 3.4607, 3.0593, 18800], # 1.6h\n",
    "[ 12, 10, 2, 800, 3200, 256, 191.5, 3.4746, 3.0607, 18800], # 1.4h\n",
    "[ 12, 10, 2, 800, 3200, 224, 184.1, 3.4603, 3.0631, 18800], # 1.3h\n",
    "[ 12, 10, 2, 760, 3040, 288, 184.5, 3.4126, 3.0619, 18800], # 1.5h\n",
    "[ 12, 10, 2, 760, 3040, 272, 181.0, 3.4455, 3.0631, 18800], # 1.4h\n",
    "[ 12, 10, 2, 760, 3040, 256, 177.5, 3.4365,+3.0585, 18800], # 1.3h\n",
    "[ 12, 10, 2, 760, 3040, 240, 174.0, 3.4319, 3.0705, 18800], # 1.3h\n",
    "[ 12, 10, 2, 760, 3040, 224, 170.5, 3.4363, 3.0620, 18800], # 1.2h\n",
    "[ 12, 10, 2, 760, 3040, 208, 167.0, 3.5392, 3.0672, 18800], # 1.2h\n",
    "[ 12, 10, 2, 720, 2880, 288, 170.7, 3.3995, 3.0626, 18800], # 1.4h\n",
    "[ 12, 10, 2, 720, 2880, 272, 167.3, 3.4345, 3.0637, 18800], # 1.4h\n",
    "[ 12, 10, 2, 720, 2880, 256, 164.0, 3.4099, 3.0619, 18800], # 1.3h\n",
    "[ 12, 10, 2, 720, 2880, 240, 160.7, 3.4206, 3.0640, 18800], # 1.3h\n",
    "[ 12, 10, 2, 720, 2880, 224, 157.4, 3.4171, 3.0671, 18800], # 1.2h\n",
    "[ 12, 10, 2, 720, 2880, 208, 154.1, 3.4047, 3.0677, 18800], # 1.2h\n",
    "[ 12, 10, 2, 680, 2720, 288, 157.3, 3.4030, 3.0623, 18800], # 1.4h\n",
    "[ 12, 10, 2, 680, 2720, 272, 154.1, 3.3888, 3.0676, 18800], # 1.4h\n",
    "[ 12, 10, 2, 680, 2720, 256, 151.0, 3.3980, 3.0652, 18800], # 1.3h\n",
    "[ 12, 10, 2, 680, 2720, 240, 147.9, 3.3794, 3.0726, 18800], # 1.2h\n",
    "[ 12, 10, 2, 680, 2720, 224, 144.7, 3.4080, 3.0702, 18800], # 1.2h\n",
    "[ 12, 10, 2, 680, 2720, 208, 141.6, 3.3961, 3.0664, 18800], # 1.1h\n",
    "[ 12, 10, 2, 640, 2560, 288, 144.3, 3.3958, 3.0634, 18800], # 1.3h\n",
    "[ 12, 10, 2, 640, 2560, 272, 141.4, 3.4113, 3.0652, 18800], # 1.3h\n",
    "[ 12, 10, 2, 640, 2560, 256, 138.4, 3.3983, 3.0648, 18800], # 1.2h\n",
    "[ 12, 10, 2, 640, 2560, 240, 135.5, 3.3833, 3.0683, 18800], # 1.2h\n",
    "[ 12, 10, 2, 640, 2560, 224, 132.5, 3.3589, 3.0721, 18800], # 1.1h\n",
    "[ 12, 10, 2, 640, 2560, 208, 129.6, 3.3808, 3.0676, 18800], # 1.1h\n",
    "\n",
    "[ 12, 10, 1, 840, 3360, 256, 200.7, 3.5330, 3.0673, 18800], # 1.4h\n",
    "[ 12, 10, 1, 800, 3200, 256, 186.5, 3.4692, 3.0605, 18800], # 1.4h\n",
    "[ 12, 10, 1, 760, 3040, 256, 172.8, 3.4150, 3.0654, 18800], # 1.3h\n",
    "[ 12, 10, 1, 720, 2880, 256, 159.6, 3.4646, 3.0718, 18800], # 1.3h\n",
    "[ 12, 10, 1, 680, 2720, 256, 146.8, 3.3897, 3.0707, 18800], # 1.2h\n",
    "[ 12, 10, 1, 640, 2560, 256, 134.5, 3.3742, 3.0662, 18800], # 1.2h\n",
    "\n",
    "\n",
    "[ 12, 9, 3, 828, 3312, 320, 216.8, 3.5691, 3.0688, 18800], # 1.6h\n",
    "[ 12, 9, 3, 828, 3312, 304, 212.9, 3.5873, 3.0721, 18800], # 1.6h\n",
    "[ 12, 9, 3, 828, 3312, 288, 209.1, 3.6156, 3.0656, 18800], # 1.6h\n",
    "[ 12, 9, 3, 828, 3312, 272, 205.3, 3.5256, 3.0667, 18800], # 1.5h\n",
    "[ 12, 9, 3, 828, 3312, 256, 201.5, 3.5053, 3.0609, 18800], # 1.4h\n",
    "[ 12, 9, 3, 828, 3312, 240, 197.7, 3.5004, 3.0683, 18800], # 1.4h\n",
    "[ 12, 9, 3, 828, 3312, 224, 193.9, 3.5324, 3.0670, 18800], # 1.4h\n",
    "[ 12, 9, 3, 828, 3312, 208, 190.0, 3.5041, 3.0657, 18800], # 1.3h\n",
    "[ 12, 9, 3, 828, 3312, 192, 186.2, 3.5127, 3.0639, 18800], # 1.3h\n",
    "[ 12, 9, 3, 828, 3312, 176, 182.4, 3.5192, 3.0628, 18800], # 1.2h\n",
    "\n",
    "[ 12, 9, 3, 792, 3168, 320, 203.2, 3.5038, 3.0634, 18800], # 1.6h\n",
    "[ 12, 9, 3, 792, 3168, 304, 199.6, 3.4986, 3.0625, 18800], # 1.6h\n",
    "[ 12, 9, 3, 792, 3168, 288, 195.9, 3.4826, 3.0641, 18800], # 1.5h\n",
    "[ 12, 9, 3, 792, 3168, 272, 192.3, 3.4782, 3.0614, 18800], # 1.5h\n",
    "[ 12, 9, 3, 792, 3168, 256, 188.6, 3.4595, 3.0591, 18800], # 1.4h\n",
    "[ 12, 9, 3, 792, 3168, 240, 185.0, 3.5006, 3.0653, 18800], # 1.3h\n",
    "[ 12, 9, 3, 792, 3168, 224, 181.3, 3.4544, 3.0641, 18800], # 1.3h\n",
    "[ 12, 9, 3, 792, 3168, 208, 177.7, 3.4977, 3.0715, 18800], # 1.3h\n",
    "[ 12, 9, 3, 792, 3168, 192, 174.0, 3.4760, 3.0632, 18800], # 1.2h\n",
    "[ 12, 9, 3, 792, 3168, 176, 170.4, 3.4878, 3.0674, 18800], # 1.2h\n",
    "\n",
    "[ 12, 9, 3, 774, 3096, 320, 196.6, 3.4755, 3.0637, 18800], # 1.6h\n",
    "[ 12, 9, 3, 774, 3096, 304, 193.0, 3.4842, 3.0617, 18800], # 1.6h\n",
    "[ 12, 9, 3, 774, 3096, 288, 189.5, 3.5492, 3.0642, 18800], # 1.5h 12.4G\n",
    "[ 12, 9, 3, 774, 3096, 272, 185.9, 3.4738,+3.0582, 18800], # 1.5h\n",
    "[ 12, 9, 3, 774, 3096, 256, 182.3, 3.4671, 3.0595, 18800], # 1.4h\n",
    "[ 12, 9, 3, 774, 3096, 240, 178.8, 3.5141, 3.0607, 18800], # 1.4h\n",
    "[ 12, 9, 3, 774, 3096, 224, 175.2, 3.4376, 3.0610, 18800], # 1.3h\n",
    "[ 12, 9, 3, 774, 3096, 208, 171.6, 3.4674, 3.0604, 18800], # 1.3h\n",
    "[ 12, 9, 3, 774, 3096, 192, 168.1, 3.4507, 3.0598, 18800], # 1.2h 11.8G\n",
    "[ 12, 9, 3, 774, 3096, 176, 164.5, 3.4933, 3.0686, 18800], # 1.2h\n",
    "\n",
    "[ 12, 9, 3, 756, 3024, 320, 190.1, 3.5012, 3.0633, 18800], # 1.5h\n",
    "[ 12, 9, 3, 756, 3024, 304, 186.6, 3.4405, 3.0607, 18800], # 1.5h\n",
    "[ 12, 9, 3, 756, 3024, 288, 183.1, 3.4570, 3.0644, 18800], # 1.4h\n",
    "[ 12, 9, 3, 756, 3024, 272, 179.6, 3.4318,+3.0585, 18800], # 1.4h\n",
    "[ 12, 9, 3, 756, 3024, 256, 176.1, 3.4145, 3.0661, 18800], # 1.3h\n",
    "[ 12, 9, 3, 756, 3024, 240, 172.7, 3.4595, 3.0609, 18800], # 1.2h\n",
    "[ 12, 9, 3, 756, 3024, 224, 169.2, 3.5017, 3.0608, 18800], # 1.2h\n",
    "[ 12, 9, 3, 756, 3024, 208, 165.7, 3.4386,+3.0572, 18800], # 1.2h\n",
    "[ 12, 9, 3, 756, 3024, 192, 162.2, 3.4209, 3.0621, 18800], # 1.1h\n",
    "[ 12, 9, 3, 756, 3024, 176, 158.7, 3.4357, 3.0686, 18800], # 1.1h\n",
    "\n",
    "[ 12, 9, 3, 738, 2952, 272, 173.4, 3.4245, 3.0601, 18800], # 1.4h\n",
    "[ 12, 9, 3, 738, 2952, 208, 159.8, 3.4423, 3.0672, 18800], # 1.2h\n",
    "\n",
    "[ 12, 9, 3, 720, 2880, 320, 177.3, 3.4992, 3.0633, 18800], # 1.4h\n",
    "[ 12, 9, 3, 720, 2880, 304, 174.0, 3.4517, 3.0636, 18800], # 1.4h\n",
    "[ 12, 9, 3, 720, 2880, 288, 170.7, 3.4460, 3.0644, 18800], # 1.4h\n",
    "[ 12, 9, 3, 720, 2880, 272, 167.3, 3.4657, 3.0603, 18800], # 1.3h\n",
    "[ 12, 9, 3, 720, 2880, 256, 164.0, 3.4371, 3.0634, 18800], # 1.2h\n",
    "[ 12, 9, 3, 720, 2880, 240, 160.7, 3.4386, 3.0617, 18800], # 1.2h\n",
    "[ 12, 9, 3, 720, 2880, 224, 157.4, 3.4546, 3.0636, 18800], # 1.2h\n",
    "[ 12, 9, 3, 720, 2880, 208, 154.1, 3.4212, 3.0719, 18800], # 1.1h\n",
    "[ 12, 9, 3, 720, 2880, 192, 150.7, 3.4188, 3.0632, 18800], # 1.1h\n",
    "[ 12, 9, 3, 720, 2880, 176, 147.4, 3.4195, 3.0717, 18800], # 1.0h\n",
    "\n",
    "[ 12, 9, 3, 684, 2736, 320, 164.9, 3.4477, 3.0604, 18800], # 1.4h\n",
    "[ 12, 9, 3, 684, 2736, 304, 161.7, 3.3901, 3.0625, 18800], # 1.4h\n",
    "[ 12, 9, 3, 684, 2736, 288, 158.6, 3.5520, 3.0642, 18800], # 1.4h\n",
    "[ 12, 9, 3, 684, 2736, 272, 155.4, 3.4267, 3.0649, 18800], # 1.3h\n",
    "[ 12, 9, 3, 684, 2736, 256, 152.3, 3.4058, 3.0637, 18800], # 1.2h\n",
    "[ 12, 9, 3, 684, 2736, 240, 149.1, 3.4212, 3.0670, 18800], # 1.2h\n",
    "[ 12, 9, 3, 684, 2736, 224, 146.0, 3.4064, 3.0678, 18800], # 1.1h\n",
    "[ 12, 9, 3, 684, 2736, 208, 142.8, 3.4380, 3.0629, 18800], # 1.1h\n",
    "[ 12, 9, 3, 684, 2736, 192, 139.7, 3.4218, 3.0614, 18800], # 1.0h\n",
    "[ 12, 9, 3, 684, 2736, 176, 136.5, 3.3918, 3.0676, 18800], # 1.0h\n",
    "\n",
    "[ 12, 9, 3, 648, 2592, 320, 152.8, 3.3899, 3.0663, 18800], # 1.4h\n",
    "[ 12, 9, 3, 648, 2592, 304, 149.9, 3.4045, 3.0678, 18800], # 1.4h\n",
    "[ 12, 9, 3, 648, 2592, 288, 146.9, 3.3746, 3.0695, 18800], # 1.3h\n",
    "[ 12, 9, 3, 648, 2592, 272, 143.9, 3.3976, 3.0657, 18800], # 1.3h\n",
    "[ 12, 9, 3, 648, 2592, 256, 140.9, 3.4087, 3.0663, 18800], # 1.2h\n",
    "[ 12, 9, 3, 648, 2592, 240, 137.9, 3.4076, 3.0684, 18800], # 1.2h\n",
    "[ 12, 9, 3, 648, 2592, 224, 134.9, 3.3624, 3.0665, 18800], # 1.1h\n",
    "[ 12, 9, 3, 648, 2592, 208, 131.9, 3.4122, 3.0649, 18800], # 1.1h\n",
    "[ 12, 9, 3, 648, 2592, 192, 129.0, 3.3747, 3.0658, 18800], # 1.0h\n",
    "[ 12, 9, 3, 648, 2592, 176, 126.0, 3.3636, 3.0673, 18800], # 1.0h\n",
    "\n",
    "\n",
    "[ 12, 8, 4, 768, 3072, 256, 180.3, 3.4587, 3.0616, 18800], # 1.2h\n",
    "[ 12, 8, 4, 736, 2944, 256, 169.4, 3.4620, 3.0624, 18800], # 1.2h\n",
    "[ 12, 8, 4, 704, 2816, 256, 158.7, 3.4441, 3.0645, 18800], # 1.2h\n",
    "[ 12, 8, 4, 672, 2688, 256, 148.4, 3.4098, 3.0675, 18800], # 1.1h\n",
    "[ 12, 8, 4, 640, 2560, 256, 138.4, 3.3825, 3.0692, 18800], # 1.1h\n",
    "[ 12, 8, 4, 608, 2432, 256, 128.7, 3.3610, 3.0727, 18800], # 1.1h\n",
    "\n",
    "[ 12, 4, 4, 768, 3072, 256, 161.4, 3.5250, 3.0741, 18800], # 0.9h\n",
    "\n",
    "# N_steps searched (default: B12 lr11)\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.3019, 3.0822, 25600]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.2949, 3.0819, 24800]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.3484, 3.0863, 24000]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.3387, 3.0822, 23200]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.3632, 3.0780, 22400]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.3969, 3.0792, 21600]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4362, 3.0764, 20800]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4240, 3.0769, 20000]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4603, 3.0793, 19800]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4999, 3.0795, 19700]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4607, 3.0733, 19600]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4620, 3.0793, 19500]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4850, 3.0884, 19400]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4575, 3.0787, 19300]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.7165, 3.0989, 19200] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4580, 3.0714, 19200]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4144, 3.0780, 19100]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5051, 3.0662, 19000] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4866, 3.0728, 19000]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5081, 3.0770, 18900] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4869, 3.0834, 18900]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4828, 3.0716, 18850] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4844, 3.0724, 18850]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 4.8055, 3.3580, 18800] # lr40.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 4.2462, 3.1682, 18800] # lr30.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.9243, 3.1191, 18800] # lr25.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.6832, 3.1008, 18800] # lr20.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5780, 3.0776, 18800] # lr17.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5856, 3.0791, 18800] # lr16.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.6062, 3.0767, 18800] # lr16.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5417, 3.0864, 18800] # lr15.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5213, 3.0666, 18800] # lr15.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5200, 3.0702, 18800] # lr14.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4941, 3.0703, 18800] # lr14.0e-4\n",
    "[+12, 6, 3, 792, 3168, 256, 174.0, 3.4771,+3.0650, 18800] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5000, 3.0691, 18800] # lr13.5e-4 again\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4183, 3.0700, 18800] # lr13.5e-4 B14\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4628, 3.0677, 18800] # lr13.5e-4 B13\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5295, 3.0749, 18800] # lr13.5e-4 B11\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.6493, 3.0849, 18800] # lr13.5e-4 B10\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5127, 3.0662, 18800] # lr13.5e-4 WD1.1\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4668, 3.0721, 18800] # lr13.5e-4 WD0.9\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4753, 3.0708, 18800] # lr13.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4725, 3.0685, 18800] # lr12.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4814, 3.0725, 18800] # lr12.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4562, 3.0750, 18800] # lr11.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4535, 3.0711, 18800] # lr11.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5093, 3.0812, 18800] # lr10.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4771, 3.0810, 18800] # lr10.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5435, 3.0769, 18800] # lr 9.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4848, 3.0881, 18800] # lr 9.0e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4983, 3.0726, 18750] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4645, 3.0700, 18750]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5291, 3.0727, 18700] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5250, 3.0761, 18700]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5022, 3.0671, 18600] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4922, 3.0787, 18600]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4776, 3.0814, 18500]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.6029, 3.0668, 18400] # lr13.5e-4\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5317, 3.0718, 18400]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5170, 3.0772, 18300]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.4921, 3.0799, 18200]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5353, 3.0761, 18100]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5368, 3.0815, 18000]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5443, 3.0818, 17600]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.5807, 3.0799, 16800]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.6023, 3.0897, 16000]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.7741, 3.0893, 15200]\n",
    "[ 12, 6, 3, 792, 3168, 256, 174.0, 3.7133, 3.0976, 14400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea054808-e313-49b5-9455-609b14bfbeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.5072\n",
      "L12 att10 kv_heads5 hidden720 intermediate2880 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18800' max='18800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18800/18800 1:35:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.442500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4425, 3.0543, 18800],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  12 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =10 # 4 # G16 G8\n",
    "num_key_value_heads = 5 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*72 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=18800; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0543:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a49635-3ca8-4b7b-8934-faac13136321",
   "metadata": {},
   "source": [
    "# L10 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964584b-e297-4e55-883c-3077ff5ab532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L10 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.1807, 3.1066, 30000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.2931, 3.0871, 25000],\n",
    "[+10, 8, 4, 800, 3200, 256, 166.2, 3.4480,+3.0815, 20000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.6975, 3.0919, 15000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.8016, 3.1057, 14000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.9316, 3.1021, 13000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.9621, 3.1133, 12000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.0615, 3.1327, 11000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.2371, 3.1642, 10000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.3337, 3.2016, 9000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.5622, 3.2823, 8000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.8595, 3.3677, 7000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 5.1612, 3.6063, 6000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 5.7221, 3.7153, 5000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578ef800-2d45-4ac6-bba4-047b98ce55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.6\n",
      "L10 att8 kv_heads4 hidden800 intermediate3200 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 1:53:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.293100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.2931, 3.0871],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  10 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        \n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=25000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] # if loss_current < 3.03: torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28041e-2099-4850-b73c-234c00b323bc",
   "metadata": {},
   "source": [
    "# L9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb274541-2a17-4ae7-b8ca-9bc7f9cb312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L9 Normal Model (default: B12 WD1 lr13.5e-4)\n",
    "\n",
    "[ 9, 16,16, 832, 3328, 224, 224.0, 3.5464, 3.0698, 18800], # 1.7h\n",
    "\n",
    "[ 9, 16, 8, 832, 3328, 224, 197.2, 3.5460, 3.0681, 18800], # 1.5h\n",
    "\n",
    "\n",
    "[ 9, 16, 4, 864, 3456, 256, 203.8, 3.5357, 3.0680, 18800], # 1.5h\n",
    "[ 9, 16, 4, 864, 3456, 224, 193.8, 3.5879, 3.0735, 18800], # 1.4h\n",
    "[ 9, 16, 4, 864, 3456, 192, 183.8, 3.5325, 3.0645, 18800], # 1.3h\n",
    "[ 9, 16, 4, 864, 3456, 160, 173.9, 3.5402, 3.0695, 18800], # 1.2h\n",
    "\n",
    "[ 9, 16, 4, 832, 3328, 256, 193.3, 3.6155, 3.0704, 18800], # 1.5h\n",
    "[ 9, 16, 4, 832, 3328, 240, 188.5, 3.5121, 3.0710, 18800], # 1.5h\n",
    "[+9, 16, 4, 832, 3328, 224, 183.7, 3.5710,+3.0603, 18800], # 1.4h    BEST (not saved)\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.5195, 3.0677, 18800], # again\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.5808,+3.0624, 18800], # again  minGemma-hidden_layers9-att_heads16-kv_heads4-hidden832-intermediate3328-head_dim224-T512--2025-08-07-10-13\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.5172, 3.0657, 18800], # again\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.5329, 3.0649, 18000], # again\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.4813, 3.0656, 19600], # again with larger N 1.5h\n",
    "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.4682, 3.0661, 20400], # again with larger N 1.6h\n",
    "[ 9, 16, 4, 832, 3328, 208, 179.0, 3.4931, 3.0636, 18800], # 1.4h 11.8G\n",
    "[ 9, 16, 4, 832, 3328, 192, 174.2, 3.4975, 3.0688, 18800], # 1.3h\n",
    "[ 9, 16, 4, 832, 3328, 160, 164.6, 3.5157, 3.0669, 18800], # 1.2h\n",
    "\n",
    "[ 9, 16, 4, 800, 3200, 256, 183.1, 3.4738, 3.0657, 18800], # 1.5h\n",
    "[ 9, 16, 4, 800, 3200, 224, 173.9, 3.4711, 3.0725, 18800], # 1.4h 12.0G\n",
    "[ 9, 16, 4, 800, 3200, 192, 164.7, 3.4743, 3.0663, 18800], # 1.3h\n",
    "[ 9, 16, 4, 800, 3200, 160, 155.5, 3.4558, 3.0673, 18800], # 1.2h\n",
    "\n",
    "[ 9, 16, 4, 768, 3072, 256, 173.2, 3.4544, 3.0724, 18800], # 1.4h\n",
    "[ 9, 16, 4, 768, 3072, 224, 164.3, 3.4609, 3.0685, 18800], # 1.3h\n",
    "[ 9, 16, 4, 768, 3072, 192, 155.5, 3.5287, 3.0644, 18800], # 1.2h\n",
    "[ 9, 16, 4, 768, 3072, 160, 146.6, 3.4299, 3.0689, 18800], # 1.1h\n",
    "\n",
    "[ 9, 16, 4, 736, 2944, 256, 163.4, 3.4356, 3.0711, 18800], # 1.4h\n",
    "[ 9, 16, 4, 736, 2944, 224, 154.9, 3.4432, 3.0677, 18800], # 1.3h\n",
    "[ 9, 16, 4, 736, 2944, 192, 146.4, 3.4491, 3.0680, 18800], # 1.2h\n",
    "[ 9, 16, 4, 736, 2944, 160, 138.0, 3.4271, 3.0724, 18800], # 1.1h\n",
    "\n",
    "[ 9, 16, 4, 704, 2816, 256, 153.9, 3.3986, 3.0670, 18800], # 1.4h\n",
    "[ 9, 16, 4, 704, 2816, 224, 145.7, 3.3816, 3.0670, 18800], # 1.3h 11.5G\n",
    "[ 9, 16, 4, 704, 2816, 192, 137.6, 3.4961, 3.0689, 18800], # 1.2h\n",
    "[ 9, 16, 4, 704, 2816, 160, 129.5, 3.4273, 3.0710, 18800], # 1.1h\n",
    "\n",
    "[ 9, 16, 4, 672, 2688, 160, 121.3, 3.4063, 3.0783, 18800], # 1.1h\n",
    "[ 9, 16, 4, 672, 2688, 192, 129.1, 3.3909, 3.0705, 18800], # 1.1h\n",
    "[ 9, 16, 4, 672, 2688, 224, 136.8, 3.4070, 3.0706, 18800], # 1.3h\n",
    "[ 9, 16, 4, 672, 2688, 256, 144.5, 3.4042, 3.0805, 18800], # 1.3h\n",
    "\n",
    "\n",
    "[ 9, 16, 2, 832, 3328, 224, 177.0, 3.4650, 3.0683, 18800], # 1.4h\n",
    "\n",
    "[ 9, 16, 1, 832, 3328, 224, 173.7, 3.4425, 3.0779, 18800], # 1.4h 12.1G\n",
    "\n",
    "[ 9, 14, 2, 784, 3136, 256, 163.7, 3.4456,+3.0666, 18800], # 1.4h\n",
    "[ 9, 14, 2, 784, 3136, 240, 160.1, 3.4428, 3.0706, 18800], # 1.3h 11.5G\n",
    "[ 9, 14, 2, 784, 3136, 224, 156.4, 3.4706, 3.0690, 18800], # 1.3h 11.1G\n",
    "[ 9, 14, 2, 784, 3136, 208, 152.8, 3.4890, 3.0786, 18800], # 1.2h\n",
    "[ 9, 14, 2, 784, 3136, 192, 149.2, 3.4527, 3.0760, 18800], # 1.2h\n",
    "[ 9, 14, 2, 784, 3136, 176, 145.6, 3.4268, 3.0714, 18800], # 1.1h\n",
    "[ 9, 14, 2, 728, 2912, 256, 147.6, 3.4112, 3.0715, 18800], # 1.3h\n",
    "[ 9, 14, 2, 728, 2912, 240, 144.2, 3.3943, 3.0733, 18800], # 1.2h\n",
    "[ 9, 14, 2, 728, 2912, 224, 140.9, 3.4147, 3.0747, 18800], # 1.2h\n",
    "[ 9, 14, 2, 728, 2912, 208, 137.5, 3.4142, 3.0688, 18800], # 1.1h\n",
    "[ 9, 14, 2, 728, 2912, 192, 134.2, 3.4139,+3.0671, 18800], # 1.1h\n",
    "[ 9, 14, 2, 728, 2912, 176, 130.8, 3.4167, 3.0724, 18800], # 1.1h\n",
    "[ 9, 14, 2, 672, 2688, 256, 132.2, 3.4289, 3.0698, 18800], # 1.2h\n",
    "[ 9, 14, 2, 672, 2688, 240, 129.1, 3.3927, 3.0731, 18800], # 1.2h\n",
    "[ 9, 14, 2, 672, 2688, 224, 126.0, 3.3842, 3.0711, 18800], # 1.1h\n",
    "[ 9, 14, 2, 672, 2688, 208, 122.9, 3.3703, 3.0765, 18800], # 1.1h\n",
    "[ 9, 14, 2, 672, 2688, 192, 119.8, 3.3924, 3.0739, 18800], # 1.0h\n",
    "[ 9, 14, 2, 616, 2464, 256, 117.4, 3.3998, 3.0709, 18800], # 1.1h\n",
    "[ 9, 14, 2, 616, 2464, 240, 114.6, 3.3693, 3.0902, 18800], # 1.1h\n",
    "[ 9, 14, 2, 616, 2464, 224, 111.7, 3.3645, 3.0787, 18800], # 1.1h\n",
    "[ 9, 14, 2, 616, 2464, 208, 108.9, 3.3624, 3.0785, 18800], # 1.0h\n",
    "[ 9, 14, 2, 616, 2464, 192, 106.1, 3.3817, 3.0845, 18800], # 1.0h\n",
    "\n",
    "[ 9, 12, 4, 840, 3360, 256, 180.4, 3.5653, 3.0671, 18800], # 1.3h\n",
    "[ 9, 12, 4, 840, 3360, 240, 176.6, 3.5260, 3.0703, 18800], # 1.3h\n",
    "[ 9, 12, 4, 840, 3360, 224, 172.7, 3.5663, 3.0699, 18800], # 1.2h\n",
    "[ 9, 12, 4, 840, 3360, 208, 168.8, 3.5900, 3.0701, 18800], # 1.2h\n",
    "[ 9, 12, 4, 840, 3360, 192, 165.0, 3.5307, 3.0689, 18800], # 1.1h\n",
    "[ 9, 12, 4, 792, 3168, 272, 169.7, 3.4622, 3.0714, 18800], # 1.4h\n",
    "[ 9, 12, 4, 792, 3168, 256, 166.0, 3.5069,+3.0640, 18800], # 1.3h\n",
    "[ 9, 12, 4, 792, 3168, 240, 162.4, 3.4975, 3.0719, 18800], # 1.2h\n",
    "[ 9, 12, 4, 792, 3168, 224, 158.7, 3.4818, 3.0645, 18800], # 1.2h\n",
    "[ 9, 12, 4, 792, 3168, 208, 155.1, 3.4770, 3.0712, 18800], # 1.2h\n",
    "[ 9, 12, 4, 792, 3168, 192, 151.4, 3.4705, 3.0708, 18800], # 1.1h\n",
    "[ 9, 12, 4, 768, 3072, 256, 159.0, 3.4603, 3.0656, 18800], # 1.2h\n",
    "[ 9, 12, 4, 768, 3072, 240, 155.5, 3.4874, 3.0655, 18800], # 1.2h\n",
    "[ 9, 12, 4, 768, 3072, 224, 151.9, 3.4489, 3.0678, 18800], # 1.1h\n",
    "[ 9, 12, 4, 768, 3072, 208, 148.4, 3.4478, 3.0705, 18800], # 1.1h\n",
    "[ 9, 12, 4, 768, 3072, 192, 144.8, 3.5688, 3.0653, 18800], # 1.0h\n",
    "[ 9, 12, 4, 768, 3072, 176, 141.3, 3.4437, 3.0673, 18800], # 1.0h\n",
    "[ 9, 12, 4, 744, 2976, 256, 152.1, 3.4429, 3.0680, 18800], # 1.2h\n",
    "[ 9, 12, 4, 744, 2976, 240, 148.7, 3.4751, 3.0690, 18800], # 1.1h\n",
    "[ 9, 12, 4, 744, 2976, 224, 145.2, 3.4869, 3.0680, 18800], # 1.1h\n",
    "[ 9, 12, 4, 744, 2976, 208, 141.8, 3.4814, 3.0725, 18800], # 1.1h\n",
    "[ 9, 12, 4, 744, 2976, 192, 138.4, 3.4448, 3.0654, 18800], # 1.0h\n",
    "[ 9, 12, 4, 744, 2976, 176, 135.0, 3.4468, 3.0681, 18800], # 1.0h\n",
    "[ 9, 12, 4, 696, 2784, 256, 138.7, 3.4219, 3.0673, 18800], # 1.1h\n",
    "[ 9, 12, 4, 696, 2784, 240, 135.5, 3.4061, 3.0683, 18800], # 1.1h\n",
    "[ 9, 12, 4, 696, 2784, 224, 132.3, 3.4161, 3.0686, 18800], # 1.1h\n",
    "[ 9, 12, 4, 696, 2784, 208, 129.1, 3.4159, 3.0693, 18800], # 1.0h\n",
    "[ 9, 12, 4, 696, 2784, 192, 125.9, 3.4453, 3.0715, 18800], # 1.0h\n",
    "[ 9, 12, 4, 648, 2592, 256, 125.8, 3.4167, 3.0684, 18800], # 1.1h\n",
    "[ 9, 12, 4, 648, 2592, 240, 122.8, 3.4018, 3.0759, 18800], # 1.1h\n",
    "[ 9, 12, 4, 648, 2592, 224, 119.8, 3.3658, 3.0662, 18800], # 1.0h\n",
    "[ 9, 12, 4, 648, 2592, 208, 116.8, 3.3787, 3.0734, 18800], # 1.0h\n",
    "[ 9, 12, 4, 648, 2592, 192, 113.8, 3.3868, 3.0741, 18800], # 1.0h\n",
    "\n",
    "[ 9, 12, 3, 792, 3168, 256, 162.4, 3.4998, 3.0668, 18800], # 1.3h\n",
    "[ 9, 12, 3, 792, 3168, 224, 155.5, 3.4815, 3.0674, 18800], # 1.2h\n",
    "[ 9, 12, 3, 792, 3168, 192, 148.7, 3.4487, 3.0705, 18800], # 1.1h\n",
    "[ 9, 12, 3, 768, 3072, 272, 158.8, 3.4713, 3.0697, 18800], # 1.3h\n",
    "[ 9, 12, 3, 768, 3072, 256, 155.5, 3.4743,+3.0638, 18800], # 1.2h\n",
    "[ 9, 12, 3, 768, 3072, 240, 152.1, 3.4600, 3.0675, 18800], # 1.1h\n",
    "[ 9, 12, 3, 768, 3072, 224, 148.8, 3.4618, 3.0708, 18800], # 1.1h\n",
    "[ 9, 12, 3, 768, 3072, 208, 145.5, 3.4669, 3.0647, 18800], # 1.1h\n",
    "[ 9, 12, 3, 768, 3072, 192, 142.2, 3.4780, 3.0710, 18800], # 1.0h 9.5G\n",
    "[ 9, 12, 3, 768, 3072, 176, 138.9, 3.5307, 3.0761, 18800], # 1.0h\n",
    "[ 9, 12, 3, 744, 2976, 272, 151.9, 3.4361, 3.0660, 18800], # 1.3h\n",
    "[ 9, 12, 3, 744, 2976, 256, 148.7, 3.4452, 3.0748, 18800], # 1.1h\n",
    "[ 9, 12, 3, 744, 2976, 240, 145.5, 3.4428, 3.0678, 18800], # 1.1h\n",
    "[ 9, 12, 3, 744, 2976, 224, 142.2, 3.4119, 3.0694, 18800], # 1.1h\n",
    "[ 9, 12, 3, 744, 2976, 208, 139.0, 3.4525, 3.0675, 18800], # 1.1h\n",
    "[ 9, 12, 3, 744, 2976, 192, 135.8, 3.4313, 3.0682, 18800], # 1.0h\n",
    "[ 9, 12, 3, 744, 2976, 176, 132.6, 3.4401, 3.0697, 18800], # 1.0h\n",
    "\n",
    "\n",
    "[ 9, 12, 2, 840, 3360, 192, 159.1, 3.4712, 3.0702, 18800], # 1.1h\n",
    "[ 9, 12, 2, 840, 3360, 176, 155.8, 3.5350, 3.0783, 18800], # 1.1h\n",
    "\n",
    "[ 9, 12, 2, 816, 3264, 288, 172.2, 3.4944, 3.0746, 18800], # 1.4h\n",
    "[ 9, 12, 2, 816, 3264, 272, 168.9, 3.4952, 3.0703, 18800], # 1.3h\n",
    "[ 9, 12, 2, 816, 3264, 256, 165.6, 3.4738, 3.0728, 18800], # 1.2h\n",
    "[ 9, 12, 2, 816, 3264, 240, 162.4, 3.4590, 3.0726, 18800], # 1.2h\n",
    "[ 9, 12, 2, 816, 3264, 224, 159.1, 3.4795, 3.0698, 18800], # 1.2h\n",
    "[ 9, 12, 2, 816, 3264, 208, 155.8, 3.4799, 3.0699, 18800], # 1.1h\n",
    "[ 9, 12, 2, 816, 3264, 192, 152.5, 3.4891, 3.0702, 18800], # 1.1h\n",
    "[ 9, 12, 2, 816, 3264, 176, 149.2, 3.4719, 3.0689, 18800], # 1.1h\n",
    "\n",
    "[ 9, 12, 2, 792, 3168, 304, 168.3, 3.4654, 3.0677, 18800], # 1.6h 12.2G\n",
    "[ 9, 12, 2, 792, 3168, 288, 165.1, 3.5211,+3.0620, 18800], # 1.4h\n",
    "[ 9, 12, 2, 792, 3168, 272, 161.9, 3.4868, 3.0747, 18800], # 1.3h\n",
    "[ 9, 12, 2, 792, 3168, 256, 158.7, 3.5385, 3.0700, 18800], # 1.2h\n",
    "[ 9, 12, 2, 792, 3168, 240, 155.5, 3.4701, 3.0658, 18800], # 1.2h\n",
    "[ 9, 12, 2, 792, 3168, 224, 152.3, 3.5675, 3.0712, 18800], # 1.2h\n",
    "[ 9, 12, 2, 792, 3168, 208, 149.1, 3.4482, 3.0663, 18800], # 1.1h\n",
    "[ 9, 12, 2, 792, 3168, 192, 145.9, 3.4813, 3.0742, 18800], # 1.1h\n",
    "[ 9, 12, 2, 792, 3168, 176, 142.8, 3.4643, 3.0733, 18800], # 1.0h\n",
    "\n",
    "[ 9, 12, 2, 768, 3072, 288, 158.1, 3.4810, 3.0741, 18800], # 1.3h\n",
    "[ 9, 12, 2, 768, 3072, 272, 155.0, 3.4388, 3.0666, 18800], # 1.3h 11.8G\n",
    "[ 9, 12, 2, 768, 3072, 256, 151.9, 3.4515, 3.0701, 18800], # 1.1h\n",
    "[ 9, 12, 2, 768, 3072, 240, 148.8, 3.4402, 3.0724, 18800], # 1.1h\n",
    "[ 9, 12, 2, 768, 3072, 224, 145.7, 3.5115, 3.0697, 18800], # 1.1h\n",
    "[ 9, 12, 2, 768, 3072, 208, 142.6, 3.4219, 3.0720, 18800], # 1.1h\n",
    "[ 9, 12, 2, 768, 3072, 192, 139.5, 3.4133, 3.0681, 18800], # 1.0h\n",
    "[ 9, 12, 2, 768, 3072, 176, 136.4, 3.4134, 3.0738, 18800], # 1.0h\n",
    "\n",
    "[ 9, 12, 2, 744, 2976, 288, 151.2, 3.4307, 3.0713, 18800], # 1.3h\n",
    "[ 9, 12, 2, 744, 2976, 272, 148.2, 3.3995, 3.0684, 18800], # 1.2h\n",
    "[ 9, 12, 2, 744, 2976, 256, 145.2, 3.5323, 3.0676, 18800], # 1.1h\n",
    "[ 9, 12, 2, 744, 2976, 240, 142.2, 3.4562, 3.0695, 18800], # 1.1h\n",
    "[ 9, 12, 2, 744, 2976, 224, 139.2, 3.4050,+3.0607, 18800], # 1.1h   minGemma-hidden_layers9-att_heads12-kv_heads2-hidden744-intermediate2976-head_dim224-T512--2025-08-08-15-54\n",
    "[ 9, 12, 2, 744, 2976, 224, 139.2, 3.4115, 3.0666, 18800], # again\n",
    "[ 9, 12, 2, 744, 2976, 224, 139.2, 3.4094, 3.0716, 19600], # again with larger N 1.1h\n",
    "[ 9, 12, 2, 744, 2976, 208, 136.2, 3.4400, 3.0700, 18800], # 1.0h\n",
    "[ 9, 12, 2, 744, 2976, 192, 133.2, 3.4398, 3.0695, 18800], # 1.0h\n",
    "[ 9, 12, 2, 744, 2976, 176, 130.2, 3.4472, 3.0719, 18800], # 1.0h\n",
    "\n",
    "[ 9, 12, 2, 720, 2880, 288, 144.5, 3.4211, 3.0737, 18800], # 1.3h\n",
    "[ 9, 12, 2, 720, 2880, 272, 141.6, 3.4304, 3.0695, 18800], # 1.2h\n",
    "[ 9, 12, 2, 720, 2880, 256, 138.7, 3.4615, 3.0697, 18800], # 1.1h\n",
    "[ 9, 12, 2, 720, 2880, 240, 135.8, 3.4284, 3.0711, 18800], # 1.1h\n",
    "[ 9, 12, 2, 720, 2880, 224, 132.9, 3.4037, 3.0740, 18800], # 1.1h\n",
    "[ 9, 12, 2, 720, 2880, 208, 130.0, 3.4216, 3.0723, 18800], # 1.0h\n",
    "[ 9, 12, 2, 720, 2880, 192, 127.1, 3.4123, 3.0690, 18800], # 1.0h\n",
    "[ 9, 12, 2, 720, 2880, 176, 124.2, 3.4981, 3.0739, 18800], # 0.9h\n",
    "\n",
    "[ 9, 12, 2, 696, 2784, 288, 137.9, 3.4314, 3.0727, 18800], # 1.2h\n",
    "[ 9, 12, 2, 696, 2784, 272, 135.1, 3.4460, 3.0657, 18800], # 1.2h\n",
    "[ 9, 12, 2, 696, 2784, 256, 132.3, 3.4491, 3.0724, 18800], # 1.1h\n",
    "[ 9, 12, 2, 696, 2784, 240, 129.5, 3.4136, 3.0718, 18800], # 1.1h\n",
    "[ 9, 12, 2, 696, 2784, 224, 126.7, 3.4001, 3.0724, 18800], # 1.0h\n",
    "[ 9, 12, 2, 696, 2784, 208, 123.8, 3.4122, 3.0771, 18800], # 1.0h\n",
    "[ 9, 12, 2, 696, 2784, 192, 121.0, 3.3852, 3.0722, 18800], # 1.0h\n",
    "[ 9, 12, 2, 696, 2784, 176, 118.2, 3.4399, 3.0763, 18800], # 0.9h\n",
    "\n",
    "[ 9, 12, 2, 672, 2688, 288, 131.4, 3.4598, 3.0683, 18800], # 1.2h\n",
    "[ 9, 12, 2, 672, 2688, 272, 128.7, 3.4000, 3.0748, 18800], # 1.2h\n",
    "[ 9, 12, 2, 672, 2688, 256, 126.0, 3.4495, 3.0656, 18800], # 1.1h\n",
    "[ 9, 12, 2, 672, 2688, 240, 123.3, 3.4021, 3.0771, 18800], # 1.1h\n",
    "[ 9, 12, 2, 672, 2688, 224, 120.5, 3.4064, 3.0712, 18800], # 1.0h\n",
    "[ 9, 12, 2, 672, 2688, 208, 117.8, 3.3774, 3.0760, 18800], # 1.0h\n",
    "[ 9, 12, 2, 672, 2688, 192, 115.1, 3.4019, 3.0777, 18800], # 0.9h\n",
    "[ 9, 12, 2, 672, 2688, 176, 112.4, 3.4113, 3.0764, 18800], # 0.9h\n",
    "\n",
    "\n",
    "# N_step and learning_rate searched for a small model\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3356, 3.0836, 25600]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3317, 3.0791, 24800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3480, 3.0824, 24400]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3500, 3.0816, 24200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3406, 3.0746, 24000]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3549, 3.0769, 23800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4098, 3.0799, 23600]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4043, 3.0812, 23200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4159, 3.0753, 22800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3865, 3.0858, 22600]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4104, 3.0869, 22500]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3913, 3.0785, 22450]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4464, 3.0752, 22400] # lr13.0e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4195, 3.0713, 22400] # lr12.5e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4586, 3.0706, 22400] # lr12.0e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3941,+3.0693, 22400] # lr11.5e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4065, 3.0724, 22400] # lr11.0e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4465, 3.0768, 22400] # lr11.0e-4 again\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3985, 3.0825, 22400] # lr10.5e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3950, 3.0895, 22400] # lr10.0e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4140, 3.0931, 22400] # lr 9.5e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3784, 3.0989, 22400] # lr 9.0e-4\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.3942, 3.0708, 22350]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4018, 3.0779, 22300]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4421, 3.0786, 22200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4423, 3.0775, 22000]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4387, 3.0764, 21600]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4480, 3.0771, 21200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.4425, 3.0770, 20800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5036, 3.0800, 20000]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5056, 3.0798, 19200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5649, 3.0780, 18800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5397, 3.0761, 18400]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5433, 3.0779, 18000]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.5917, 3.0796, 17600]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.6719, 3.0808, 16800]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.6828, 3.0815, 16000]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.8448, 3.0929, 15200]\n",
    "[ 9, 8, 4, 832, 3328, 256, 162.7, 3.7973, 3.0893, 14400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f663498-8b1e-4309-8df1-f16e32e2cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.5072\n",
      "L9 att16 kv_heads4 hidden832 intermediate3328 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18800' max='18800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18800/18800 1:28:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.571000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9, 16, 4, 832, 3328, 224, 183.7, 3.5710, 3.0603, 18800],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   9 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =16 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*52 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=18800; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0636:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec6671e-bfab-4fb9-b62f-d3889d854713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.5072\n",
      "L9 att12 kv_heads2 hidden744 intermediate2976 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18800' max='18800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18800/18800 1:07:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.405000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9, 12, 2, 744, 2976, 224, 139.2, 3.4050, 3.0607, 18800],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   9 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =12 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*62 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=18800; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0607:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67b62e-1692-40c9-9464-b0cde91ff7ce",
   "metadata": {},
   "source": [
    "# L8 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea191f9-e096-4f24-982b-2bd221f9cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L8 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 5.7465, 3.7427, 5000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 5.1040, 3.5869, 6000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.7882, 3.4449, 7000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.5029, 3.3351, 8000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.4746, 3.2090, 9000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.2663, 3.1698, 10000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.1281, 3.1661, 11000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.9549, 3.1171, 12000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.8525, 3.1050, 13000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.8220, 3.0935, 14000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 6.2252, 4.5156, 14000], # lr10e-3\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.7526, 3.0987, 15000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.6494, 3.0859, 16000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.6319, 3.0848, 17000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5341, 3.0828, 18000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5459, 3.0801, 19000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5318, 3.0850, 20000],\n",
    "[+8, 8, 4, 800, 3200, 480, 175.4, 3.4380,+3.0768, 21000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.4266, 3.0830, 22000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.3882, 3.0785, 23000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.3449, 3.0896, 24000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2977, 3.0901, 26000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2600, 3.0904, 28000], # 2.1h\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2057, 3.1062, 30000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d23992a-90ef-49a0-bca8-cafd2c3d571e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.024\n",
      "L8 att8 kv_heads4 hidden800 intermediate3200 head_dim480 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21000' max='21000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21000/21000 1:35:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.438000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.4380, 3.0768],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   8 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 480 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=21000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] \n",
    "if torch.Tensor(loss).mean() < 3.0768:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e023cc-ae96-46f9-b090-30686f7090c3",
   "metadata": {},
   "source": [
    "# L6 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853da4f-95c8-4ce2-b2f0-9a74f1bdb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L6 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1039, 3.1584, 40000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1093, 3.1519, 39000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1261, 3.1433, 38000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1181, 3.1443, 37000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1365, 3.1336, 36000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1393, 3.1364, 35000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1610, 3.1337, 34000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1499, 3.1289, 33000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2074, 3.1233, 32000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2335, 3.1137, 31000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2334, 3.1148, 30000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2369, 3.1145, 29000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2508, 3.1103, 28000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3013, 3.1137, 27000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3184, 3.1186, 26000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3556, 3.1025, 25000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3530, 3.1014, 24000],\n",
    "[+6, 8, 4, 768, 3072, 256, 109.4, 3.3593,+3.0972, 24500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3756, 3.1009, 23000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4384,+3.0983, 21500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4643, 3.1026, 20800],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4771, 3.1049, 20000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5378, 3.1052, 19200],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5558, 3.0994, 18900],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5543, 3.0984, 18900], # lr10.5e-4\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5785,+3.0984, 18500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5754, 3.1091, 18000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.6484, 3.1038, 17000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.6696, 3.1147, 16000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8011, 3.1100, 15000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8267, 3.1166, 14000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8805, 3.1264, 13000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.0110, 3.1507, 12000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.1352, 3.1548, 11000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.2552, 3.1765, 10000],\n",
    "\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4519, 3.0979, 19200], # cosine instead of linear lr scheduling\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5426, 3.0966, 19200], # cosine instead of linear lr scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31c9eb23-496b-47b5-8373-a2ec89017134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.456\n",
      "L6 att8 kv_heads4 hidden768 intermediate3072 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24000' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24000/24000 1:07:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.353000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3530, 3.1014],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   6 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*96 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=24000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] # if loss_current < 3.03: torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
