{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d1f54a-eff5-43bd-bad2-ae64c3513f64",
   "metadata": {},
   "source": [
    "### Note: only L16 and L30 were uploaded so far. After the clean up is done, the summaries for the other layers will come soon in the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4dc65c-6467-45fb-b5c9-7c24dac02a6a",
   "metadata": {},
   "source": [
    "# Summary of best models for given numbers of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395f15-8b92-41a5-8c3d-af5ec3f42f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is the output for a model in the following format. Different lines for different model structures. See the cell below for the excecuted codes.\n",
    "# print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')\n",
    "# Example: [16layers, 12(6)heads, hidden-dim=696, inter-layer-dim = 696x4, head-dim=224, #param=218million, train_loss, val_loss, Num-steps]\n",
    "\n",
    "[+30, 4, 2, 648, 2592, 224, 236.2, 3.3476,+3.0493, 19600], # 2.3h 16.7G  minGemma-hidden_layers30-att_heads4-kv_heads2-hidden648-intermediate2592-head_dim224-T512--2025-06-07-15-50.pth\n",
    "[ 28, 6, 3, 648, 2592, 240, 252.2, 3.3411,+3.0502, 19600], # 2.1h 15.6G\n",
    "[ 26, 4, 4, 648, 2592, 240, 228.5, 3.3902,+3.0504, 19600], # 1.7h\n",
    "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.4022,+3.0508, 19600], # 2.1h        minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-03-23-24.pth\n",
    "[+16, 8, 4, 672, 2688, 192, 170.1, 3.3666,+3.0526, 19600], # 1.3h        minGemma-hidden_layers16-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-07-15-16-14.pth\n",
    "[ 14, 8, 4, 704, 2816, 192, 164.2, 3.3670,+3.0553, 20800],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.4480,+3.0815, 20000],\n",
    "[  8, 8, 4, 800, 3200, 480, 175.4, 3.4380,+3.0768, 21000],\n",
    "[  6, 8, 4, 768, 3072, 256, 109.4, 3.3593,+3.0972, 24500],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f20c11-804b-497f-934b-7eaf79b88de7",
   "metadata": {},
   "source": [
    "# L30 (Models with 30 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99cac9-b669-4ecd-9450-2cea05999741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L30 Normal Model (default: x4 B12 lr13.5e-4 WD1)  # Best model for each number of heads: 3(3):3.0542, 4(2):3.0493, 5(1):3.0498, 6(3):3.0500\n",
    "[ 30, 6, 3, 756, 3024, 256, 348.5, 3.5112, 3.0693, 19600], # 3.1h\n",
    "[ 30, 6, 3, 756, 3024, 240, 342.0, 3.6438, 3.0718, 19600], # 3.0h 18.8G(VRAM)\n",
    "[ 30, 6, 3, 756, 3024, 224, 335.4, 3.3749, 3.0595, 19600], # 2.9h\n",
    "[ 30, 6, 3, 756, 3024, 208, 328.9, 3.5371, 3.0578, 19600], # 2.9h\n",
    "[ 30, 6, 3, 756, 3024, 192, 322.4, 3.4953, 3.0545, 19600], # 2.8h\n",
    "[ 30, 6, 3, 756, 3024, 176, 315.8, 3.4483,+3.0533, 19600], # 2.7h\n",
    "[ 30, 6, 3, 756, 3024, 160, 309.3, 3.4155, 3.0563, 19600], # 2.7h (96,112,128,144,272)\n",
    "\n",
    "[ 30, 6, 3, 720, 2880, 272, 328.8, 3.3990, 3.0544, 19600], # 3.2h\n",
    "[ 30, 6, 3, 720, 2880, 256, 322.6, 3.3970, 3.0576, 19600], # 2.9h\n",
    "[ 30, 6, 3, 720, 2880, 240, 316.4, 3.3645, 3.0542, 19600], # 2.9h\n",
    "[ 30, 6, 3, 720, 2880, 224, 310.1, 3.4218, 3.0572, 19600], # 2.8h\n",
    "[+30, 6, 3, 720, 2880, 208, 303.9, 3.4047,+3.0500, 19600], # 2.8h\n",
    "[ 30, 6, 3, 720, 2880, 192, 297.7, 3.3858,+3.0522, 19600], # 2.6h\n",
    "[ 30, 6, 3, 720, 2880, 176, 291.5, 3.3717,+3.0507, 19600], # 2.6h\n",
    "[ 30, 6, 3, 720, 2880, 160, 285.3, 3.4169,+3.0525, 19600], # 2.5h\n",
    "[ 30, 6, 3, 720, 2880, 144, 279.0, 3.4071, 3.0595, 19600], # 2.5h\n",
    "[ 30, 6, 3, 720, 2880, 128, 272.8, 3.5295, 3.0676, 19600], # 2.1h 16.6G\n",
    "[ 30, 6, 3, 720, 2880, 112, 266.6, 3.4299, 3.0560, 19600], # 2.1h\n",
    "[ 30, 6, 3, 720, 2880,  96, 260.4, 3.4120, 3.0547, 19600],\n",
    "\n",
    "[ 30, 6, 3, 684, 2736, 320, 321.2, 3.4579, 3.0607, 19600], # 3.3h 18.5G\n",
    "[ 30, 6, 3, 684, 2736, 304, 315.3, 3.3796,+3.0527, 19600], # 3.3h\n",
    "[ 30, 6, 3, 684, 2736, 288, 309.4, 3.4154,+3.0536, 19600], # 3.2h\n",
    "[ 30, 6, 3, 684, 2736, 272, 303.5, 3.3830,+3.0514, 19600], # 3.1h\n",
    "[ 30, 6, 3, 684, 2736, 256, 297.6, 3.3459, 3.0570, 19600], # 2.9h\n",
    "[ 30, 6, 3, 684, 2736, 240, 291.7, 3.3628,+3.0520, 19600], # 2.9h\n",
    "[ 30, 6, 3, 684, 2736, 224, 285.8, 3.3681,+3.0519, 19600], # 2.8h\n",
    "[ 30, 6, 3, 684, 2736, 208, 279.9, 3.3712,+3.0513, 19600], # 2.7h\n",
    "[ 30, 6, 3, 684, 2736, 192, 273.9, 3.3857, 3.0541, 19600], # 2.6h\n",
    "[ 30, 6, 3, 684, 2736, 160, 262.1, 3.3688, 3.0559, 19600], # 2.5h\n",
    "[ 30, 6, 3, 684, 2736, 144, 256.2, 3.3902,+3.0525, 19600], # 2.5h\n",
    "[ 30, 6, 3, 684, 2736, 128, 250.3, 3.4015, 3.0580, 19600], # 2.1h\n",
    "[ 30, 6, 3, 684, 2736,  96, 238.5, 3.3598, 3.0613, 19600], # 1.9h\n",
    "\n",
    "[ 30, 6, 3, 648, 2592, 272, 279.1, 3.5865, 3.0815, 19600], # 3.0h\n",
    "[ 30, 6, 3, 648, 2592, 256, 273.5, 3.3280, 3.0543, 19600], # 2.8h\n",
    "[ 30, 6, 3, 648, 2592, 224, 262.3, 3.3216, 3.0544, 19600], # 2.7h 17.0G\n",
    "[ 30, 6, 3, 648, 2592, 192, 251.1, 3.3217, 3.0554, 19600], # 2.5h\n",
    "[ 30, 6, 3, 648, 2592, 176, 245.5, 3.3365, 3.0556, 19600], # 2.4h\n",
    "[ 30, 6, 3, 648, 2592, 160, 239.9, 3.3363, 3.0577, 19600], # 2.7h 15.6G\n",
    "[ 30, 6, 3, 648, 2592, 144, 234.3, 3.3932,+3.0537, 19600], # 17.9h\n",
    "[ 30, 6, 3, 648, 2592, 128, 228.7, 3.3485, 3.0606, 19600], # 1.4h\n",
    "[ 30, 6, 3, 648, 2592, 112, 223.1, 3.3513, 3.0566, 19600], # 1.4h\n",
    "[ 30, 6, 3, 648, 2592,  96, 217.5, 3.3735, 3.0636, 19600], # 1.3h\n",
    "\n",
    "[ 30, 6, 3, 612, 2448, 272, 255.7, 3.3341, 3.0566, 19600], # 2.8h\n",
    "[ 30, 6, 3, 612, 2448, 256, 250.4, 3.3796, 3.0550, 19600], # 2.6h\n",
    "[ 30, 6, 3, 612, 2448, 224, 239.8, 3.3351,+3.0541, 19600], # 2.5h\n",
    "[ 30, 6, 3, 612, 2448, 208, 234.5, 3.2887, 3.0556, 19600], # 2.2h 15.6G\n",
    "[ 30, 6, 3, 612, 2448, 192, 229.2, 3.3320, 3.0550, 19600], # 2.3h \n",
    "[ 30, 6, 3, 612, 2448, 176, 224.0, 3.4298, 3.0543, 19600], # 21.2h\n",
    "[ 30, 6, 3, 612, 2448, 144, 213.4, 3.3164, 3.0625, 19600], # 1.7h\n",
    "[ 30, 6, 3, 612, 2448, 128, 208.1, 3.3355, 3.0635, 19600], # 1.3h\n",
    "[ 30, 6, 3, 612, 2448, 112, 202.8, 3.3323, 3.0654, 19600], # 1.3h\n",
    "[ 30, 6, 3, 612, 2448,  96, 197.5, 3.3270, 3.0643, 19600], # 1.2h\n",
    "\n",
    "[ 30, 6, 3, 576, 2304, 256, 228.2, 3.2910, 3.0560, 19600], # 2.5h\n",
    "[ 30, 6, 3, 576, 2304, 224, 218.3, 3.2848, 3.0603, 19600], # 2.4h 16.0G\n",
    "[ 30, 6, 3, 576, 2304, 192, 208.3, 3.3373, 3.0574, 19600], # 2.2h \n",
    "[ 30, 6, 3, 576, 2304, 176, 203.3, 3.3430, 3.0574, 19600], # 1.8h\n",
    "[ 30, 6, 3, 576, 2304, 144, 193.4, 3.4027, 3.0640, 19600], # 1.6h\n",
    "[ 30, 6, 3, 576, 2304, 128, 188.4, 3.3448, 3.0656, 19600], # 1.2h\n",
    "[ 30, 6, 3, 576, 2304, 112, 183.4, 3.3133, 3.0675, 19600], # 1.2h\n",
    "[ 30, 6, 3, 576, 2304,  96, 178.4, 3.4221, 3.0722, 19600], # 1.1h\n",
    "\n",
    "[ 30, 6, 3, 540, 2160, 256, 206.9, 3.2858, 3.0691, 19600], # 2.5h 15.5G\n",
    "[ 30, 6, 3, 540, 2160, 224, 197.6, 3.3048, 3.0699, 19600], # 2.3h\n",
    "[ 30, 6, 3, 540, 2160, 192, 188.3, 3.2948, 3.0612, 19600], # 2.2h 14.8G\n",
    "[ 30, 6, 3, 540, 2160, 176, 183.6, 3.3108, 3.0590, 19600], # 2.1h\n",
    "[ 30, 6, 3, 540, 2160, 160, 178.9, 3.3201, 3.0617, 19600], # 1.6h 14.1G\n",
    "[ 30, 6, 3, 540, 2160, 144, 174.3, 3.3020, 3.0683, 19600], # 1.6h\n",
    "[ 30, 6, 3, 540, 2160, 128, 169.6, 3.4055, 3.0744, 19600], # 1.2h\n",
    "[ 30, 6, 3, 540, 2160, 112, 165.0, 3.3633, 3.0700, 19600], # 1.2h\n",
    "[ 30, 6, 3, 540, 2160,  96, 160.3, 3.2944, 3.0780, 19600], # 1.1h\n",
    "\n",
    "\n",
    "# 5(5) is TOO big? But there is no 5(2) or 5(3)...\n",
    "[ 30, 5, 5, 760, 3040, 256, 363.1, 3.4868, 3.0620, 19600], # 3.0h\n",
    "[ 30, 5, 5, 760, 3040, 224, 348.5, 3.3850, 3.0563, 19600], # 2.9h\n",
    "[ 30, 5, 5, 760, 3040, 192, 333.9, 3.4469, 3.0570, 19600], # 2.7h\n",
    "[ 30, 5, 5, 760, 3040, 160, 319.3, 3.5307, 3.0586, 19600], # 2.6h\n",
    "\n",
    "[ 30, 5, 5, 720, 2880, 256, 333.6, 3.4710, 3.0630, 19600], # 2.9h\n",
    "[ 30, 5, 5, 720, 2880, 224, 319.8, 3.4334, 3.0571, 19600], # 2.7h\n",
    "[ 30, 5, 5, 720, 2880, 192, 306.0, 3.4144, 3.0546, 19600], # 2.6h\n",
    "[ 30, 5, 5, 720, 2880, 160, 292.2, 3.4221, 3.0557, 19600], # 2.5h\n",
    "[ 30, 5, 5, 720, 2880, 144, 285.3, 3.4572, 3.0557, 19600], # 2.4h\n",
    "[ 30, 5, 5, 720, 2880, 128, 278.3, 3.4040, 3.0613, 19600], # 2.1h\n",
    "[ 30, 5, 5, 720, 2880,  96, 264.5, 3.4217, 3.0650, 19600], # 1.9h\n",
    "\n",
    "[ 30, 5, 5, 700, 2800, 144, 272.3, 3.3942, 3.0580, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 5, 680, 2720, 256, 305.3, 3.4081, 3.0567, 19600], # 2.8h\n",
    "[ 30, 5, 5, 680, 2720, 224, 292.3, 3.3949, 3.0566, 19600], # 2.6h\n",
    "[ 30, 5, 5, 680, 2720, 192, 279.2, 3.4985, 3.0565, 19600], # 2.5h\n",
    "[ 30, 5, 5, 680, 2720, 160, 266.1, 3.4381, 3.0591, 19600], # 2.4h\n",
    "[ 30, 5, 5, 680, 2720, 144, 259.6, 3.5180,+3.0509, 19600], # 2.3h\n",
    "[ 30, 5, 5, 680, 2720, 128, 253.1, 3.3855, 3.0616, 19600], # 2.0h\n",
    "[ 30, 5, 5, 680, 2720,  96, 240.0, 3.3802, 3.0641, 19600], # 1.8h\n",
    "\n",
    "[ 30, 5, 5, 660, 2640, 144, 247.2, 3.3556, 3.0578, 19600], # 2.3h\n",
    "\n",
    "[ 30, 5, 5, 640, 2560, 320, 302.7, 3.3873, 3.0604, 19600], # 2.9h 18.7G\n",
    "[ 30, 5, 5, 640, 2560, 304, 296.6, 3.3551, 3.0568, 19600], # 2.9h\n",
    "[ 30, 5, 5, 640, 2560, 288, 290.4, 3.3438, 3.0575, 19600], # 2.8h\n",
    "[ 30, 5, 5, 640, 2560, 272, 284.3, 3.3421, 3.0635, 19600], # 2.7h\n",
    "[ 30, 5, 5, 640, 2560, 256, 278.1, 3.5115, 3.0738, 19600], # 2.5h\n",
    "[ 30, 5, 5, 640, 2560, 240, 272.0, 3.3667, 3.0617, 19600], # 2.5h\n",
    "[ 30, 5, 5, 640, 2560, 224, 265.8, 3.3682, 3.0582, 19600], # 2.4h\n",
    "[ 30, 5, 5, 640, 2560, 208, 259.7, 3.3680, 3.0557, 19600], # 2.4h\n",
    "[ 30, 5, 5, 640, 2560, 192, 253.6, 3.3625, 3.0576, 19600], # 2.3h\n",
    "[ 30, 5, 5, 640, 2560, 176, 247.4, 3.3677, 3.0556, 19600], # 2.2h\n",
    "[ 30, 5, 5, 640, 2560, 160, 241.3, 3.3420, 3.0565, 19600], # 2.1h\n",
    "[ 30, 5, 5, 640, 2560, 144, 235.1, 3.3597,+3.0526, 19600], # 2.1h\n",
    "[ 30, 5, 5, 640, 2560, 128, 229.0, 3.3500, 3.0618, 19600], # 1.3h\n",
    "[ 30, 5, 5, 640, 2560, 112, 222.8, 3.4009, 3.0617, 19600], # 1.3h\n",
    "[ 30, 5, 5, 640, 2560,  96, 216.7, 3.4304, 3.0598, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 5, 620, 2480, 192, 241.2, 3.3452, 3.0561, 19600], # 2.3h\n",
    "[ 30, 5, 5, 620, 2480, 144, 223.3, 3.4249, 3.0594, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 5, 600, 2400, 256, 252.1, 3.3228, 3.0617, 19600], # 2.5h\n",
    "[ 30, 5, 5, 600, 2400, 224, 240.6, 3.3396, 3.0589, 19600], # 2.4h\n",
    "[ 30, 5, 5, 600, 2400, 208, 234.8, 3.3405, 3.0592, 19600], # 2.3h\n",
    "[ 30, 5, 5, 600, 2400, 192, 229.1, 3.3347,+3.0530, 19600], # 2.2h\n",
    "[ 30, 5, 5, 600, 2400, 176, 223.3, 3.3384, 3.0562, 19600], # 2.2h\n",
    "[ 30, 5, 5, 600, 2400, 160, 217.6, 3.3287, 3.0621, 19600], # 2.1h\n",
    "[ 30, 5, 5, 600, 2400, 144, 211.8, 3.4175,+3.0531, 19600], # 2.0h\n",
    "[ 30, 5, 5, 600, 2400, 128, 206.0, 3.3877, 3.0670, 19600], # 1.3h\n",
    "[ 30, 5, 5, 600, 2400, 112, 200.3, 3.3393, 3.0653, 19600], # 1.3h\n",
    "[ 30, 5, 5, 600, 2400,  96, 194.5, 3.3596, 3.0706, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 5, 580, 2320, 192, 217.3, 3.3805, 3.0620, 19600], # 2.3h\n",
    "[ 30, 5, 5, 580, 2320, 144, 200.6, 3.3160, 3.0621, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 5, 560, 2240, 256, 227.2, 3.3053, 3.0595, 19600], # 2.4h\n",
    "[ 30, 5, 5, 560, 2240, 224, 216.5, 3.3893, 3.0612, 19600], # 2.3h\n",
    "[ 30, 5, 5, 560, 2240, 192, 205.7, 3.3257, 3.0620, 19600], # 2.1h\n",
    "[ 30, 5, 5, 560, 2240, 176, 200.4, 3.3239, 3.0611, 19600], # 2.1h\n",
    "[ 30, 5, 5, 560, 2240, 144, 189.6, 3.3348, 3.0660, 19600], # 1.9h\n",
    "[ 30, 5, 5, 560, 2240, 128, 184.2, 3.4130, 3.0675, 19600], # 1.2h\n",
    "[ 30, 5, 5, 560, 2240, 112, 178.9, 3.3855, 3.0773, 19600], # 1.2h\n",
    "[ 30, 5, 5, 560, 2240,  96, 173.5, 3.3408, 3.0669, 19600], # 1.1h\n",
    "\n",
    "[ 30, 5, 5, 520, 2080, 256, 203.5, 3.3369, 3.0670, 19600], # 2.3h\n",
    "[ 30, 5, 5, 520, 2080, 224, 193.5, 3.2985, 3.0669, 19600], # 2.2h\n",
    "[ 30, 5, 5, 520, 2080, 192, 183.6, 3.2937, 3.0658, 19600], # 2.1h\n",
    "[ 30, 5, 5, 520, 2080, 176, 178.6, 3.3091, 3.0718, 19600], # 2.0h\n",
    "[ 30, 5, 5, 520, 2080, 144, 168.6, 3.3362, 3.0766, 19600], # 1.9h\n",
    "[ 30, 5, 5, 520, 2080, 128, 163.6, 3.2943, 3.0791, 19600], # 1.1h\n",
    "[ 30, 5, 5, 520, 2080, 112, 158.6, 3.3111, 3.0767, 19600], # 1.1h\n",
    "[ 30, 5, 5, 520, 2080,  96, 153.6, 3.3091, 3.0823, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 5, 1, 760, 3040, 288, 325.2, 3.4164, 3.0626, 19600], # 2.9h\n",
    "[ 30, 5, 1, 760, 3040, 256, 316.4, 3.5018, 3.0587, 19600], # 2.7h\n",
    "[ 30, 5, 1, 760, 3040, 224, 307.7, 3.4642, 3.0611, 19600], # 2.6h\n",
    "[ 30, 5, 1, 760, 3040, 192, 298.9, 3.6474, 3.0768, 19600], # 2.5h\n",
    "[ 30, 5, 1, 760, 3040, 160, 290.2, 3.3903, 3.0561, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 1, 740, 2960, 320, 319.8, 3.3693, 3.0632, 19600], # 3.0h\n",
    "[ 30, 5, 1, 740, 2960, 288, 311.3, 3.3513, 3.0565, 19600], # 2.9h\n",
    "[ 30, 5, 1, 740, 2960, 256, 302.8, 3.4523, 3.0587, 19600], # 2.7h\n",
    "\n",
    "[ 30, 5, 1, 720, 2880, 336, 310.1, 3.4626, 3.0556, 19600], # 3.0h\n",
    "[ 30, 5, 1, 720, 2880, 320, 306.0, 3.3628,+3.0522, 19600], # 2.9h\n",
    "[ 30, 5, 1, 720, 2880, 304, 301.8, 3.4707, 3.0615, 19600], # 2.9h\n",
    "[ 30, 5, 1, 720, 2880, 288, 297.7, 3.3806,+3.0498, 19600], # 2.8h  minGemma-hidden_layers30-att_heads5-kv_heads1-hidden720-intermediate2880-head_dim288-T512--2025-06-15-19-41.pth\n",
    "[ 30, 5, 1, 720, 2880, 272, 293.5, 3.3816, 3.0603, 19600], # 2.8h\n",
    "[ 30, 5, 1, 720, 2880, 256, 289.4, 3.3950,+3.0520, 19600], # 2.6h\n",
    "[ 30, 5, 1, 720, 2880, 240, 285.3, 3.3593, 3.0560, 19600],\n",
    "[ 30, 5, 1, 720, 2880, 224, 281.1, 3.5347, 3.0639, 19600], # 2.5h\n",
    "[ 30, 5, 1, 720, 2880, 192, 272.8, 3.3698, 3.0584, 19600], # 2.4h\n",
    "[ 30, 5, 1, 720, 2880, 160, 264.5, 3.3863, 3.0568, 19600], # 2.3h\n",
    "[ 30, 5, 1, 720, 2880, 128, 256.2, 3.3940, 3.0590, 19600], # 2.0h\n",
    "[ 30, 5, 1, 720, 2880,  96, 247.9, 3.5566, 3.0732, 19600], # 1.8h\n",
    "\n",
    "[ 30, 5, 1, 700, 2800, 320, 292.5, 3.3813, 3.0546, 19600], # 2.9h\n",
    "[ 30, 5, 1, 700, 2800, 288, 284.4, 3.4136, 3.0585, 19600], # 2.8h\n",
    "[ 30, 5, 1, 700, 2800, 272, 280.4, 3.3201,+3.0525, 19600], # 2.8h\n",
    "[ 30, 5, 1, 700, 2800, 256, 276.3, 3.3776,+3.0532, 19600], # 2.6h\n",
    "[ 30, 5, 1, 700, 2800, 240, 272.3, 3.3686, 3.0561, 19600], # 2.6h\n",
    "[ 30, 5, 1, 700, 2800, 176, 256.2, 3.3420, 3.0560, 19600], # 2.4h\n",
    "\n",
    "[ 30, 5, 1, 680, 2720, 288, 271.4, 3.4339, 3.0568, 19600],\n",
    "[ 30, 5, 1, 680, 2720, 272, 267.4, 3.3299,+3.0501, 19600], # 2.7h 17.0G\n",
    "[ 30, 5, 1, 680, 2720, 256, 263.5, 3.3431,+3.0532, 19600], # 2.5h 17.0G\n",
    "[ 30, 5, 1, 680, 2720, 240, 259.6, 3.3514, 3.0536, 19600], # 2.5h\n",
    "[ 30, 5, 1, 680, 2720, 224, 255.7, 3.3404, 3.0575, 19600], # 2.4h\n",
    "[ 30, 5, 1, 680, 2720, 192, 247.9, 3.3513, 3.0585, 19600], # 2.3h\n",
    "[ 30, 5, 1, 680, 2720, 176, 243.9, 3.3583, 3.0535, 19600], # 2.3h\n",
    "[ 30, 5, 1, 680, 2720, 160, 240.0, 3.4630, 3.0580, 19600], # 2.2h\n",
    "[ 30, 5, 1, 680, 2720, 128, 232.2, 3.4498, 3.0575, 19600], # 1.9h\n",
    "[ 30, 5, 1, 680, 2720,  96, 224.4, 3.3838, 3.0607, 19600], # 1.7h\n",
    "\n",
    "[ 30, 5, 1, 660, 2640, 272, 254.8, 3.3152, 3.0546, 19600],\n",
    "[ 30, 5, 1, 660, 2640, 256, 251.0, 3.3693, 3.0546, 19600], # 2.5h\n",
    "[ 30, 5, 1, 660, 2640, 176, 232.0, 3.4496, 3.0584, 19600], # 2.3h\n",
    "\n",
    "[ 30, 5, 1, 640, 2560, 272, 242.5, 3.3231, 3.0550, 19600], # 2.5h\n",
    "[ 30, 5, 1, 640, 2560, 256, 238.8, 3.3305, 3.0584, 19600], # 2.3h\n",
    "[ 30, 5, 1, 640, 2560, 224, 231.4, 3.3544, 3.0561, 19600], # 2.2h\n",
    "[ 30, 5, 1, 640, 2560, 192, 224.1, 3.3839, 3.0591, 19600], # 2.1h 15.7G\n",
    "[ 30, 5, 1, 640, 2560, 176, 220.4, 3.3457,+3.0532, 19600], # 2.0h\n",
    "[ 30, 5, 1, 640, 2560, 160, 216.7, 3.3194, 3.0593, 19600], # 1.6h 15.2G\n",
    "[ 30, 5, 1, 640, 2560, 144, 213.0, 3.3429, 3.0671, 19600], # 1.6h\n",
    "[ 30, 5, 1, 640, 2560, 128, 209.3, 3.3543, 3.0632, 19600], # 1.3h\n",
    "[ 30, 5, 1, 640, 2560, 112, 205.6, 3.3628, 3.0705, 19600], # 1.3h\n",
    "[ 30, 5, 1, 640, 2560,  96, 202.0, 3.3768, 3.0705, 19600], # 1.2h\n",
    "\n",
    "[ 30, 5, 1, 620, 2480, 176, 209.0, 3.3276, 3.0621, 19600], # 2.1h\n",
    "\n",
    "[ 30, 5, 1, 600, 2400, 256, 215.2, 3.2990, 3.0638, 19600], # 2.2h\n",
    "[ 30, 5, 1, 600, 2400, 224, 208.3, 3.3282, 3.0570, 19600], # 2.1h\n",
    "[ 30, 5, 1, 600, 2400, 192, 201.4, 3.3335, 3.0591, 19600], # 2.0h\n",
    "[ 30, 5, 1, 600, 2400, 176, 198.0, 3.3205, 3.0565, 19600], # 2.0h\n",
    "[ 30, 5, 1, 600, 2400, 160, 194.5, 3.3683, 3.0618, 19600], # 1.9h\n",
    "[ 30, 5, 1, 600, 2400, 144, 191.1, 3.3659, 3.0629, 19600], # 1.5h\n",
    "[ 30, 5, 1, 600, 2400, 128, 187.6, 3.3189, 3.0649, 19600], # 1.2h\n",
    "[ 30, 5, 1, 600, 2400, 112, 184.1, 3.3267, 3.0677, 19600], # 1.2h\n",
    "[ 30, 5, 1, 600, 2400,  96, 180.7, 3.3310, 3.0822, 19600], # 1.1h\n",
    "\n",
    "[ 30, 5, 1, 560, 2240, 256, 192.8, 3.4028, 3.0589, 19600], # 2.1h\n",
    "[ 30, 5, 1, 560, 2240, 224, 186.4, 3.3118, 3.0637, 19600], # 2.0h\n",
    "[ 30, 5, 1, 560, 2240, 192, 179.9, 3.2912, 3.0641, 19600], # 1.9h\n",
    "[ 30, 5, 1, 560, 2240, 160, 173.5, 3.4181, 3.0673, 19600], # 1.8h\n",
    "[ 30, 5, 1, 560, 2240, 144, 170.3, 3.3355, 3.0689, 19600], # 1.5h\n",
    "[ 30, 5, 1, 560, 2240, 128, 167.0, 3.2986, 3.0749, 19600], # 1.1h\n",
    "[ 30, 5, 1, 560, 2240, 112, 163.8, 3.3302, 3.0713, 19600], # 1.1h\n",
    "[ 30, 5, 1, 560, 2240,  96, 160.6, 3.3252, 3.0808, 19600], # 1.0h\n",
    "\n",
    "[ 30, 5, 1, 520, 2080, 256, 171.6, 3.2779, 3.0708, 19600], # 2.1h\n",
    "[ 30, 5, 1, 520, 2080, 224, 165.6, 3.2791, 3.0686, 19600], # 2.0h\n",
    "[ 30, 5, 1, 520, 2080, 192, 159.6, 3.3124, 3.0675, 19600], # 1.9h 14.5G\n",
    "[ 30, 5, 1, 520, 2080, 160, 153.6, 3.3038, 3.0811, 19600], # 1.8h\n",
    "[ 30, 5, 1, 520, 2080, 144, 150.6, 3.2904, 3.0879, 19600], # 1.4h\n",
    "[ 30, 5, 1, 520, 2080, 128, 147.6, 3.2620, 3.0870, 19600], # 1.1h\n",
    "[ 30, 5, 1, 520, 2080, 112, 144.6, 3.3155, 3.0899, 19600], # 1.1h\n",
    "[ 30, 5, 1, 520, 2080,  96, 141.6, 3.3356, 3.0786, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 4, 4, 744, 2976, 256, 328.3, 3.5070, 3.0617, 19600], # 2.7h\n",
    "[ 30, 4, 4, 744, 2976, 224, 316.9, 3.5518, 3.0663, 19600], # 2.6h\n",
    "[ 30, 4, 4, 744, 2976, 192, 305.5, 3.4527, 3.0577, 19600], # 2.4h\n",
    "[ 30, 4, 4, 744, 2976, 160, 294.0, 3.5288, 3.0617, 19600], # 2.3h\n",
    "[ 30, 4, 4, 744, 2976, 128, 282.6, 3.4108, 3.0561, 19600], # 2.0h\n",
    "[ 30, 4, 4, 744, 2976,  96, 271.2, 3.4913, 3.0648, 19600], # 1.9h\n",
    "\n",
    "[ 30, 4, 4, 696, 2784, 256, 295.1, 3.4248, 3.0570, 19600], # 2.6h\n",
    "[ 30, 4, 4, 696, 2784, 224, 284.4, 3.4183, 3.0555, 19600], # 2.4h\n",
    "[ 30, 4, 4, 696, 2784, 192, 273.7, 3.4213, 3.0593, 19600],\n",
    "[ 30, 4, 4, 696, 2784, 160, 263.1, 3.4704, 3.0564, 19600], # 2.2h\n",
    "[ 30, 4, 4, 696, 2784, 128, 252.4, 3.4785, 3.0578, 19600], # 1.9h\n",
    "[ 30, 4, 4, 696, 2784,  96, 241.7, 3.4670, 3.0620, 19600], # 1.8h\n",
    "\n",
    "[ 30, 4, 4, 648, 2592, 288, 273.5, 3.4527, 3.0625, 19600], # 2.7h\n",
    "[ 30, 4, 4, 648, 2592, 272, 268.5, 3.3627, 3.0542, 19600], # 2.6h\n",
    "[ 30, 4, 4, 648, 2592, 256, 263.6, 3.3826, 3.0544, 19600], # 2.5h\n",
    "[ 30, 4, 4, 648, 2592, 240, 258.6, 3.3735, 3.0552, 19600], # 2.4h\n",
    "[ 30, 4, 4, 648, 2592, 224, 253.6, 3.3645, 3.0541, 19600], # 2.4h\n",
    "[ 30, 4, 4, 648, 2592, 208, 248.6, 3.3461, 3.0548, 19600], # 2.3h\n",
    "[ 30, 4, 4, 648, 2592, 192, 243.7, 3.3766,+3.0540, 19600], # 2.2h\n",
    "[ 30, 4, 4, 648, 2592, 176, 238.7, 3.4333, 3.0591, 19600], # 2.2h\n",
    "[ 30, 4, 4, 648, 2592, 144, 228.7, 3.3872, 3.0582, 19600], # 2.1h\n",
    "[ 30, 4, 4, 648, 2592, 128, 223.8, 3.3698, 3.0633, 19600], # 1.3h\n",
    "[ 30, 4, 4, 648, 2592, 112, 218.8, 3.3654, 3.0600, 19600], # 1.3h\n",
    "[ 30, 4, 4, 648, 2592,  96, 213.8, 3.4010, 3.0700, 19600], # 1.2h\n",
    "\n",
    "[ 30, 4, 4, 600, 2400, 256, 233.7, 3.3319, 3.0594, 19600], # 2.2h\n",
    "[ 30, 4, 4, 600, 2400, 224, 224.5, 3.3309, 3.0574, 19600], # 2.2h\n",
    "[ 30, 4, 4, 600, 2400, 192, 215.2, 3.3723, 3.0591, 19600], # 2.0h\n",
    "[ 30, 4, 4, 600, 2400, 160, 206.0, 3.3955, 3.0641, 19600], # 1.9h\n",
    "[ 30, 4, 4, 600, 2400, 144, 201.4, 3.3313, 3.0638, 19600], # 1.5h\n",
    "[ 30, 4, 4, 600, 2400, 128, 196.8, 3.3538, 3.0690, 19600], # 1.2h\n",
    "[ 30, 4, 4, 600, 2400, 112, 192.2, 3.3520, 3.0625, 19600], # 1.2h\n",
    "[ 30, 4, 4, 600, 2400,  96, 187.6, 3.3686, 3.0737, 19600], # 1.1h\n",
    "\n",
    "[ 30, 4, 4, 552, 2208, 256, 205.4, 3.3888, 3.0657, 19600], # 2.1h\n",
    "[ 30, 4, 4, 552, 2208, 224, 197.0, 3.3201, 3.0626, 19600], # 2.0h\n",
    "[ 30, 4, 4, 552, 2208, 192, 188.5, 3.3213, 3.0679, 19600], # 1.9h\n",
    "[ 30, 4, 4, 552, 2208, 160, 180.0, 3.3181, 3.0645, 19600], # 1.8h\n",
    "[ 30, 4, 4, 552, 2208, 144, 175.8, 3.2948, 3.0709, 19600], # 1.4h\n",
    "[ 30, 4, 4, 552, 2208, 128, 171.5, 3.3072, 3.0667, 19600], # 1.1h\n",
    "[ 30, 4, 4, 552, 2208, 112, 167.3, 3.3141, 3.0779, 19600], # 1.1h\n",
    "[ 30, 4, 4, 552, 2208,  96, 163.1, 3.3914, 3.0784, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 4, 2, 768, 3072, 176, 299.8, 3.4905, 3.0542, 19600], # 2.3h\n",
    "\n",
    "[ 30, 4, 2, 744, 2976, 256, 305.5, 3.5984, 3.0655, 19600], # 2.6h\n",
    "[ 30, 4, 2, 744, 2976, 224, 296.9, 3.4216, 3.0547, 19600], # 2.5h\n",
    "[ 30, 4, 2, 744, 2976, 192, 288.3, 3.4492, 3.0541, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 176, 284.1, 3.3963,+3.0526, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 160, 279.8, 3.4147, 3.0579, 19600], # 2.3h\n",
    "[ 30, 4, 2, 744, 2976, 144, 275.5, 3.3991, 3.0551, 19600], # 2.2h\n",
    "[ 30, 4, 2, 744, 2976, 128, 271.2, 3.4006, 3.0539, 19600], # 2.0h\n",
    "[ 30, 4, 2, 744, 2976,  96, 262.6, 3.4483, 3.0652, 19600], # 1.8h\n",
    "\n",
    "[ 30, 4, 2, 720, 2880, 288, 297.7, 3.4547, 3.0581, 19600], # 2.7h 17.0G\n",
    "[ 30, 4, 2, 720, 2880, 272, 293.5, 3.4387, 3.0562, 19600], # 2.6h\n",
    "[ 30, 4, 2, 720, 2880, 256, 289.4, 3.5357, 3.0568, 19600], # 2.5h\n",
    "[ 30, 4, 2, 720, 2880, 240, 285.3, 3.4114, 3.0535, 19600], # 2.5h\n",
    "[ 30, 4, 2, 720, 2880, 224, 281.1, 3.3812, 3.0547, 19600], # 2.4h\n",
    "[ 30, 4, 2, 720, 2880, 208, 277.0, 3.5865,+3.0529, 19600], # 2.4h\n",
    "[ 30, 4, 2, 720, 2880, 192, 272.8, 3.4129, 3.0553, 19600], # 2.3h 16.3G\n",
    "[ 30, 4, 2, 720, 2880, 160, 264.5, 3.4269, 3.0572, 19600], # 2.2h\n",
    "[ 30, 4, 2, 720, 2880, 144, 260.4, 3.4326, 3.0532, 19600], # 2.2h\n",
    "\n",
    "[ 30, 4, 2, 696, 2784, 256, 273.7, 3.4693, 3.0546, 19600], # 2.4h\n",
    "[ 30, 4, 2, 696, 2784, 224, 265.7, 3.3820, 3.0544, 19600], # 2.3h\n",
    "[ 30, 4, 2, 696, 2784, 208, 261.7, 3.4757, 3.0571, 19600], # 2.3h 16.2G\n",
    "[ 30, 4, 2, 696, 2784, 192, 257.7, 3.4217,+3.0518, 19600], # 2.2h\n",
    "[ 30, 4, 2, 696, 2784, 176, 253.7, 3.3886,+3.0505, 19600], # 2.2h\n",
    "[ 30, 4, 2, 696, 2784, 160, 249.7, 3.4924, 3.0585, 19600], # 2.1h\n",
    "[ 30, 4, 2, 696, 2784, 128, 241.7, 3.5089, 3.0620, 19600], # 1.9h 16.0G\n",
    "[ 30, 4, 2, 696, 2784,  96, 233.7, 3.4015, 3.0658, 19600], # 1.7h\n",
    "\n",
    "[ 30, 4, 2, 672, 2688, 288, 266.2, 3.3754,+3.0522, 19600], # 2.5h\n",
    "[ 30, 4, 2, 672, 2688, 272, 262.4, 3.3835, 3.0531, 19600], # 2.5h\n",
    "[ 30, 4, 2, 672, 2688, 256, 258.5, 3.3646, 3.0522, 19600], # 2.4h\n",
    "[ 30, 4, 2, 672, 2688, 240, 254.6, 3.3848,+3.0494, 19600], # 2.3h 16.1G # 2nd BEST\n",
    "[ 30, 4, 2, 672, 2688, 240, 254.6, 3.3810, 3.0509, 19600], # again\n",
    "[ 30, 4, 2, 672, 2688, 224, 250.8, 3.4306, 3.0564, 19600], # 2.3h 16.2G\n",
    "[ 30, 4, 2, 672, 2688, 208, 246.9, 3.4401, 3.0532, 19600], # 2.2h\n",
    "[ 30, 4, 2, 672, 2688, 192, 243.0, 3.4030, 3.0552, 19600], # 2.2h\n",
    "[ 30, 4, 2, 672, 2688, 160, 235.3, 3.5064, 3.0621, 19600], # 2.1h\n",
    "[ 30, 4, 2, 672, 2688, 144, 231.4, 3.4365, 3.0534, 19600], # 2.0h\n",
    "\n",
    "[ 30, 4, 2, 648, 2592, 256, 243.7, 3.5282, 3.0651, 19600], # 2.3h\n",
    "[ 30, 4, 2, 648, 2592, 240, 239.9, 3.3948, 3.0552, 19600], # 2.3h\n",
    "[+30, 4, 2, 648, 2592, 224, 236.2, 3.3476,+3.0493, 19600], # 2.3h 16.7G  minGemma-hidden_layers30-att_heads4-kv_heads2-hidden648-intermediate2592-head_dim224-T512--2025-06-07-15-50.pth\n",
    "[ 30, 4, 2, 648, 2592, 224, 236.2, 3.4655, 3.0500, 19600], # again\n",
    "[ 30, 4, 2, 648, 2592, 208, 232.5, 3.3631,+3.0526, 19600], # 2.2h\n",
    "[ 30, 4, 2, 648, 2592, 192, 228.7, 3.3444,+3.0529, 19600], # 2.2h\n",
    "[ 30, 4, 2, 648, 2592, 176, 225.0, 3.3478, 3.0550, 19600], # 14.9h\n",
    "[ 30, 4, 2, 648, 2592, 144, 217.5, 3.3630, 3.0581, 19600], # 1.6h\n",
    "[ 30, 4, 2, 648, 2592, 128, 213.8, 3.3636, 3.0593, 19600], # 1.3h\n",
    "[ 30, 4, 2, 648, 2592, 112, 210.1, 3.3532, 3.0654, 19600], # 1.3h\n",
    "[ 30, 4, 2, 648, 2592,  96, 206.3, 3.3662, 3.0704, 19600], # 1.2h\n",
    "\n",
    "[ 30, 4, 2, 624, 2496, 288, 236.4, 3.3387, 3.0555, 19600], # 2.3h\n",
    "[ 30, 4, 2, 624, 2496, 272, 232.8, 3.3566, 3.0572, 19600], # 2.3h\n",
    "[ 30, 4, 2, 624, 2496, 256, 229.3, 3.3961,+3.0519, 19600], # 2.2h\n",
    "[ 30, 4, 2, 624, 2496, 240, 225.7, 3.3466, 3.0565, 19600], # 2.1h\n",
    "[ 30, 4, 2, 624, 2496, 224, 222.1, 3.3535, 3.0603, 19600], # 2.1h\n",
    "[ 30, 4, 2, 624, 2496, 208, 218.5, 3.3998, 3.0553, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 192, 214.9, 3.3619, 3.0551, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 176, 211.3, 3.4264, 3.0626, 19600], # 2.0h\n",
    "[ 30, 4, 2, 624, 2496, 160, 207.7, 3.3471,+3.0529, 19600], # 1.9h\n",
    "[ 30, 4, 2, 624, 2496, 144, 204.1, 3.3446, 3.0577, 19600], # 1.9h\n",
    "\n",
    "[ 30, 4, 2, 600, 2400, 272, 218.7, 3.3463, 3.0565, 19600], # 2.3h\n",
    "[ 30, 4, 2, 600, 2400, 256, 215.2, 3.3483, 3.0567, 19600], # 2.1h\n",
    "[ 30, 4, 2, 600, 2400, 240, 211.8, 3.3763,+3.0527, 19600], # 2.1h\n",
    "[ 30, 4, 2, 600, 2400, 224, 208.3, 3.3332, 3.0601, 19600], # 1.6h\n",
    "[ 30, 4, 2, 600, 2400, 192, 201.4, 3.3268, 3.0593, 19600], # 1.5h\n",
    "[ 30, 4, 2, 600, 2400, 160, 194.5, 3.3324, 3.0613, 19600], # 1.5h\n",
    "[ 30, 4, 2, 600, 2400, 128, 187.6, 3.3435, 3.0633, 19600], # 1.2h\n",
    "[ 30, 4, 2, 600, 2400, 112, 184.1, 3.4804, 3.0720, 19600], # 1.2h\n",
    "[ 30, 4, 2, 600, 2400,  96, 180.7, 3.4408, 3.0762, 19600], # 1.1h\n",
    "\n",
    "[ 30, 4, 2, 576, 2304, 240, 198.3, 3.3021, 3.0568, 19600], # 2.0h\n",
    "\n",
    "[ 30, 4, 2, 552, 2208, 288, 194.8, 3.3074, 3.0585, 19600], # 2.2h 15.1G\n",
    "[ 30, 4, 2, 552, 2208, 256, 188.5, 3.3132, 3.0607, 19600], # 2.0h\n",
    "[ 30, 4, 2, 552, 2208, 240, 185.3, 3.3064, 3.0645, 19600], # 1.6h\n",
    "[ 30, 4, 2, 552, 2208, 208, 179.0, 3.2810, 3.0668, 19600],\n",
    "[ 30, 4, 2, 552, 2208, 176, 172.6, 3.2951, 3.0700, 19600], # 1.4h\n",
    "[ 30, 4, 2, 552, 2208, 144, 166.2, 3.3499, 3.0692, 19600], # 1.4h\n",
    "[ 30, 4, 2, 552, 2208, 128, 163.1, 3.3455, 3.0768, 19600], # 1.1h\n",
    "[ 30, 4, 2, 552, 2208, 112, 159.9, 3.3103, 3.0785, 19600], # 1.1h\n",
    "[ 30, 4, 2, 552, 2208,  96, 156.7, 3.3366, 3.0854, 19600], # 1.0h\n",
    "\n",
    "\n",
    "[ 30, 3, 3, 744, 2976, 256, 305.5, 3.4562, 3.0603, 19600], # 2.4h\n",
    "[ 30, 3, 3, 744, 2976, 224, 296.9, 3.4309, 3.0558, 19600], # 2.3h\n",
    "[ 30, 3, 3, 744, 2976, 192, 288.3, 3.4363, 3.0584, 19600], # 2.2h\n",
    "[ 30, 3, 3, 744, 2976, 160, 279.8, 3.4955, 3.0554, 19600], # 2.2h\n",
    "[ 30, 3, 3, 744, 2976, 128, 271.2, 3.4472, 3.0570, 19600], # 1.9h\n",
    "[ 30, 3, 3, 744, 2976,  96, 262.6, 3.4764, 3.0643, 19600], # 1.8h\n",
    "\n",
    "[ 30, 3, 3, 696, 2784, 272, 277.8, 3.4326, 3.0586, 19600], # 2.4h\n",
    "[ 30, 3, 3, 696, 2784, 256, 273.7, 3.4425,+3.0542, 19600], # 2.3h 16.7G\n",
    "[ 30, 3, 3, 696, 2784, 240, 269.7, 3.4506, 3.0576, 19600], # 2.3h\n",
    "[ 30, 3, 3, 696, 2784, 224, 265.7, 3.3916, 3.0597, 19600], # 2.2h\n",
    "[ 30, 3, 3, 696, 2784, 192, 257.7, 3.4272, 3.0578, 19600], # 2.1h\n",
    "[ 30, 3, 3, 696, 2784, 160, 249.7, 3.4733, 3.0598, 19600], # 2.0h\n",
    "[ 30, 3, 3, 696, 2784, 128, 241.7, 3.4498, 3.0583, 19600], # 1.8h\n",
    "[ 30, 3, 3, 696, 2784,  96, 233.7, 3.4212, 3.0630, 19600], # 1.7h\n",
    "\n",
    "[ 30, 3, 3, 648, 2592, 256, 243.7, 3.4881, 3.0622, 19600], # 2.2h\n",
    "[ 30, 3, 3, 648, 2592, 224, 236.2, 3.4027, 3.0580, 19600], # 2.1h\n",
    "[ 30, 3, 3, 648, 2592, 208, 232.5, 3.4521, 3.0634, 19600], # 1.6h\n",
    "[ 30, 3, 3, 648, 2592, 176, 225.0, 3.4189, 3.0618, 19600], # 1.5h 15.3G\n",
    "[ 30, 3, 3, 648, 2592, 144, 217.5, 3.3768, 3.0614, 19600], # 1.5h 15.5G\n",
    "[ 30, 3, 3, 648, 2592, 128, 213.8, 3.3747, 3.0598, 19600], # 1.3h\n",
    "[ 30, 3, 3, 648, 2592, 112, 210.1, 3.4036, 3.0695, 19600], # 1.3h\n",
    "[ 30, 3, 3, 648, 2592,  96, 206.3, 3.4314, 3.0673, 19600], # 1.2h\n",
    "\n",
    "[ 30, 3, 3, 600, 2400, 256, 215.2, 3.3999, 3.0619, 19600], # 2.0h 15.8G\n",
    "[ 30, 3, 3, 600, 2400, 224, 208.3, 3.5221, 3.0700, 19600], # 1.9h \n",
    "[ 30, 3, 3, 600, 2400, 208, 204.9, 3.3432, 3.0638, 19600], # 1.5h 14.9G\n",
    "[ 30, 3, 3, 600, 2400, 176, 198.0, 3.3879, 3.0648, 19600], # 1.4h 14.8G\n",
    "[ 30, 3, 3, 600, 2400, 144, 191.1, 3.3947, 3.0616, 19600], # 1.3h\n",
    "[ 30, 3, 3, 600, 2400, 128, 187.6, 3.4213, 3.0613, 19600], # 1.1h\n",
    "[ 30, 3, 3, 600, 2400, 112, 184.1, 3.3592, 3.0644, 19600], # 1.1h 14.4G\n",
    "[ 30, 3, 3, 600, 2400,  96, 180.7, 3.5229, 3.0751, 19600], # 1.1h\n",
    "\n",
    "[ 30, 3, 3, 552, 2208, 256, 188.5, 3.3234, 3.0605, 19600], # 1.9h\n",
    "[ 30, 3, 3, 552, 2208, 224, 182.1, 3.3561, 3.0695, 19600], # 1.8h\n",
    "[ 30, 3, 3, 552, 2208, 208, 179.0, 3.3546, 3.0735, 19600], # 1.4h 14.1G\n",
    "[ 30, 3, 3, 552, 2208, 176, 172.6, 3.3689, 3.0672, 19600], # 1.3h\n",
    "[ 30, 3, 3, 552, 2208, 144, 166.2, 3.3299, 3.0734, 19600], # 1.3h\n",
    "[ 30, 3, 3, 552, 2208, 128, 163.1, 3.3513, 3.0766, 19600], # 1.1h\n",
    "[ 30, 3, 3, 552, 2208, 112, 159.9, 3.3357, 3.0823, 19600], # 1.0h\n",
    "[ 30, 3, 3, 552, 2208,  96, 156.7, 3.3515, 3.0824, 19600], # 1.0h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f7c48899-34f2-4c71-8b26-bf979ee6a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att4 kv_heads2 hidden648 intermediate2592 head_dim224 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:18:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.347600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 4, 2, 648, 2592, 224, 236.2, 3.3476, 3.0493, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 224 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c6b0941b-a43f-4297-98e2-182bee855d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att5 kv_heads1 hidden720 intermediate2880 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:52:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.380600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 5, 1, 720, 2880, 288, 297.7, 3.3806, 3.0498, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 5 # 4 # G16 G8\n",
    "num_key_value_heads = 1 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*144 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)\n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49bda719-6f20-44e2-b508-509c5d9b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L30 att6 kv_heads3 hidden720 intermediate2880 head_dim208 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:50:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.404700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30, 6, 3, 720, 2880, 208, 303.9, 3.4047, 3.0500, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  30 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*120 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 208 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0493:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a744e-9d7f-4591-b105-1af96ef79b69",
   "metadata": {},
   "source": [
    "# L28 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3078f96-fed1-4f36-b64a-ed357b010064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L28 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 28, 6, 3, 672, 2688, 192, 250.7, 3.4478, 3.0530, 19600], # 1.9h 15.4G\n",
    "[ 28, 6, 3, 672, 2688, 160, 239.9, 3.3624, 3.0524, 19600], # 1.8h 15.3G\n",
    "[ 28, 6, 3, 672, 2688, 128, 229.1, 3.3520, 3.0544, 19600], # 1.4h\n",
    "[+28, 6, 3, 648, 2592, 240, 252.2, 3.3411,+3.0502, 19600], # 2.1h 15.6G\n",
    "[ 28, 6, 3, 648, 2592, 224, 247.0, 3.3187, 3.0566, 19600], # 2.0h 15.6G\n",
    "[ 28, 6, 3, 648, 2592, 160, 226.1, 3.3505, 3.0544, 19600], # 1.8h\n",
    "[ 28, 6, 3, 648, 2592, 128, 215.7, 3.4224, 3.0643, 19600], # 1.3h\n",
    "[ 28, 6, 3, 648, 2592,  96, 205.2, 3.3543, 3.0629, 19600], # 1.2h\n",
    "[ 28, 6, 3, 612, 2448, 192, 216.0, 3.3135, 3.0522, 19600], # 1.8h\n",
    "[ 28, 6, 3, 576, 2304, 192, 196.3, 3.3502, 3.0646, 19600], # 1.7h\n",
    "[ 28, 4, 2, 648, 2592, 240, 226.1, 3.3823,+3.0515, 19600], # 1.7h 15.2G\n",
    "[ 28, 4, 2, 648, 2592, 224, 222.6, 3.4075, 3.0567, 19600], # 1.7h 15.1G\n",
    "[ 28, 4, 2, 616, 2464, 240, 208.3, 3.3324, 3.0567, 19600], # 1.6h 14.8G\n",
    "[ 28, 3, 3, 648, 2592, 240, 226.1, 3.4360, 3.0578, 19600], # 1.6h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a7e224-acbc-4ef2-99f3-9cdd2cd3e946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\anaconda3\\envs\\minGemma\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L28 att6 kv_heads3 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miura_lab2\\AppData\\Local\\Temp\\ipykernel_6572\\1931068396.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:06:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.341100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28, 6, 3, 648, 2592, 240, 252.2, 3.3411, 3.0502, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  28 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 6 # 4 # G16 G8\n",
    "num_key_value_heads = 3 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*108 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0502:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7e4813-54ac-42da-bb70-8bb733e58ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L28 att4 kv_heads2 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:44:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.382300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28, 4, 2, 648, 2592, 240, 226.1, 3.3823, 3.0515, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  28 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 2 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Linear Single Epoch # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0502:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0be87-cc3e-4021-abe8-e405467921f8",
   "metadata": {},
   "source": [
    "# L26 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759f375-5189-43dd-9baf-fcbd8b9dfc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L26 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 26, 6, 3, 648, 2592, 240, 236.5, 3.3565, 3.0574, 19600], # 1.9h\n",
    "[+26, 4, 4, 648, 2592, 240, 228.5, 3.3902,+3.0504, 19600], # 1.7h\n",
    "[ 26, 4, 2, 648, 2592, 240, 212.3, 3.3869, 3.0558, 19600], # 1.6h\n",
    "[ 26, 3, 3, 648, 2592, 240, 212.3, 3.4225, 3.0616, 19600], # 1.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d253e43-998d-4917-8f38-f96c091c7dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L26 att4 kv_heads4 hidden648 intermediate2592 head_dim240 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:42:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.390200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 26, 4, 4, 648, 2592, 240, 228.5, 3.3902, 3.0504, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  26 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 4 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*162 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 240 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0504:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338fd481-14c7-455b-97cd-c15daba1d1ac",
   "metadata": {},
   "source": [
    "# L20 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94cbb8d-c12c-4b9f-8f56-772bca57136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L20 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 20, 8, 4, 928, 3712, 128, 310.5, 3.5399, 3.0653, 19600], # 2.0h\n",
    "[ 20, 8, 4, 864, 3456, 128, 275.9, 3.5633, 3.0599, 19600], # 1.5h\n",
    "[ 20, 8, 4, 800, 3200, 128, 243.1, 3.5120, 3.0582, 19600], # 1.4h\n",
    "[ 20, 8, 4, 848, 3392, 192, 293.5, 3.4913, 3.0596, 19600], # 20.9h\n",
    "[ 20, 8, 4, 800, 3200, 192, 267.7, 3.4751, 3.0610, 19600], # 1.9h\n",
    "[ 20, 8, 4, 736, 2944, 192, 235.0, 3.4224, 3.0582, 19600], # 1.7h\n",
    "[ 20, 8, 4, 704, 2816, 192, 219.4, 3.3811, 3.0561, 19600], # 1.6h\n",
    "[ 20, 8, 4, 768, 3072, 256, 274.7, 3.4261, 3.0589, 19600], # 2.0h\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.3451, 3.0546, 20800], # 2.0h\n",
    "[+20, 8, 4, 704, 2816, 272, 246.4, 3.4022,+3.0508, 19600], # 2.1h  minGemma-hidden_layers20-att_heads8-kv_heads4-hidden704-intermediate2816-head_dim272-T512--2025-05-03-23-24.pth\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.3796, 3.0536, 19600], # 1.9h\n",
    "[ 20, 8, 4, 704, 2816, 256, 241.0, 3.4142, 3.0549, 18400], # 1.8h\n",
    "[ 20, 8, 4, 640, 2560, 256, 209.3, 3.3627, 3.0575, 19600], # 1.8h\n",
    "\n",
    "[ 20, 6, 3, 792, 3168, 256, 263.5, 3.4414, 3.0612, 19600], # 1.9h 14.7G\n",
    "[ 20, 6, 3, 768, 3072, 240, 246.7, 3.4773, 3.0552, 19600], # 1.7h 14.2G\n",
    "[ 20, 6, 3, 696, 2784, 192, 199.5, 3.4094, 3.0554, 19600], # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df660d90-a0d1-4371-b4d7-c67b09bf98c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L20 att8 kv_heads4 hidden704 intermediate2816 head_dim272 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 2:08:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.402200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20, 8, 4, 704, 2816, 272, 246.4, 3.4022, 3.0508, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  20 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 272 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0508: #3.0484:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a810b8b-e27b-49b1-95c0-e5b6c9d0ada8",
   "metadata": {},
   "source": [
    "# L16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1600dad-6725-4a5d-b199-a0b098e77bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L16 Normal Model (default: B12 x4 lr13.5e-4 WD1)\n",
    "[ 16,12, 6, 696, 2784, 224, 217.9, 3.3786, 3.0553, 19600], # 1.9h 13.7G\n",
    "[ 16,12, 4, 696, 2784, 224, 207.9, 3.3824, 3.0603, 19600], # 1.8h 13.7G\n",
    "[ 16,12, 3, 696, 2784, 224, 202.9, 3.3565, 3.0611, 19600], # 1.8h\n",
    "[ 16,10, 5, 680, 2720, 224, 196.2, 3.3722, 3.0608, 19600], # 1.7h\n",
    "[ 16,10, 2, 680, 2720, 224, 181.6, 3.3138, 3.0634, 19600], # 1.6h\n",
    "[ 16,10, 1, 680, 2720, 224, 176.7, 3.3655, 3.0689, 19600], # 1.6h\n",
    "[ 16, 9, 9, 756, 3024, 256, 259.3, 3.4331, 3.0571, 19600], # 2.0h\n",
    "[ 16, 9, 9, 684, 2736, 224, 212.6, 3.3968, 3.0588, 19600], # 1.7h 14.0G\n",
    "\n",
    "[ 16, 9, 3, 792, 3168, 288, 248.0, 3.4878, 3.0605, 19600], # 2.1h\n",
    "[ 16, 9, 3, 792, 3168, 256, 238.2, 3.4353, 3.0605, 19600], # 1.9h\n",
    "[ 16, 9, 3, 792, 3168, 224, 228.5, 3.4456, 3.0610, 19600], # 1.7h\n",
    "[ 16, 9, 3, 792, 3168, 192, 218.8, 3.5141, 3.0611, 19600], # 1.6h\n",
    "[ 16, 9, 3, 792, 3168, 160, 209.0, 3.4412, 3.0646, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 304, 236.1, 3.4335, 3.0568, 19600], # 2.0h\n",
    "[ 16, 9, 3, 756, 3024, 288, 231.5, 3.4106,+3.0552, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 272, 226.8, 3.4141, 3.0610, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 256, 222.2, 3.4238, 3.0567, 19600], # 1.7h 13.3G\n",
    "[ 16, 9, 3, 756, 3024, 224, 212.9, 3.4077, 3.0592, 19600], # 1.6h\n",
    "[ 16, 9, 3, 756, 3024, 192, 203.6, 3.3903, 3.0588, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 160, 194.3, 3.4151, 3.0577, 19600], # 1.4h 13.1G\n",
    "[ 16, 9, 3, 720, 2880, 320, 224.3, 3.3521, 3.0561, 19600], # 2.0h\n",
    "[ 16, 9, 3, 720, 2880, 304, 219.9, 3.4003, 3.0572, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 288, 215.5, 3.3564,+3.0556, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 272, 211.0, 3.3431, 3.0589, 19600], # 1.8h\n",
    "[ 16, 9, 3, 720, 2880, 256, 206.6, 3.3858, 3.0575, 19600], # 1.7h\n",
    "[ 16, 9, 3, 720, 2880, 224, 197.8, 3.4157, 3.0565, 19600], # 1.6h\n",
    "[ 16, 9, 3, 720, 2880, 192, 188.9, 3.4725, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 720, 2880, 160, 180.1, 3.4023, 3.0613, 19600], # 1.3h\n",
    "[ 16, 9, 3, 702, 2808, 224, 190.4, 3.3924, 3.0624, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 288, 200.0, 3.3698, 3.0613, 19600], # 1.8h\n",
    "[ 16, 9, 3, 684, 2736, 256, 191.6, 3.3788, 3.0613, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 240, 187.4, 3.3933, 3.0585, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 3420, 224, 205.6, 3.4053, 3.0570, 19600], # 1.6h x5\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3528,+3.0538, 19600], # 1.5h # local min\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3641, 3.0562, 19600], # again\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3679,+3.0545, 19600], # again\n",
    "[ 16, 9, 3, 684, 2052, 224, 160.7, 3.3504, 3.0636, 19600], # 1.4h x3\n",
    "[ 16, 9, 3, 684, 2736, 208, 179.0, 3.4294, 3.0651, 19600], # 1.5h\n",
    "[ 16, 9, 3, 684, 2736, 192, 174.8, 3.3822, 3.0578, 19600], # 1.4h\n",
    "[ 16, 9, 3, 684, 2736, 160, 166.4, 3.3546, 3.0708, 19600], # 1.3h\n",
    "[ 16, 9, 3, 666, 2664, 224, 176.0, 3.3408, 3.0619, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 288, 185.0, 3.4168, 3.0575, 19600], # 1.8h\n",
    "[ 16, 9, 3, 648, 2592, 256, 177.0, 3.3494, 3.0597, 19600], # 1.6h\n",
    "[ 16, 9, 3, 648, 2592, 224, 169.0, 3.3636, 3.0596, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 208, 165.1, 3.3476, 3.0566, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 192, 161.1, 3.3674,+3.0552, 19600], # 1.4h\n",
    "[ 16, 9, 3, 648, 2592, 176, 157.1, 3.3518, 3.0571, 19600], # 1.3h\n",
    "[ 16, 9, 3, 648, 2592, 160, 153.1, 3.4071, 3.0621, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 288, 170.5, 3.3219, 3.0604, 19600], # 1.7h\n",
    "[ 16, 9, 3, 612, 2448, 256, 162.9, 3.3793, 3.0589, 19600], # 1.5h\n",
    "[ 16, 9, 3, 612, 2448, 224, 155.4, 3.3291, 3.0650, 19600], # 1.4h\n",
    "[ 16, 9, 3, 612, 2448, 192, 147.9, 3.3395, 3.0629, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 160, 140.4, 3.3432, 3.0705, 19600], # 1.2h\n",
    "[ 16, 9, 3, 576, 2304, 288, 156.5, 3.3139, 3.0602, 19600], # 1.7h\n",
    "[ 16, 9, 3, 576, 2304, 256, 149.4, 3.3302, 3.0645, 19600], # 1.5h\n",
    "[ 16, 9, 3, 576, 2304, 224, 142.3, 3.3453, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 576, 2304, 192, 135.2, 3.3139, 3.0647, 19600], # 1.3h\n",
    "[ 16, 9, 3, 576, 2304, 160, 128.1, 3.3197, 3.0732, 19600], # 1.2h\n",
    "\n",
    "[ 16, 9, 1, 684, 2736, 224, 173.4, 3.3315, 3.0629, 19600], # 1.5h\n",
    "\n",
    "[ 16, 8, 4, 960, 3840, 144, 278.4, 3.5831, 3.0678, 19600], # 1.7h\n",
    "[ 16, 8, 4, 912, 3648, 192, 272.9, 3.5625, 3.0690, 19600], # 1.8h\n",
    "[ 16, 8, 4, 896, 3584, 144, 248.9, 3.5718, 3.0678, 19600], # 1.5h\n",
    "[ 16, 8, 4, 880, 3520, 256, 279.6, 3.5187, 3.0604, 19600], # 1.9h\n",
    "[ 16, 8, 4, 864, 3456, 192, 250.6, 3.5129, 3.0600, 19600], # 1.6h\n",
    "[ 16, 8, 4, 832, 3328, 256, 256.7, 3.5653, 3.0695, 19600], # 1.8h\n",
    "[ 16, 8, 4, 832, 3328, 144, 220.9, 3.4968, 3.0645, 19600], # 1.4h\n",
    "[ 16, 8, 4, 800, 3200, 256, 241.9, 3.4717, 3.0610, 19600], # 1.7h\n",
    "[ 16, 8, 4, 800, 3200, 224, 232.0, 3.4724, 3.0602, 19600], # 1.6h\n",
    "[ 16, 8, 4, 800, 3200, 192, 222.2, 3.4555, 3.0601, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 288, 236.9, 3.4780, 3.0595, 19600], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4080, 3.0599, 20800], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4434, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 224, 218.0, 3.4582, 3.0578, 19600], # 1.5h\n",
    "[ 16, 8, 4, 768, 3072, 192, 208.6, 3.4559, 3.0593, 19600], # 1.4h\n",
    "[ 16, 8, 4, 736, 2944, 256, 213.5, 3.3695, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 736, 2944, 224, 204.4, 3.3880, 3.0584, 19600], # 1.5h\n",
    "[ 16, 8, 4, 736, 2944, 192, 195.4, 3.4246, 3.0605, 19600], # 1.4h 13.1G\n",
    "[ 16, 8, 4, 704, 2816, 256, 199.9, 3.4143, 3.0660, 19600], # 1.6h\n",
    "[ 16, 8, 4, 704, 2816, 224, 191.2, 3.4081, 3.0591, 19600], # 1.5h\n",
    "[ 16, 8, 4, 704, 2816, 192, 182.6, 3.3957, 3.0584, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 256, 186.7, 3.3549, 3.0560, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 240, 182.5, 3.4249, 3.0599, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 224, 178.4, 3.3611,+3.0553, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 208, 174.3, 3.3694,+3.0556, 19600], # 1.4h\n",
    "[+16, 8, 4, 672, 2688, 192, 170.1, 3.3666,+3.0526, 19600], # 1.3h   minGemma-hidden_layers16-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-07-15-16-14.pth\n",
    "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3690, 3.0573, 19600], # again\n",
    "[ 16, 8, 4, 672, 2688, 176, 166.0, 3.3938, 3.0599, 19600], # 1.3h\n",
    "[ 16, 8, 4, 640, 2560, 256, 173.8, 3.3768, 3.0603, 19600], # 1.5h\n",
    "[ 16, 8, 4, 640, 2560, 224, 166.0, 3.3484, 3.0612, 19600], # 1.4h\n",
    "[ 16, 8, 4, 640, 2560, 192, 158.1, 3.3972, 3.0658, 19600], # 1.3h\n",
    "\n",
    "[ 16, 6, 3, 768, 3072, 288, 215.7, 3.5378, 3.0657, 19600], # 1.5h\n",
    "[ 16, 6, 3, 768, 3072, 240, 205.1, 3.5022,+3.0585, 19600], # 1.4h\n",
    "[ 16, 6, 3, 768, 3072, 192, 194.4, 3.4339, 3.0600, 19600],\n",
    "[ 16, 6, 3, 768, 3072, 144, 183.8, 3.4668, 3.0630, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 192, 166.6, 3.4332, 3.0602, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 144, 157.0, 3.4218, 3.0633, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 192, 124.6, 3.3380, 3.0774, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 144, 116.6, 3.3442, 3.0790, 19600],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2f3060a-cc17-40ac-9528-b250529bfdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L16 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:22:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.366600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3666, 3.0526, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "# 208,224,240,256\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0538:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13be91-8a0d-488f-88f0-80499e8981e2",
   "metadata": {},
   "source": [
    "# L14 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fea3bc-0a55-4509-9e59-b2c787e441b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L14 Normal Model (default: x4 B12 lr13.5e-4 WD1)\n",
    "[ 14, 8, 4, 768, 3072, 240, 199.7, 3.4702, 3.0625, 20800],\n",
    "[ 14, 8, 4, 768, 3072, 192, 187.4, 3.4227, 3.0584, 20800],\n",
    "[ 14, 8, 4, 768, 3072, 144, 175.0, 3.3769, 3.0621, 20800], # 1.2h\n",
    "[ 14, 8, 4, 736, 2944, 192, 175.6, 3.3751, 3.0596, 20800], # 1.3h\n",
    "[ 14, 8, 4, 704, 2816, 240, 175.5, 3.4039, 3.0631, 20800],\n",
    "[ 14, 8, 4, 704, 2816, 216, 169.8, 3.3530, 3.0576, 20800], # 1.4h\n",
    "[ 14, 8, 4, 704, 3520, 192, 185.0, 3.3541, 3.0583, 20800], # 1.4h x5\n",
    "[+14, 8, 4, 704, 2816, 192, 164.2, 3.3670,+3.0553, 20800],\n",
    "[ 14, 8, 4, 704, 2816, 192, 164.2, 3.3564, 3.0626, 19600], # 1.2h\n",
    "[ 14, 8, 4, 704, 2816, 168, 158.5, 3.3300, 3.0610, 20800], # 1.2h\n",
    "[ 14, 8, 4, 704, 2816, 144, 152.8, 3.3884, 3.0668, 20800], # 1.2h\n",
    "[ 14, 8, 4, 672, 2688, 192, 153.1, 3.3537, 3.0588, 20800], # 1.3h\n",
    "[ 14, 8, 4, 640, 2560, 240, 152.7, 3.3119, 3.0687, 20800], # 1.3h\n",
    "[ 14, 8, 4, 640, 2560, 192, 142.4, 3.3184, 3.0596, 20800], # 1.2h\n",
    "[ 14, 8, 4, 640, 2560, 144, 132.0, 3.3049, 3.0674, 20800], # 1.1h\n",
    "\n",
    "# search N_steps\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3734, 3.0636, 21600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3264, 3.0617, 21200]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3558,+3.0599, 20800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3716, 3.0676, 20400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3671, 3.0644, 20000]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3863, 3.0621, 19800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4194, 3.0608, 19600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3920, 3.0630, 19400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3783, 3.0640, 19200]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4281, 3.0671, 18800]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4434, 3.0638, 18400]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4765, 3.0712, 18000]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.5028, 3.0658, 17600]\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.4935, 3.0633, 17200]\n",
    "\n",
    "# search x4 ~ x8 for FFN\n",
    "[ 14, 6, 3, 768, 3072, 240, 184.3, 3.4371, 3.0605, 20800], # x4\n",
    "[ 14, 6, 3, 768, 3072, 192, 175.0, 3.4229, 3.0628, 20800], # x4\n",
    "[ 14, 6, 3, 696, 3480, 192, 170.5, 3.3813, 3.0639, 20800], # x5\n",
    "[ 14, 6, 3, 696, 2784, 192, 150.1, 3.3558,+3.0599, 20800], # x4\n",
    "[ 14, 6, 3, 696, 4176, 192, 190.9, 3.4417, 3.0702, 19600], # x6\n",
    "[ 14, 6, 3, 696, 3480, 192, 170.5, 3.3835,+3.0595, 19600], # x5 # x4 is equally good:3.0599. So use x4 from now on.\n",
    "[ 14, 6, 3, 696, 4176, 144, 182.5, 3.4328, 3.0709, 19600], # x6\n",
    "[ 14, 6, 3, 696, 2784, 144, 141.7, 3.4093, 3.0697, 19600], # x4\n",
    "[ 14, 6, 3, 576, 2304, 192, 112.6, 3.3262, 3.0709, 19600], # x4\n",
    "[ 14, 6, 3, 576, 4608, 144, 161.5, 3.3518, 3.0762, 19600], # x8\n",
    "[ 14, 6, 3, 576, 4032, 144, 147.5, 3.3395, 3.0767, 19600], # x7\n",
    "[ 14, 6, 3, 576, 3456, 144, 133.6, 3.3417, 3.0783, 19600], # x6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df324c1-ab37-437a-b07c-365b0ff18076",
   "metadata": {},
   "source": [
    "# L12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84032f5c-cfee-4c88-8a9a-79557bf39719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea054808-e313-49b5-9455-609b14bfbeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.5072\n",
      "L12 att10 kv_heads5 hidden720 intermediate2880 head_dim288 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18800' max='18800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18800/18800 1:35:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.442500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 12, 10, 5, 720, 2880, 288, 185.6, 3.4425, 3.0543, 18800],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  12 # 8 # G28 G18 #blocks\n",
    "num_attention_heads =10 # 4 # G16 G8\n",
    "num_key_value_heads = 5 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*72 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 288 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "\n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=18800; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0543:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a49635-3ca8-4b7b-8934-faac13136321",
   "metadata": {},
   "source": [
    "# L10 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964584b-e297-4e55-883c-3077ff5ab532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L10 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.1807, 3.1066, 30000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.2931, 3.0871, 25000],\n",
    "[+10, 8, 4, 800, 3200, 256, 166.2, 3.4480,+3.0815, 20000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.6975, 3.0919, 15000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.8016, 3.1057, 14000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.9316, 3.1021, 13000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.9621, 3.1133, 12000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.0615, 3.1327, 11000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.2371, 3.1642, 10000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.3337, 3.2016, 9000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.5622, 3.2823, 8000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 4.8595, 3.3677, 7000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 5.1612, 3.6063, 6000],\n",
    "[ 10, 8, 4, 800, 3200, 256, 166.2, 5.7221, 3.7153, 5000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578ef800-2d45-4ac6-bba4-047b98ce55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.6\n",
      "L10 att8 kv_heads4 hidden800 intermediate3200 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 1:53:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.293100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10, 8, 4, 800, 3200, 256, 166.2, 3.2931, 3.0871],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  10 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        \n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=25000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] # if loss_current < 3.03: torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28041e-2099-4850-b73c-234c00b323bc",
   "metadata": {},
   "source": [
    "# L8 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea191f9-e096-4f24-982b-2bd221f9cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L8 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 5.7465, 3.7427, 5000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 5.1040, 3.5869, 6000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.7882, 3.4449, 7000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.5029, 3.3351, 8000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.4746, 3.2090, 9000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.2663, 3.1698, 10000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 4.1281, 3.1661, 11000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.9549, 3.1171, 12000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.8525, 3.1050, 13000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.8220, 3.0935, 14000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 6.2252, 4.5156, 14000], # lr10e-3\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.7526, 3.0987, 15000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.6494, 3.0859, 16000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.6319, 3.0848, 17000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5341, 3.0828, 18000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5459, 3.0801, 19000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.5318, 3.0850, 20000],\n",
    "[+8, 8, 4, 800, 3200, 480, 175.4, 3.4380,+3.0768, 21000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.4266, 3.0830, 22000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.3882, 3.0785, 23000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.3449, 3.0896, 24000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2977, 3.0901, 26000],\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2600, 3.0904, 28000], # 2.1h\n",
    "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.2057, 3.1062, 30000],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d23992a-90ef-49a0-bca8-cafd2c3d571e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129.024\n",
      "L8 att8 kv_heads4 hidden800 intermediate3200 head_dim480 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21000' max='21000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21000/21000 1:35:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.438000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8, 8, 4, 800, 3200, 480, 175.4, 3.4380, 3.0768],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   8 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*100 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 480 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=21000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] \n",
    "if torch.Tensor(loss).mean() < 3.0768:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e023cc-ae96-46f9-b090-30686f7090c3",
   "metadata": {},
   "source": [
    "# L6 (Not used for figure because not explored enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853da4f-95c8-4ce2-b2f0-9a74f1bdb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L6 Normal Model (default: lr10e-4 WD1) N_step searched\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1039, 3.1584, 40000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1093, 3.1519, 39000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1261, 3.1433, 38000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1181, 3.1443, 37000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1365, 3.1336, 36000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1393, 3.1364, 35000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1610, 3.1337, 34000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.1499, 3.1289, 33000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2074, 3.1233, 32000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2335, 3.1137, 31000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2334, 3.1148, 30000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2369, 3.1145, 29000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.2508, 3.1103, 28000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3013, 3.1137, 27000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3184, 3.1186, 26000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3556, 3.1025, 25000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3530, 3.1014, 24000],\n",
    "[+6, 8, 4, 768, 3072, 256, 109.4, 3.3593,+3.0972, 24500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3756, 3.1009, 23000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4384,+3.0983, 21500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4643, 3.1026, 20800],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4771, 3.1049, 20000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5378, 3.1052, 19200],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5558, 3.0994, 18900],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5543, 3.0984, 18900], # lr10.5e-4\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5785,+3.0984, 18500],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5754, 3.1091, 18000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.6484, 3.1038, 17000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.6696, 3.1147, 16000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8011, 3.1100, 15000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8267, 3.1166, 14000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.8805, 3.1264, 13000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.0110, 3.1507, 12000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.1352, 3.1548, 11000],\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 4.2552, 3.1765, 10000],\n",
    "\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.4519, 3.0979, 19200], # cosine instead of linear lr scheduling\n",
    "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.5426, 3.0966, 19200], # cosine instead of linear lr scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31c9eb23-496b-47b5-8373-a2ec89017134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.456\n",
      "L6 att8 kv_heads4 hidden768 intermediate3072 head_dim256 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24000' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24000/24000 1:07:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.353000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6, 8, 4, 768, 3072, 256, 109.4, 3.3530, 3.1014],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =   6 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*96 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 256 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "        \n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=24000; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal Model # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=10e-4, weight_decay=1.0, lr_scheduler_type=\"linear\", num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss=[]; model.eval(); B2=18; torch.cuda.empty_cache();\n",
    "for k in range(4000): # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()] # if loss_current < 3.03: torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
