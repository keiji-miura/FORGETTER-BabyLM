{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d1f54a-eff5-43bd-bad2-ae64c3513f64",
   "metadata": {},
   "source": [
    "### Note: only L16 was uploaded so far. After the clean up is done, the summaries for the other layers will come soon in the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f20c11-804b-497f-934b-7eaf79b88de7",
   "metadata": {},
   "source": [
    "# L16 (Models with 16 Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1600dad-6725-4a5d-b199-a0b098e77bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## L16 Normal Baby10M  # default: B12 x4 lr13.5e-4 WD1\n",
    "\n",
    "# Each row is the output for a model in the following format. Different lines for different model structures. See the cell below for the excecuted codes.\n",
    "# print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')\n",
    "# Example: [16layers, 12(6)heads, hidden-dim=696, inter-layer-dim = 696x4, head-dim=224, #param=218million, train_loss, val_loss, Num-steps]\n",
    "\n",
    "[ 16,12, 6, 696, 2784, 224, 217.9, 3.3786, 3.0553, 19600], # 1.9h 13.7G(VRAM)\n",
    "[ 16,12, 4, 696, 2784, 224, 207.9, 3.3824, 3.0603, 19600], # 1.8h 13.7G\n",
    "[ 16,12, 3, 696, 2784, 224, 202.9, 3.3565, 3.0611, 19600], # 1.8h\n",
    "[ 16,10, 5, 680, 2720, 224, 196.2, 3.3722, 3.0608, 19600], # 1.7h\n",
    "[ 16,10, 2, 680, 2720, 224, 181.6, 3.3138, 3.0634, 19600], # 1.6h\n",
    "[ 16,10, 1, 680, 2720, 224, 176.7, 3.3655, 3.0689, 19600], # 1.6h\n",
    "[ 16, 9, 9, 756, 3024, 256, 259.3, 3.4331, 3.0571, 19600], # 2.0h\n",
    "[ 16, 9, 9, 684, 2736, 224, 212.6, 3.3968, 3.0588, 19600], # 1.7h 14.0G\n",
    "\n",
    "[ 16, 9, 3, 792, 3168, 288, 248.0, 3.4878, 3.0605, 19600], # 2.1h\n",
    "[ 16, 9, 3, 792, 3168, 256, 238.2, 3.4353, 3.0605, 19600], # 1.9h\n",
    "[ 16, 9, 3, 792, 3168, 224, 228.5, 3.4456, 3.0610, 19600], # 1.7h\n",
    "[ 16, 9, 3, 792, 3168, 192, 218.8, 3.5141, 3.0611, 19600], # 1.6h\n",
    "[ 16, 9, 3, 792, 3168, 160, 209.0, 3.4412, 3.0646, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 304, 236.1, 3.4335, 3.0568, 19600], # 2.0h\n",
    "[ 16, 9, 3, 756, 3024, 288, 231.5, 3.4106,+3.0552, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 272, 226.8, 3.4141, 3.0610, 19600], # 1.9h\n",
    "[ 16, 9, 3, 756, 3024, 256, 222.2, 3.4238, 3.0567, 19600], # 1.7h 13.3G\n",
    "[ 16, 9, 3, 756, 3024, 224, 212.9, 3.4077, 3.0592, 19600], # 1.6h\n",
    "[ 16, 9, 3, 756, 3024, 192, 203.6, 3.3903, 3.0588, 19600], # 1.5h\n",
    "[ 16, 9, 3, 756, 3024, 160, 194.3, 3.4151, 3.0577, 19600], # 1.4h 13.1G\n",
    "[ 16, 9, 3, 720, 2880, 320, 224.3, 3.3521, 3.0561, 19600], # 2.0h\n",
    "[ 16, 9, 3, 720, 2880, 304, 219.9, 3.4003, 3.0572, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 288, 215.5, 3.3564,+3.0556, 19600], # 1.9h\n",
    "[ 16, 9, 3, 720, 2880, 272, 211.0, 3.3431, 3.0589, 19600], # 1.8h\n",
    "[ 16, 9, 3, 720, 2880, 256, 206.6, 3.3858, 3.0575, 19600], # 1.7h\n",
    "[ 16, 9, 3, 720, 2880, 224, 197.8, 3.4157, 3.0565, 19600], # 1.6h\n",
    "[ 16, 9, 3, 720, 2880, 192, 188.9, 3.4725, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 720, 2880, 160, 180.1, 3.4023, 3.0613, 19600], # 1.3h\n",
    "[ 16, 9, 3, 702, 2808, 224, 190.4, 3.3924, 3.0624, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 288, 200.0, 3.3698, 3.0613, 19600], # 1.8h\n",
    "[ 16, 9, 3, 684, 2736, 256, 191.6, 3.3788, 3.0613, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 2736, 240, 187.4, 3.3933, 3.0585, 19600], # 1.6h\n",
    "[ 16, 9, 3, 684, 3420, 224, 205.6, 3.4053, 3.0570, 19600], # 1.6h x5\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3528,+3.0538, 19600], # 1.5h # local min\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3641, 3.0562, 19600], # again?\n",
    "[ 16, 9, 3, 684, 2736, 224, 183.2, 3.3679,+3.0545, 19600], # again again\n",
    "[ 16, 9, 3, 684, 2052, 224, 160.7, 3.3504, 3.0636, 19600], # 1.4h x3\n",
    "[ 16, 9, 3, 684, 2736, 208, 179.0, 3.4294, 3.0651, 19600], # 1.5h\n",
    "[ 16, 9, 3, 684, 2736, 192, 174.8, 3.3822, 3.0578, 19600], # 1.4h\n",
    "[ 16, 9, 3, 684, 2736, 160, 166.4, 3.3546, 3.0708, 19600], # 1.3h\n",
    "[ 16, 9, 3, 666, 2664, 224, 176.0, 3.3408, 3.0619, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 288, 185.0, 3.4168, 3.0575, 19600], # 1.8h\n",
    "[ 16, 9, 3, 648, 2592, 256, 177.0, 3.3494, 3.0597, 19600], # 1.6h\n",
    "[ 16, 9, 3, 648, 2592, 224, 169.0, 3.3636, 3.0596, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 208, 165.1, 3.3476, 3.0566, 19600], # 1.5h\n",
    "[ 16, 9, 3, 648, 2592, 192, 161.1, 3.3674,+3.0552, 19600], # 1.4h\n",
    "[ 16, 9, 3, 648, 2592, 176, 157.1, 3.3518, 3.0571, 19600], # 1.3h\n",
    "[ 16, 9, 3, 648, 2592, 160, 153.1, 3.4071, 3.0621, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 288, 170.5, 3.3219, 3.0604, 19600], # 1.7h\n",
    "[ 16, 9, 3, 612, 2448, 256, 162.9, 3.3793, 3.0589, 19600], # 1.5h\n",
    "[ 16, 9, 3, 612, 2448, 224, 155.4, 3.3291, 3.0650, 19600], # 1.4h\n",
    "[ 16, 9, 3, 612, 2448, 192, 147.9, 3.3395, 3.0629, 19600], # 1.3h\n",
    "[ 16, 9, 3, 612, 2448, 160, 140.4, 3.3432, 3.0705, 19600], # 1.2h\n",
    "[ 16, 9, 3, 576, 2304, 288, 156.5, 3.3139, 3.0602, 19600], # 1.7h\n",
    "[ 16, 9, 3, 576, 2304, 256, 149.4, 3.3302, 3.0645, 19600], # 1.5h\n",
    "[ 16, 9, 3, 576, 2304, 224, 142.3, 3.3453, 3.0616, 19600], # 1.4h\n",
    "[ 16, 9, 3, 576, 2304, 192, 135.2, 3.3139, 3.0647, 19600], # 1.3h\n",
    "[ 16, 9, 3, 576, 2304, 160, 128.1, 3.3197, 3.0732, 19600], # 1.2h\n",
    "\n",
    "[ 16, 9, 1, 684, 2736, 224, 173.4, 3.3315, 3.0629, 19600], # 1.5h\n",
    "\n",
    "[ 16, 8, 4, 960, 3840, 144, 278.4, 3.5831, 3.0678, 19600], # 1.7h\n",
    "[ 16, 8, 4, 912, 3648, 192, 272.9, 3.5625, 3.0690, 19600], # 1.8h\n",
    "[ 16, 8, 4, 896, 3584, 144, 248.9, 3.5718, 3.0678, 19600], # 1.5h\n",
    "[ 16, 8, 4, 880, 3520, 256, 279.6, 3.5187, 3.0604, 19600], # 1.9h\n",
    "[ 16, 8, 4, 864, 3456, 192, 250.6, 3.5129, 3.0600, 19600], # 1.6h\n",
    "[ 16, 8, 4, 832, 3328, 256, 256.7, 3.5653, 3.0695, 19600], # 1.8h\n",
    "[ 16, 8, 4, 832, 3328, 144, 220.9, 3.4968, 3.0645, 19600], # 1.4h\n",
    "[ 16, 8, 4, 800, 3200, 256, 241.9, 3.4717, 3.0610, 19600], # 1.7h\n",
    "[ 16, 8, 4, 800, 3200, 224, 232.0, 3.4724, 3.0602, 19600], # 1.6h\n",
    "[ 16, 8, 4, 800, 3200, 192, 222.2, 3.4555, 3.0601, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 288, 236.9, 3.4780, 3.0595, 19600], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4080, 3.0599, 20800], # 1.8h\n",
    "[ 16, 8, 4, 768, 3072, 256, 227.5, 3.4434, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 768, 3072, 224, 218.0, 3.4582, 3.0578, 19600], # 1.5h\n",
    "[ 16, 8, 4, 768, 3072, 192, 208.6, 3.4559, 3.0593, 19600], # 1.4h\n",
    "[ 16, 8, 4, 736, 2944, 256, 213.5, 3.3695, 3.0567, 19600], # 1.6h\n",
    "[ 16, 8, 4, 736, 2944, 224, 204.4, 3.3880, 3.0584, 19600], # 1.5h\n",
    "[ 16, 8, 4, 736, 2944, 192, 195.4, 3.4246, 3.0605, 19600], # 1.4h 13.1G\n",
    "[ 16, 8, 4, 704, 2816, 256, 199.9, 3.4143, 3.0660, 19600], # 1.6h\n",
    "[ 16, 8, 4, 704, 2816, 224, 191.2, 3.4081, 3.0591, 19600], # 1.5h\n",
    "[ 16, 8, 4, 704, 2816, 192, 182.6, 3.3957, 3.0584, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 256, 186.7, 3.3549, 3.0560, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 240, 182.5, 3.4249, 3.0599, 19600], # 1.5h\n",
    "[ 16, 8, 4, 672, 2688, 224, 178.4, 3.3611,+3.0553, 19600], # 1.4h\n",
    "[ 16, 8, 4, 672, 2688, 208, 174.3, 3.3694,+3.0556, 19600], # 1.4h\n",
    "[+16, 8, 4, 672, 2688, 192, 170.1, 3.3666,+3.0526, 19600], # 1.3h  # criteria:3.0526   minGemma-hidden_layers16-att_heads8-kv_heads4-hidden672-intermediate2688-head_dim192-T512--2025-07-15-16-14.pth\n",
    "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3690, 3.0573, 19600], # again\n",
    "[ 16, 8, 4, 672, 2688, 176, 166.0, 3.3938, 3.0599, 19600], # 1.3h\n",
    "[ 16, 8, 4, 640, 2560, 256, 173.8, 3.3768, 3.0603, 19600], # 1.5h\n",
    "[ 16, 8, 4, 640, 2560, 224, 166.0, 3.3484, 3.0612, 19600], # 1.4h\n",
    "[ 16, 8, 4, 640, 2560, 192, 158.1, 3.3972, 3.0658, 19600], # 1.3h\n",
    "\n",
    "[ 16, 6, 3, 768, 3072, 288, 215.7, 3.5378, 3.0657, 19600], # 1.5h\n",
    "[ 16, 6, 3, 768, 3072, 240, 205.1, 3.5022,+3.0585, 19600], # 1.4h\n",
    "[ 16, 6, 3, 768, 3072, 192, 194.4, 3.4339, 3.0600, 19600],\n",
    "[ 16, 6, 3, 768, 3072, 144, 183.8, 3.4668, 3.0630, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 192, 166.6, 3.4332, 3.0602, 19600],\n",
    "[ 16, 6, 3, 696, 2784, 144, 157.0, 3.4218, 3.0633, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 192, 124.6, 3.3380, 3.0774, 19600],\n",
    "[ 16, 6, 3, 576, 2304, 144, 116.6, 3.3442, 3.0790, 19600],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2f3060a-cc17-40ac-9528-b250529bfdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.4224\n",
      "L16 att8 kv_heads4 hidden672 intermediate2688 head_dim192 T512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19600/19600 1:22:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.366600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16, 8, 4, 672, 2688, 192, 170.1, 3.3666, 3.0526, 19600],\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt; import numpy as np; import time, torch; device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from transformers import AutoTokenizer, TrainingArguments, DefaultDataCollator, Trainer\n",
    "vocab_size = 50257 # =tokenizer.vocab_size  # FIX!!! # G256128    ### T=256 for minGemma # G8192 for real Gemma\n",
    "num_hidden_layers =  16 # 8 # G28 G18 #blocks\n",
    "num_attention_heads = 8 # 4 # G16 G8\n",
    "num_key_value_heads = 4 # 4 # G16 G1\n",
    "hidden_size = num_attention_heads*84 # 124 # 88 # 116 # 128 # G3072 G2048 # embedding dimension\n",
    "intermediate_size = hidden_size*4 # x4 or x8 # time limiting factor #512 # G24576 G16384  # MLP inner dim\n",
    "head_dim = 192 # 32 # G256 # dim in attention # Doesn't affect time\n",
    "rms_norm_eps = 1e-6 # 1e-6\n",
    "rope_theta = 1000.0 # scale freq is small for S-model. 1000 might work too # G10000.0\n",
    "# 208,224,240,256\n",
    "def apply_rotary_emb(x: torch.Tensor, dim: int) -> torch.Tensor: # seq_len = x.size(1) # N\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, dim, 2, device=device).float() / dim)) # Dynamically compute frequency cis\n",
    "    t = torch.arange(x.size(1), device=device); freqs = torch.outer(t, freqs).float(); freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    return x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "class RMSNorm(torch.nn.Module): # RMS:4.326552, RMS_no_weight:4.410741 # RMS':4.554899\n",
    "    def __init__(self, dim: int = hidden_size):\n",
    "        super().__init__(); self.weight = torch.nn.Parameter(torch.zeros(dim)) # one weight per feature to be learned\n",
    "    def _norm(self, x): # mean square for each feature (across the last dimension)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
    "    def forward(self, x): # ensure the data type matches the input.\n",
    "        return self._norm(x.float()).type_as(x) * (1 + self.weight)\n",
    "        \n",
    "class GemmaAttention(torch.nn.Module): # MQA = K,V shared by 4Qs\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.qkv_proj = torch.nn.Linear(hidden_size, (num_attention_heads + 2 * num_key_value_heads) * head_dim, bias=False); self.o_proj = torch.nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False) # concatenated attention outputs back to the hidden size.\n",
    "    def forward(self, hidden_states: torch.Tensor,) -> torch.Tensor:  # in=(B, T, hidden_size)\n",
    "        batch_size, input_len, _ = hidden_states.shape\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "        xq, xk, xv = qkv.split([num_attention_heads * head_dim, num_key_value_heads * head_dim, num_key_value_heads * head_dim],dim=-1)\n",
    "        xq = xq.view(batch_size, -1, num_attention_heads, head_dim); xk = xk.view(batch_size, -1, num_key_value_heads, head_dim); xv = xv.view(batch_size, -1, num_key_value_heads, head_dim)\n",
    "        xq = apply_rotary_emb(xq, head_dim); xk = apply_rotary_emb(xk, head_dim)\n",
    "        if num_key_value_heads != num_attention_heads:  # Q/KV multiples of K and V to match Q\n",
    "            xk = torch.repeat_interleave(xk, num_attention_heads // num_key_value_heads, dim=2) # [B, T, n_local_heads, head_dim]\n",
    "            xv = torch.repeat_interleave(xv, num_attention_heads // num_key_value_heads, dim=2)\n",
    "        q = xq.transpose(1, 2); k = xk.transpose(1, 2); v = xv.transpose(1, 2) # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0, is_causal=True) # B nh T hs        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, input_len, -1)  # [B, T, \"hidden_dim\"]\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class GemmaDecoderLayer(torch.nn.Module): # normalize before and after the attention mechanism\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.self_attn = GemmaAttention(); self.input_layernorm = RMSNorm(); self.post_attention_layernorm = RMSNorm(); self.gate_proj = torch.nn.Linear(hidden_size, intermediate_size); self.up_proj = torch.nn.Linear(hidden_size, intermediate_size); self.down_proj = torch.nn.Linear(intermediate_size, hidden_size) # mlp\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:  # input_size = (B, T, hidden_size)\n",
    "        residual = hidden_states # Self Attention Block\n",
    "        hidden_states = self.input_layernorm(hidden_states); hidden_states = self.self_attn(hidden_states=hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states # MLP Block\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states); gate = torch.nn.functional.gelu(self.gate_proj(hidden_states)); up = self.up_proj(hidden_states); fuse = gate * up; hidden_states = self.down_proj(fuse) # mlp\n",
    "        return residual + hidden_states\n",
    "\n",
    "class minGemma(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(); self.embedder = torch.nn.Embedding(vocab_size, hidden_size); self.layers = torch.nn.ModuleList(GemmaDecoderLayer() for _ in range(num_hidden_layers)); self.norm = RMSNorm();\n",
    "    def forward(self, input_token_ids: torch.Tensor) -> torch.Tensor: # (B, T)\n",
    "        hidden_states = self.embedder(input_token_ids[:,:-1]) # (B, T) & (vocab_size, hidden_size) -> (B, T, hidden_size)\n",
    "        hidden_states = hidden_states * (hidden_size**0.5)\n",
    "        for i in range(len(self.layers)):\n",
    "            hidden_states = self.layers[i](hidden_states) # shortened too much???\n",
    "        hidden_states = self.norm(hidden_states) # -> (B, T, hidden_size)        \n",
    "        embedder_weight = self.embedder.weight\n",
    "        logits = torch.matmul(hidden_states, embedder_weight.t()); b,t,v=logits.shape; # (B, T, hidden_size) @ (hidden_size, vocab_size) -> (B, T, vocab_size)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(b*t,v), input_token_ids[:,1:].reshape(b*t)) #, weight=None, ignore_index=-100, reduction='mean')\n",
    "        return loss, logits # logits, loss\n",
    "\n",
    "def map_to_array5(ix):\n",
    "    common = torch.stack([torch.from_numpy((train_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "def map_to_array_Val(ix):\n",
    "    common = torch.stack([torch.from_numpy((val_data[i[0]:i[0]+T+1]).astype(np.int64)) for i in ix]); return {'input_token_ids': common}\n",
    "\n",
    "train_data = np.memmap('train_BabyLM_10M.bin', dtype=np.uint16, mode='r'); val_data = np.memmap('val_BabyLM.bin', dtype=np.uint16, mode='r')\n",
    "T=512; B=12; N_step=19600; print(T * B * N_step / 1000000) # 0.01 B-tokens being calculated # n_steps=N_step;\n",
    "model = minGemma().to(device); print(f'L{num_hidden_layers}' f' att{num_attention_heads}' f' kv_heads{num_key_value_heads}' f' hidden{hidden_size}' f' intermediate{intermediate_size}' f' head_dim{head_dim}' f' T{T}')\n",
    "\n",
    "# Normal # lr_scheduler_type=\"linear\" can be omitted\n",
    "training_args = TrainingArguments(learning_rate=13.5e-4, weight_decay=1.0, num_train_epochs=1, logging_strategy='epoch', output_dir='./', bf16=True, per_device_train_batch_size=B, per_device_eval_batch_size=B, eval_strategy='no', save_strategy='no', report_to='none', remove_unused_columns=False, dataloader_pin_memory=True) #, dataloader_num_workers=4\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=torch.utils.data.TensorDataset(torch.randint(len(train_data)-T-1, (B*N_step,))), data_collator=map_to_array5);\n",
    "result = trainer.train(); tloss=result[2][\"train_loss\"] # trainer = Trainer(model=model, args=training_args, eval_dataset=torch.utils.data.TensorDataset(torch.randint(len(val_data)-T-1, (B*400*4,))), data_collator=map_to_array_Val); trainer.can_return_loss = True; loss_current = trainer.evaluate()[\"eval_loss\"]\n",
    "\n",
    "loss = []; model.eval(); B2=16; B2=12; torch.cuda.empty_cache();\n",
    "for k in range(5000): #4000 # std=0.0056 for 1000 with 89sec\n",
    "    val_ind = torch.randint(len(val_data)-T-1, (B2,)); common = (torch.stack([torch.from_numpy((val_data[i:i+T+1]).astype(np.int64)) for i in val_ind]))\n",
    "    loss += [model(common.to('cuda', non_blocking=True))[0].item()]\n",
    "if torch.Tensor(loss).mean() < 3.0538:\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}' f'-hidden_layers{num_hidden_layers}' f'-att_heads{num_attention_heads}' f'-kv_heads{num_key_value_heads}' f'-hidden{hidden_size}' f'-intermediate{intermediate_size}' f'-head_dim{head_dim}' f'-T{T}' f'--{time.strftime(\"%Y-%m-%d-%H-%M\")}.pth')\n",
    "model.train(); del common; print(f'[ {num_hidden_layers}, {num_attention_heads}, {num_key_value_heads}, {hidden_size}, {intermediate_size}, {head_dim}, {sum(p.numel() for p in model.parameters()) / 10**6:.1f}, {tloss:.4f}, {torch.Tensor(loss).mean():.4f}, {N_step}],')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
